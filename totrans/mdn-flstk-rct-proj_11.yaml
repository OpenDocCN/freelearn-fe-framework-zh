- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making Sure Customers Find You with Search Engine Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we optimized the performance of our blog in the previous chapter, you may
    have noticed that the Lighthouse report also includes a **Search Engine Optimization**
    (**SEO**) score, which our app scored relatively low on. This score tells us how
    optimized our app is for being indexed properly and found by search engines such
    as Google or Bing. After successfully developing a working blog app, of course,
    we want our blog to be found by users. In this chapter, we are going to learn
    the basics of SEO and how to optimize the SEO score for our React application.
    Then, we are going to learn how to create meta tags for easier integration on
    various social media sites.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing an application for search engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving social media embeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start, please install all requirements from [*Chapter 1*](B19385_01.xhtml#_idTextAnchor016)*,
    Preparing For Full-Stack Development*, and [*Chapter 2*](B19385_02.xhtml#_idTextAnchor028)*,
    Getting to Know Node.js* *and MongoDB*.
  prefs: []
  type: TYPE_NORMAL
- en: The versions listed in those chapters are the ones used in the book. While installing
    a newer version should not be an issue, please note that certain steps might work
    differently on a newer version. If you are having an issue with the code and steps
    provided in this book, please try using the versions mentioned in *Chapters 1*
    and *2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code for this chapter on GitHub: [https://github.com/PacktPublishing/Modern-Full-Stack-React-Projects/tree/main/ch8](https://github.com/PacktPublishing/Modern-Full-Stack-React-Projects/tree/main/ch8).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CiA video for this chapter can be found at: [https://youtu.be/1xN3l0MMTbY](https://youtu.be/1xN3l0MMTbY)'
  prefs: []
  type: TYPE_NORMAL
- en: If you cloned the full repository for the book, Husky may not find the `.git`
    directory when running `npm install`. In that case, just run `git init` in the
    root of the corresponding chapter folder.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing an application for search engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get started optimizing our app for search engines, let’s briefly learn
    how search engines work. Search engines work by storing information about websites
    in an index. The **index** contains the location, content, and meta information
    of websites. Adding or updating pages in the index is called indexing and done
    by a crawler. A **crawler** is an automated software that fetches websites and
    indexes them. It is called a crawler because it follows further links on the website
    to find more websites. More advanced crawlers, such as the **Googlebot**, can
    also detect whether JavaScript is required to render the contents of a website
    and even render it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graphic visualizes how a search engine crawler works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Visualization of how a search engine crawler works](img/B19385_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Visualization of how a search engine crawler works
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, a search crawler has a queue containing URLs that it needs to
    crawl and index. It then visits the URLs one by one, fetches the HTML and, if
    it is an advanced crawler, detects whether it needs to execute JavaScript to render
    the content. In that case, the URL is added to a render queue and the rendered
    HTML is passed back into the crawler later. Then, the crawler extracts all the
    links to other pages and adds them to the queue. Finally, the parsed content is
    added to the index.
  prefs: []
  type: TYPE_NORMAL
- en: To see whether a website is already indexed by a search engine, most search
    engines provide a `site:` operator, which can be used to check whether a URL is
    already indexed by it. For example, `site:wikipedia.org` shows various URLs on
    Wikipedia that are already indexed. If your website is not indexed yet, you can
    submit it to tools such as the **Google Search Console**. The Google Search Console
    also has a detailed overview of the indexing status and any problems with indexing.
    However, it is not necessary to submit our site for it to be found, because most
    search engines automatically crawl the web and will eventually find our website.
  prefs: []
  type: TYPE_NORMAL
- en: If your website still does not get indexed, this might be because it is improperly
    configured. First, you need to create a `robots.txt` file to specify whether search
    engines are allowed to crawl parts of your website, and which parts they are allowed
    to crawl.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The **robots.txt** should not be used to hide web pages from Google search results.
    Instead, it is used to reduce traffic from crawlers on unimportant or similar
    pages. If you want to completely hide web pages from Google search results, either
    password-protect them, or use the **noindex** meta tag.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you need to make sure the contents of your website are visible to the
    crawler. Server-side rendering can help here by allowing crawlers to view the
    contents of your website without running JavaScript. Additionally, adding meta
    information using special HTML tags helps crawlers to get additional information
    about your website. For small websites, pages need to be linked properly or add
    a manual sitemap. For larger websites, such as a blog with many posts, a sitemap
    should always be defined. Finally, having good performance, fast load times, and
    a good user experience makes your website rank higher on search engines.
  prefs: []
  type: TYPE_NORMAL
- en: We have already added server-side rendering to speed up crawling by serving
    content immediately without relying on JavaScript to render it. Now, let’s further
    optimize our app for search engines. We start by creating a `robots.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a robots.txt file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s ensure that crawlers are explicitly allowed to access our app
    and index all pages on it. To do so, we need to create a `robots.txt` file, which
    crawlers will read to find out which pages they are allowed to access (if any).
    Follow these steps to create a `robots.txt` file that allows access for all crawlers
    to all pages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the **ch7** folder to a new **ch8** folder, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open the **ch8** folder in VS Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new **public/robots.txt** file in the root of our project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the newly created file and enter the following contents to allow all crawlers
    to index all pages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `robots.txt` works by defining blocks, each block being defined by matching
    a user agent. The user agent can match various crawlers, such as `Googlebot` for
    Google, or you can use `*` to match all crawlers. After the user agent, one or
    multiple `Allow` and/or `Disallow` statements can be made, that decide which paths
    a crawler is allowed or not allowed to access. In our case, we are allowing access
    to all paths. Additionally, a `Sitemap` can be specified, but we’ll see more on
    that later in the *Creating a* *sitemap* subsection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open a Terminal pane and start the frontend by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open another Terminal pane and start the backend by running the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go to **http://localhost:5173/robots.txt** in your browser to see the **robots.txt**
    file being served properly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have successfully allowed crawlers to access our app, we should
    improve our URL structure. Let’s do that by creating separate pages for each post.
  prefs: []
  type: TYPE_NORMAL
- en: Creating separate pages for posts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the moment, it is not possible to view only a single post in our blog app,
    we can only view the list of all posts. That is not good for SEO, as it means
    a search engine will always link to the index page, which might already contain
    different articles than what the user was searching for. Let’s refactor our app
    a bit to only show post titles and authors on the main page, and then link to
    separate pages for each blog post:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit **src/components/Post.jsx** to allow displaying a single full post while
    displaying a smaller version of the post in the list, with a link to the full
    version. First, we import the **Link** component from **react-router-dom**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we add an **_id** prop and a **fullPost** prop to the **Post** component.
    The **fullPost** prop will be set to **false** by default (when displayed in the
    post list) and set to **true** when using it in the single-post page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We make some adjustments to the component to show a link to the single-post
    page if we are not on a single-post page yet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, we only show the contents of the blog post on a single-post page,
    and adjust the spacing of the author info accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Adjust the prop types to add the newly defined props:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit **src/api/posts.js** and add a new function to get a single post by **id**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new **src/pages/ViewPost.jsx** file, and start by importing all the
    components and functions that we are going to need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, define a component that accepts a **postId** as props:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the component, we use a query hook to fetch a single post by **id**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, render the header and a link back to the main page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, if we managed to fetch a post with the given ID, render a post with the
    **fullPost** prop set. Otherwise, we show a **not** **found** message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, define the prop types for the **ViewPost** component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit **src/routes.jsx** and import the **ViewPost** component and the **getPostById**
    function (for server-side rendering):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a new **/posts/:postId** route for viewing a single post. In the loader,
    we fetch the single blog post and an author, if it has one. We then return the
    dehydrated state and the post ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a **Component** method for the route, where we get **dehydratedState**
    and **postId** and pass them on to the **ViewPost** component, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Go to **http://localhost:5173/** in your browser and you will see that all
    blog posts in the list now have a link in their title. Click on the link to see
    the full blog post, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Viewing a single blog post on a separate page](img/B19385_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Viewing a single blog post on a separate page
  prefs: []
  type: TYPE_NORMAL
- en: Now our blog app is already much more organized, as we do not see the full contents
    of all blog posts on the main page. We only see the title and author now and can
    then decide whether the article is interesting to us or not. Furthermore, a search
    engine can provide separate entries for each blog post, making it easier to find
    posts on our app. There is still room for improvement with the URL structure though,
    as it currently only contains the post ID. Let’s introduce more meaningful URLs
    in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Creating meaningful URLs (slugs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Websites often put keywords in the URLs to make it easier for users to see
    what they will be opening just by looking at the URL. Keywords in URLs are also
    a ranking factor for search engines, albeit a not-so-strong one. The strongest
    one is always good content. Nevertheless, having a good URL structure improves
    the user experience. For example, if the link is `/posts/64a42dfd6a7b7ab47009f5e3/making-sure-customers-find-you-with-search-engine-optimization`
    instead of just `/posts/64a42dfd6a7b7ab47009f5e3`, it is already clear from the
    URL alone what content they will find on the page. Such keywords in the URL are
    called a URL slug, named after “slugs” in journalism, which refers to using short
    descriptions of articles as internal names. Let’s get started introducing slugs
    on our post pages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit **src/routes.jsx** and adjust the path to allow for optionally including
    a slug:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We are not doing any checks on whether the slug is correct or not. In fact,
    this is not really necessary, and many pages do not do this. As long as we have
    a correct ID, we can render the blog post. We only need to make sure that the
    links to the page all include the correct slug. However, we could additionally
    add a **<link>** element with the **rel="canonical"** attribute to a page, specifying
    the canonical page with the correct slug. This would tell crawlers not to index
    duplicate pages when incorrect slugs are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the root of our project, install the **slug** npm package, which contains
    a function to properly slugify a title:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit **src/components/Post.jsx** and import the **slug** function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, adjust the link to the blog post by adding the slug, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, when we open a link from the list, the URL will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we have human readable URLs for our blog posts! However, you might have
    noticed that the title is still **Vite + React** on all pages of our app. Let’s
    change that now by introducing dynamic titles and including the blog post title
    in the page title.
  prefs: []
  type: TYPE_NORMAL
- en: Adding dynamic titles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The title of a page is even more important for SEO than keywords in the URL,
    as that is the title that will be shown in the search results in most cases. So,
    we should choose our title wisely, and if we have dynamic content (like in our
    blog), we should also dynamically adjust the title to fit the content. We can
    use the React Helmet library to facilitate changes in the `<head>` section of
    the HTML document. This library allows us to render a special `Helmet` component.
    The children of this component will replace existing tags in the `<head>` section.
    Follow these steps to use React Helmet to dynamically set the title:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let’s change the general title of our app, as it is still **Vite
    + React**. Edit **index.html** in the root of our project and change the title.
    We are going to call our blog app **Full-Stack** **React Blog**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the root of our project, install the **react-helmet-async** dependency to
    be able to dynamically change the title:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: React Helmet Async is a fork of the original React Helmet that adds support
    for newer React versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit **src/pages/ViewPost.jsx** and import the **Helmet** component from **react-helmet-async**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Render the **Helmet** component and define the **<title>** tag inside it, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit **src/pages/Blog.jsx** and import **Helmet**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, reset the title to **Full-Stack React Blog** in the **Blog** component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit **src/App.jsx** and import the **HelmetProvider**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, adjust the **App** component to render **HelmetProvider**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click on a single post in the app and you will see that the title now updates
    to include the post title!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have successfully set a dynamic title, let’s pay some attention
    to other important information in the `<head>` section, the **meta tags**.
  prefs: []
  type: TYPE_NORMAL
- en: Adding other meta tags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Meta tags, as the name tells us, contain meta information about a page. Besides
    the title, we can set meta information such as a short description, or information
    on how the browser should render a website. In this section, we will cover the
    most important SEO-relevant meta tags, starting with the description meta tag.
  prefs: []
  type: TYPE_NORMAL
- en: Description meta tag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The description meta tag contains a short description of the contents of the
    page. Similarly to the title tag, we can also dynamically set this tag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit **src/pages/Blog.jsx** and add the following generic description **<****meta>**
    tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s add a dynamic meta description tag for each blog post. The meta description
    should have between 50 and 160 characters, and since we do not have a short summary
    of our blog posts, let’s just use the full contents and truncate them after 160
    characters. Of course, it would be even better to let authors add a short summary
    when creating posts, but for simplicity, we just truncate the description here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Edit the **src/pages/ViewPost.jsx** file and define a simple function to truncate
    a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We limit the string to 160 characters, and if it’s above 160, we truncate it
    to 157 characters and add three dots at the end.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the truncated content as a meta description tag to the **Helmet** component,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After adding the description meta tag, let’s learn about other meta tags that
    could be used.
  prefs: []
  type: TYPE_NORMAL
- en: Robots meta tag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `robots` meta tag tells crawlers whether and how they should crawl web
    pages. It can be used in addition to `robots.txt`, but we should only use it if
    we want to dynamically restrict the way a certain page is crawled. It looks as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `index` keyword tells crawlers to index the page, the `follow` keyword tells
    crawlers to crawl further links on the page. The `index` and `follow` keywords
    can be toggled off by using `noindex` and `nofollow`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Viewport meta tag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another important meta tag to add is the viewport tag, which tells the browser
    (and crawlers) that your website is mobile friendly. See the following example
    of how the meta tag affects how pages are rendered on mobile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – A blog post rendering before and after adding the viewport meta
    tag](img/B19385_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – A blog post rendering before and after adding the viewport meta
    tag
  prefs: []
  type: TYPE_NORMAL
- en: 'Vite already added this meta tag automatically for us in the `index.html` template
    it provided. You can see it by looking at the `index.html` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: After learning about the viewport tag, we continue by learning about the charset
    meta tag.
  prefs: []
  type: TYPE_NORMAL
- en: Charset meta tag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The charset meta tag tells the browser and crawlers about the character encoding
    of the web page. Usually, you want to set this to UTF-8 to ensure all Unicode
    characters are rendered properly. Again, Vite already added this meta tag automatically
    for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have learned about the relevant meta tags, let’s move on to creating
    a sitemap, which helps crawlers find all the pages on our app more easily.
  prefs: []
  type: TYPE_NORMAL
- en: Other relevant meta information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is additional meta information that can be relevant for a website, such
    as setting the language in the `<html>` tag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Setting a favicon also improves the search snippet, which is what users see
    when deciding whether they should click on a link.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a sitemap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A sitemap contains a list of URLs that are part of an app, so that crawlers
    can easily detect new content and crawl the app more efficiently. It also makes
    sure that all content is found, which is especially important for content-based
    apps with a large number of pages/posts. Usually, sitemaps are provided in XML
    format. They are not mandatory for SEO, but will make it easier and faster for
    crawlers to pick up content on your app. As we have dynamic content on our blog
    app, we should also create a dynamic sitemap. Follow these steps to create a dynamic
    sitemap for our blog app:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to need a base URL for our (deployed) frontend to prefix
    all paths on our sitemap with. For now, we are simply going to set this to our
    localhost URL, but in production, this environment variable should be changed
    to the proper base URL of the app. Edit **.env** in the root of our project and
    add a **FRONTEND_URL** environment variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new **generateSitemap.js** file in the root of our project, start
    by importing the **slug** function and **dotenv**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, save the previously created environment variable in a **baseUrl** variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define an **async** function to generate a sitemap. In this function,
    we start by fetching a list of blog posts, as we want each blog post to be part
    of the sitemap:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we return a string containing the XML for the sitemap. We start by defining
    the XML header and a **<****urlset>** tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the **<urlset>** tag, we can use **<url>** tags with **<loc>** tags
    to link to various pages. We first list all the static pages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we loop over all posts that we fetched from the backend and generate
    a **<url>** tag for each of them, constructing the URLs from the post ID and the
    slug:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also optionally specify a **<lastmod>** tag, telling the crawler when
    the content was last modified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, we join all generated **<url>** tags together into a single string
    and close the **<****urlset>** tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have a function to dynamically generate a sitemap, we still need
    to include a route to it in our server.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Edit **server.js** and import the **generateSitemap** function there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, go to the first **app.use(''*'')** declaration inside the **createProdServer**
    function and check whether the URL is **/sitemap.xml**. If yes, generate the sitemap
    and return it as XML:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In a more sophisticated setup, we could cache the generated sitemap either on
    our Express server, our own web server, or a separate caching service.
  prefs: []
  type: TYPE_NORMAL
- en: We do the same change as in the previous step for the second **app.use('*')**
    declaration inside the **createDevServer** function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart the server and go to **http://localhost:5173/sitemap.xml** to see the
    sitemap being dynamically generated, with links to all created posts and their
    last modified timestamps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can now link to the sitemap in the **robots.txt** file. As an example, we
    are going to set the URL to localhost. In a production app, you would adjust this
    URL to point to the sitemap on the URL of the deployed application. Edit **public/robots.txt**
    and add the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have successfully implemented measures to improve our app for search
    engines, let’s take a look at our SEO score in the Lighthouse report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Our Lighthouse SEO score is now 100!](img/B19385_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Our Lighthouse SEO score is now 100!
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, our SEO score is now 100 (from 91 before). This might only seem
    like a slight improvement, but the Lighthouse report only takes into account basic
    checks, such as having a title, description, viewport tag, and a `robots.txt`
    file. We have done much more to optimize the user experience for visitors and
    search engines, such as improving the URL structure and adding dynamic titles
    and descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: We could still further optimize our app by serving static assets via a **Content
    Delivery Network** (**CDN**) and using responsive images (serving images in different
    sizes to optimize performance on slower connections and avoid loading the full-size
    images). However, that is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up this chapter, we are going to take a look at improving embeds on
    social media sites.
  prefs: []
  type: TYPE_NORMAL
- en: Improving social media embeds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already added the important meta tags for search engines. However, social
    media websites, such as Facebook and X (formerly Twitter), read additional meta
    tags to improve the embedding of your app on their sites and apps. Most social
    networks use a standard called Open Graph Meta Tags, which was originally created
    at Facebook. These tags can contain additional information on the type of page,
    a special title, the description, and an image for embedding the page on a social
    media website.
  prefs: []
  type: TYPE_NORMAL
- en: Open Graph meta tags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Open Graph** (**OG**) meta tags have four generic properties that every
    page can have:'
  prefs: []
  type: TYPE_NORMAL
- en: '**og:type**: Describes the type of the page; specific types may have additional
    properties'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**og:title**: Describes the title of the page as it should appear on embeds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**og:image**: An URL to an image that should be used for the embed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**og:url**: An URL to a link that should be used for the embed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `og:type` meta tag describes the type of content available on the page.
    It tells the social media sites how the embed should be formatted. Among others,
    the following values are possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '**website**: The default value, a basic embed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**article**: This is for news and blog posts, and has additional parameters
    for **published_time**, **modified_time**, **author**, **section**, and **tag**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**profile**: For user profiles, with additional parameters for **first_name**,
    **last_name**, **username**, and **gender**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**book**: For books, with additional parameters for **author**, **isbn**, **release_date**,
    and **tag**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**music** types: This includes **music.song**, **music.album**, **music.playlist**,
    and **music.radio_station**, each of them having different additional parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**video** types: This includes **video.movie**, **video.episode**, **video.tv_show**,
    and **video.other**, each of them having different additional parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A full description of the OG meta tags and all possible values can be found
    on their official website: [https://ogp.me/](https://ogp.me/).'
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: Most social media sites support OG meta tags for embeds. However, some websites,
    including X (formerly Twitter), have their own meta tags, which take priority
    over OG meta tags, if provided. X can still read OG meta tags though, so it is
    enough to only provide those.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to focus on the `article` type, as we are developing a blog
    application, so we can use this type to provide better embeds for the blog posts.
  prefs: []
  type: TYPE_NORMAL
- en: Using the OG article meta tags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have learned, the `article` type allows us to include meta information
    about the published time, modified time, and author of an article on our page.
    Let’s do this now for our single-post page:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit **src/pages/ViewPost.jsx** and import the **getUserInfo** API function,
    as we will need to resolve the author name for the corresponding meta tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the **ViewPost** component after we fetch the post, fetch the author
    name. We make sure to only do this call if the **post?.author** attribute exists
    by using the **enabled** option of the **useQuery** hook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the **Helmet** component, we define the **og:type** tag as **article**
    and define the title, published time, and modified time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we set the **og:article:author** to the resolved username:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, we loop through the tags (if there are none, we default to an empty
    array) and define a meta tag for each tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Arrays in OG meta tags work by redefining the same property multiple times.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have successfully added meta tags, our blog app is optimized for
    search engines and social media sites!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first briefly learned how search engines work. Then, we
    created a `robots.txt` file, along with separate pages for each blog post, to
    better optimize our blog for search engines. Next, we created meaningful URLs
    (slugs) and set dynamic titles and meta tags. Then, we created a sitemap and evaluated
    the SEO score of our blog after all optimizations. Finally, we learned how social
    media embeds work and which meta tags can be used to improve embeds for articles,
    such as blog posts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [*Chapter 9*](B19385_09.xhtml#_idTextAnchor176)*, Implementing
    End-to-End Tests Using Playwright*, we are going to learn how to write end-to-end
    tests for our user interface by setting up Playwright. Then, we are going to write
    some frontend tests for our blog application.
  prefs: []
  type: TYPE_NORMAL
