- en: Introduction to Microservice Server-Side Technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wasn't it easy to develop a production-ready monolithic application with JHipster?
    So far, we have created an application from scratch, added a few entities with
    JDL Studio, and then deployed it to the production environment along with tests.
    We have also added a continuous integration and continuous delivery pipeline.
    Wasn't the experience faster, easier, and better than coding everything from scratch?
  prefs: []
  type: TYPE_NORMAL
- en: So what's next? Yes, you guessed it right—**microservices**!
  prefs: []
  type: TYPE_NORMAL
- en: Microservices is the buzzword everywhere these days. Many companies out there
    are trying to solve their problems with microservices. We already saw an overview
    of the benefits of microservices in [Chapter 1](498dbd6d-b882-4551-92dd-97cdde4b62ac.xhtml), *Introduction
    to Modern Web Application Development*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of microservices over monoliths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components that we need for building a complete microservices architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will see how the monolithic application we created earlier
    can be converted into a microservice application.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we will see how easy it is to create a microservice architecture
    using the options JHipster provides.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice applications versus monoliths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The benefits of microservice architectures can be better understood by comparing
    them with monolithic architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of microservices over monoliths are phenomenal when they are designed
    and deployed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: It is not as simple as splitting a monolithic application based on structure,
    component, or functionality and then deploying them as individual services. This
    will not work out. Converting a monolithic application or even a monolithic design
    into microservices needs a clear vision of the product. It includes knowledge
    of what part of the project will change and what part will be consistent. We must
    have low-level details, such as which entities we should group together and those
    that can be separated.
  prefs: []
  type: TYPE_NORMAL
- en: This clearly illustrates the need for an ever-evolving model. It is much easier
    to split the technologies used in the application, but not the interdependent
    models or the business logic of the application. So it is essential to place the
    project's primary focus on core domain and its logic.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices should be independent. They will fail when a component is tightly
    coupled with another. The trickiest part is identifying and segregating the components.
  prefs: []
  type: TYPE_NORMAL
- en: When we have done that, it offers the following benefits over monolithic applications.
  prefs: []
  type: TYPE_NORMAL
- en: The monolithic code is a single unit. Thus, all parts of the application share
    the same memory. For a bigger system, we need to have a bigger infrastructure.
    When the application grows, we need to scale the infrastructure as needed. The
    scaling of an already bigger infrastructure is always a difficult and costlier
    task for operations.
  prefs: []
  type: TYPE_NORMAL
- en: Even though they have all the necessary code to handle anything in the product
    at a single place (no need to worry about latency or availability), it is difficult
    to handle the resources that it consumes to run and it is definitely not scalable.
    If any one part of the application fails, then the whole product will be impacted.
    When any one thread or query of the product clings on to the memory, then the
    impact will be seen by millions of our customers.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices, on the other hand, require less memory to run since we are splitting
    the application into smaller components, which in turn reduces the infrastructure's
    cost. For example, it is cheaper to run 10 2GB instances (costs ~$170 per month
    on AWS) than running a single 16 GB instance (costs ~$570 per month on AWS). Each
    component runs in its own environment, which makes microservices much more developer-friendly
    and cloud-native. Similarly, microservices also increase the throughput across
    services. A memory intensive operation on one service will not affect any other
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic architecture, over a period of time, will remove the agility of a
    team, which will delay the application rollout. This means people will tend to
    invest more time to find a workaround to fix a problem when a new feature is added,
    or something in the existing feature breaks. The monolithic architecture will
    bring a greater amount of inefficiency that in turn increases the technical debt.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices, on the other hand, reduce the technical debt in terms of architecture
    since everything is reduced to individual components. Teams tend to be more agile
    and they will find handling changes easier.
  prefs: []
  type: TYPE_NORMAL
- en: The less code there is, the fewer bugs there are, meaning less pain and a shorter
    time to fix.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic applications are more time consuming to work with. Imagine there
    is a big monolithic application and you have to reverse an *if condition* in your
    service layer. After changing the code, it has to be built, which usually takes
    a few minutes, and then you must test the entire application, which will reduce
    the team's performance.
  prefs: []
  type: TYPE_NORMAL
- en: You can reboot or reload an application in seconds for a microservice architecture.
    When you have to reverse an *if condition*, you need not wait for minutes to build
    and deploy the application to test, you can do it in seconds. This will decrease
    the time it takes to do mundane tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Faster iterations/releases and decreased downtime are the key things to increase
    user engagement and user retention, which in turn results in better revenue.
  prefs: []
  type: TYPE_NORMAL
- en: A human mind (unless you are superhuman) can handle only a limited amount of
    information. So cognitively, microservices help people to reduce the clutter and
    focus on the functionality. This enables better productivity and faster rollouts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Embracing microservices will:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximize productivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve agility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve customer experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speed up development/unit testing (if designed properly)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve revenue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building blocks of a microservice architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Running a microservice architecture requires a lot of components/features and
    involves a lot of advanced concepts. For the sake of understanding these concepts,
    imagine we have a microservice-based application for our e-commerce shopping website.
    This includes the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pricing services: Responsible for giving us the price of the product based
    on demand'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Demand services: Responsible for calculating the demand for the product based
    on the sales and stocks left'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inventory services: Responsible for tracking the quantity left in the inventory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many other services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the concepts we will see in this section are:'
  prefs: []
  type: TYPE_NORMAL
- en: Service registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health check
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic routing and resiliency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security (authentication and authorization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance and failover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are independent, but many use cases will need them to be interdependent.
    This means for some services to work properly they need data from another service,
    which in turn may or may not depend on other services or sources.
  prefs: []
  type: TYPE_NORMAL
- en: For example, our pricing service will directly depend on the demand service,
    which in turn depends on the inventory service. But these three services are completely
    independent, that is they can be deployed on any host, port, or location and scaled
    at will.
  prefs: []
  type: TYPE_NORMAL
- en: If the pricing service wants to communicate with the demand service, it has
    to know the exact location to which it can send requests to get the required information.
    Similarly, the demand service should know about the inventory service's details
    in order to communicate.
  prefs: []
  type: TYPE_NORMAL
- en: So we need a service registry that registers all other services and their locations.
    All services should register themselves to this registry service when the service
    is started and deregister itself when the service goes down.
  prefs: []
  type: TYPE_NORMAL
- en: The service registry should act as a database of services, recording all the
    available instances and their details.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The service registry has details of the services available. But in order to
    find out where the required service is and which services to connect, we need
    to have service discovery.
  prefs: []
  type: TYPE_NORMAL
- en: When the pricing service wants to communicate with the demand service, it needs
    to know the network location of the demand service. In the case of traditional
    architecture, this is a fixed physical address but in the microservices world,
    this is a dynamic address that is assigned and updated dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: The pricing service (client) will have to locate the demand service in the service
    registry and determine the location and then load balance the request o the available
    demand service. The demand service, in turn, will respond to the request of the
    requested client (pricing service).
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery is used to discover the exact service to which the client
    should connect to, in order to get the necessary details.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery helps the API gateway to discover the right endpoint for a
    request.
  prefs: []
  type: TYPE_NORMAL
- en: They will also have a load balancer, which regulates the traffic and ensures
    the high availability of the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the location where load balancing happens, the service discovery is
    classified into:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client-side discovery pattern**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The load balancing will happen on the client service side. The client service
    will determine where to send the request and the logic of load balancing will
    be in the client service. For example, Netflix Eureka ([https://github.com/Netflix/eureka](https://github.com/Netflix/eureka))
    is a service registry. It provides endpoints to register and discover the services.
  prefs: []
  type: TYPE_NORMAL
- en: When the pricing service wants to invoke the demand service, it will connect
    to the service registry and then find the available services. Then, based on the
    load balancing logic configured, the pricing service (client) will determine which
    demand service to request.
  prefs: []
  type: TYPE_NORMAL
- en: 'The services will then do an intelligent and application-specific load balancing.
    On the downside, this adds an extra layer of load balancing in every service,
    which is an overhead:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Server-side discovery pattern**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pricing service will request the load balancer to connect to the demand
    service. Then, the load balancer will connect to the service registry to determine
    the available instance, and then route the request based on the load balancing
    configured.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in Kubernetes, each pod will have its own server or proxy. All
    the requests are sent through this proxy (which has a dedicated IP and port associated
    with it).
  prefs: []
  type: TYPE_NORMAL
- en: The load balancing logic is moved away from the service and isolated into a
    separate service. On the downside, it requires yet another highly available service
    to handle the requests.
  prefs: []
  type: TYPE_NORMAL
- en: Health check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the microservices world, instances can start, change, update, and stop at
    random. They can also scale up and down based on their traffic and other settings.
    This requires a health check service that will constantly monitor the availability
    of the services.
  prefs: []
  type: TYPE_NORMAL
- en: Services can send their status periodically to this health check service, and
    this keeps a track of the health of the services. When a service goes down, the
    health check service will stop getting the heartbeat from the service. Then, the
    health check service will mark the service down and cascade the information to
    the service registry. Similarly, when the service resumes, the heartbeat is sent
    to the health check service. Upon receiving a few positive heartbeats, the service
    is marked UP and then the information is sent to the service registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The health check service can check for health in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Push configuration**: All the services will send their heartbeat periodically
    to the health check service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pull configuration**: A single health check service instance will query for
    the availability of the systems periodically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This also requires a **high availability system**. All the services should connect
    to this service to share their heartbeat and this has to connect to the service
    registry to tell them whether a service is available or not.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic routing and resiliency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The health check services will track the health of available services and send details
    to the service registry about the health of services.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this, services should intelligently route requests to healthy instances
    and shut down the traffic to unhealthy instances.
  prefs: []
  type: TYPE_NORMAL
- en: Since the services dynamically change their location (address /port), every
    time a client wants to connect to the service, it should first check for the availability
    of the services from the service registry. Every connection to the client will
    also need to have a timeout added to it, beyond which the request has to be served
    or it has to be retried (configured) to another instance. This way we can minimize
    the *cascading failure*.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a client invokes the available service, we need to validate the request.
    In order to prevent unwanted requests from piling up, we should have an additional
    layer of **security**. The requests from the client should be authenticated and
    authorized to call the other service, to prevent unauthorized calls to the service.
    The service should, in turn, decrypt the request, understand whether it is valid
    or invalid, and do the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to provide secure microservices, it should have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Confidentiality: Allow only the authorized clients to access and consume the
    information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Integrity: Can guarantee the integrity of the information that it receives
    from the client and ensure that it is not modified by a third-party (for example,
    when a gateway and a service is talking to each other, no party can tamper with
    or alter the messages that are sent between them. This a classic man-in-the-middle
    attack).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Availability: A secure API service should be highly available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reliability: Should handle the requests and process them reliably.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on MITM, or man-in-the-middle attacks check, the following
    link: [https://www.owasp.org/index.php/Man-in-the-middle_attack.](https://www.owasp.org/index.php/Man-in-the-middle_attack)
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance and failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a microservice architecture, there might be many reasons for a fault. It
    is important to handle faults or failovers gracefully, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When the request takes a long time to complete, have a predetermined timeout
    instead of waiting for the service to respond.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the request fails, identify the server, notify the service registry, and
    stop connecting to the server. This way, we can prevent other requests from going
    to that server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shut down the service when it is not responding and start a new service to make
    sure services are working as expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This can be achieved using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fault tolerance** libraries, which prevent cascading failures by isolating
    the remote instance and services that are not responding or taking a longer time
    than in the SLA to respond. This prevents other services from calling the failed
    or unhealthy instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed tracing system** libraries help to trace the timing and latency
    of the service or system, and highlight any discrepancies with the agreed SLA.
    They also help you to understand where the performance bottleneck is so that you
    can act on this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JHipster provides options to fulfill many of the preceding concepts. The most
    important of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: JHipster Registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HashiCorp Consul
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JHipster Gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JHipster console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JHipster UAA server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JHipster Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JHipster provides JHipster Registry ([http://www.jhipster.tech/jhipster-registry/](http://www.jhipster.tech/jhipster-registry/))
    as the default **service registry**. The JHipster Registry is a runtime application
    that all microservice applications register with and get their configuration from.
    It also provides additional features such as monitoring and health check dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: 'JHipster Registry is made up of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Netflix Eureka server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring cloud config server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netflix Eureka server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Eureka ([https://github.com/Netflix/eureka](https://github.com/Netflix/eureka))
    consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Eureka server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eureka is a REST-based service. It is used for locating services for load balancing
    and failover middle tiers.
  prefs: []
  type: TYPE_NORMAL
- en: Eureka servers help to load balance among the instances. They are more useful
    in a cloud-based environment where the availability is intermittent. On the other
    hand, traditional load balancers help in load balancing the traffic between known
    and fixed instances.
  prefs: []
  type: TYPE_NORMAL
- en: The Eureka client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eureka provides a Eureka client, which makes the interaction between servers
    seamless. It is a Java-based client.
  prefs: []
  type: TYPE_NORMAL
- en: Eureka acts as a **middle tier** *load balancer* that helps to load balance
    the host of a middle-tier services. They provide a simple round robin-based load
    balancing by default. The load balancing algorithm can be customized as needed
    with a wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: They cannot provide sticky sessions. They also fit perfectly for client-based
    load balancing scenarios (as seen earlier).
  prefs: []
  type: TYPE_NORMAL
- en: Eureka has no restriction on the communication technology. We can use anything,
    such as Thrift, HTTP, or any RPC mechanisms, for communication.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine our application is in different AWS Availability Zones. We register
    a Eureka cluster in each of the zones that holds information about available services
    in that region only and start the Eureka server in each zone to handle zone failures.
  prefs: []
  type: TYPE_NORMAL
- en: All the services will register themselves to the Eureka server and send their
    heartbeats. When the client no longer sends a heartbeat, the service is taken
    out of the registry itself and the information is passed across the Eureka nodes
    in the cluster. Then, any client from any zone will look up the registry information
    to locate it and then make any remote calls. Also, we need to ensure that Eureka
    clusters between regions do not communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Eureka prefers availability over consistency. That is when the services are
    connected to the Eureka server and it shares the complete configuration between
    the services. This enables services to run even when the Eureka server goes down.
    In production, we have to run Eureka in a high availability cluster for better
    consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Eureka also has the ability to add or remove the servers on the fly. This makes
    it the right choice for service registry and service discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Spring cloud config server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a microservice architecture, the services are dynamic in nature. They will
    go down and come up based on traffic or any other configuration. Due to this dynamic
    nature, there should be a separate, *highly available* server that holds the essential
    configuration details that all the servers need to know.
  prefs: []
  type: TYPE_NORMAL
- en: For example, our pricing service will need to know where the registry service
    is and how it has to communicate to the registry service. The registry service,
    on the other hand, should be highly available. If for any reason the server has
    to go down, we will spin up a new server. The pricing service needs to communicate
    with the config service in order to find out about the registry service. On the
    other hand, when the registry service is changed, it has to communicate the changes
    to the config server, which will then cascade the information to all the necessary
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Spring cloud config server ([https://github.com/spring-cloud/spring-cloud-config](https://github.com/spring-cloud/spring-cloud-config))
    provides server and client-side support for external configuration.
  prefs: []
  type: TYPE_NORMAL
- en: With the cloud config server, we have a central place to manage all our external
    properties across all environments. The concept is similar to Spring-based environment
    property source abstractions on both client and server. They fit for any application
    running in any language.
  prefs: []
  type: TYPE_NORMAL
- en: They are also helpful for carrying the configuration data between various (development/test/production)
    environments and help to migrate much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Spring config server has a HTTP, resource-based API for external configuration.
    They will encrypt and decrypt property values. They bind to the config server
    and initialize a Spring environment with remote property sources. The configuration
    can be stored in a Git repository or in a file system.
  prefs: []
  type: TYPE_NORMAL
- en: HashiCorp Consul
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consul ([https://www.consul.io/](https://www.consul.io/)) is primarily a service
    discovery client from Hashicorp. It focuses on consistency. Consul is completely
    written in Go.
  prefs: []
  type: TYPE_NORMAL
- en: This means it will have a lower memory footprint. Added to that, we can also
    use Consul with services written in any programming language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantages of using Consul are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It has a lower memory footprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used with services that are written in any programming language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It focuses on consistency rather than availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consul also provides service discovery, failure detection, multi-data center
    configuration, and storage.
  prefs: []
  type: TYPE_NORMAL
- en: This is an alternative option to JHipster Registry. There is an option to choose
    between JHipster Registry and Consul during application creation.
  prefs: []
  type: TYPE_NORMAL
- en: Eureka (JHipster Registry) requires each application to use its APIs for registering
    and discovering themselves. It focuses on availability over consistency. It supports
    only applications or services written in Spring Boot.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Consul runs as an agent in the services, and checks the health
    information and a few other extra operations listed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consul can provide a service and other clients can use Consul to discover the
    providers of a given service. Using either DNS or HTTP, applications can easily
    find the services that they depend on.
  prefs: []
  type: TYPE_NORMAL
- en: Health discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consul clients can provide any number of health checks, either associated with
    a given service or with the local node. This information can be used by a health
    check service to monitor services' health, and it is in turn used to discover
    the service components and route traffic away from unhealthy hosts and towards
    the healthy hosts.
  prefs: []
  type: TYPE_NORMAL
- en: K/V store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consul has an easy-to-use HTTP API that makes it simple for applications to
    use Consul's key/value store for dynamically configuring services, electing the
    leader when the current leader goes down, and segregating containers based on
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple data centers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consul supports multiple data centers out of the box. This means you do not
    have to worry about building additional layers of abstraction to grow to multiple
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: Consul should be a distributed and highly available service. Every node that
    provides services to Consul runs a consul agent, which is mainly responsible for
    health checking. These agents will then talk with one or more Consul servers,
    which collect and add this information. These servers will also elect a leader
    among themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, Consul serves as a service registry, service discovery, health check,
    and K/V store.
  prefs: []
  type: TYPE_NORMAL
- en: JHipster Gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a microservice architecture, we need an entry point to access all the running
    services. So we need a service that acts as a gateway. This will proxy or route
    clients' requests to the respective services. In JHipster, we provide JHipster
    Gateway for that.
  prefs: []
  type: TYPE_NORMAL
- en: JHipster Gateway is a microservice application that can be generated. It integrates
    Netflix Zuul and Hystrix in order to provide routing, filtering, security, circuit
    breaking, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Netflix Zuul
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a microservice architecture, Zuul is a front door for all the requests (gatekeeper).
    It acts as an edge service application. Zuul is built to enable *dynamic routing,
    monitoring, resiliency, and security *among the services. It also has the ability
    to dynamically route requests as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trivia: In *Ghostbusters*, Zuul is the gatekeeper.'
  prefs: []
  type: TYPE_NORMAL
- en: Zuul works based on different types of filter that enable us to quickly and
    nimbly apply functionality to our edge service.
  prefs: []
  type: TYPE_NORMAL
- en: 'These filters help us to perform the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Authentication and security: To identify each resource''s authentication requirements
    and to reject requests that do not satisfy the requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Insights and monitoring: To track data and statistics at the edge and to give
    an insight into the production application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic routing: To dynamically route requests to different backend clusters
    as needed based on health and other factors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-regional resiliency (AWS): To route requests across AWS regions in order
    to diversify our Elastic Load Balancer usage and move our edge closer to our members'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For even more information on Zuul, please check [https://github.com/Netflix/zuul/wiki.](https://github.com/Netflix/zuul/wiki)
  prefs: []
  type: TYPE_NORMAL
- en: Hystrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hystrix ([https://github.com/Netflix/Hystrix](https://github.com/Netflix/Hystrix))
    is a latency and fault tolerance library designed to isolate points of access
    to remote systems, services, and third-party libraries, stop cascading failures;
    and enable resilience in complex distributed systems where failure is inevitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hystrix is designed to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop failure cascades in a complex distributed system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protect the system from the failures of dependencies over the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control the latency of the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recover rapidly and fail faster to prevent cascading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fall back and gracefully degrade when possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable near-real-time monitoring, alerting, and operational control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications in complex distributed architectures have a lot of dependencies,
    each of which will inevitably fail at some point. If the host application is not
    isolated from these external failures, it risks being taken down with them.
  prefs: []
  type: TYPE_NORMAL
- en: JHipster Console
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The JHipster Console ([https://github.com/jhipster/jhipster-console](https://github.com/jhipster/jhipster-console))
    is a monitoring solution for microservices built using the ELK stack. It comes
    bundled with preset dashboards and configuration. It is provided as a runtime
    component in the form of a Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: The ELK Stack is made up of Elasticsearch, Logstash, and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash can be used to normalize the data (usually from logs) and then Elasticsearch
    is used to process the same data faster. Finally, Kibana is used to visualize
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch is a widely used search engine in data analytics. It helps you
    to extract data really fast from data haystacks. It also helps to provide real-time
    analytics and data extraction. It is highly scalable, available, and multi-tenanted.
  prefs: []
  type: TYPE_NORMAL
- en: It also provides full text-based searches saved as a document. These documents,
    in turn, will be updated and modified based on any changes to the data. This,
    in turn, will provide a faster search and analyze the data.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logstash ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash))
    will take the logs, process them, and convert them into output. They can read
    any type of logs, such as system logs, error logs, and app logs. They are the
    **heavy working** component of this stack, which helps to store, query, and analyze
    the logs.
  prefs: []
  type: TYPE_NORMAL
- en: They act as a pipeline for event processing and are capable of processing huge
    amounts of data with the filters and, along with Elasticsearch, deliver results
    really fast. JHipster makes sure that the logs are in the correct format so that
    they can be grouped and visualized in the correct way.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kibana ([https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana))
    forms the frontend of the ELK stack. It is used for data visualization. It is
    merely a log data dashboard. It is helpful in visualizing the trends and patterns
    in data that are otherwise tedious to read and interpret. It also provides an
    option to share/save, which makes visualization of the data more useful.
  prefs: []
  type: TYPE_NORMAL
- en: Zipkin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zipkin ([https://zipkin.io/](https://zipkin.io/)) is a distributed tracing system.
    Microservice architecture always has latency problems, and a system is needed
    to troubleshoot the latency problem. Zipkin helps to solve the problem by collecting
    timing data. Zipkin also helps to search the data.
  prefs: []
  type: TYPE_NORMAL
- en: All registered services will report timing data to Zipkin. Zipkin creates a
    dependency diagram based on the received traced requests for each of the applications
    or services. Then, it can be used to analyze, spot an application that takes a
    long time to resolve, and fix it as needed.
  prefs: []
  type: TYPE_NORMAL
- en: When a request is made, the trace instrumentation will record tags, add the
    trace headers to the request, and finally record the timestamp. Then, the request
    is sent to the original destination and the response is sent back to the trace
    instrumentation, which then records the duration and shares the result with the
    Zipkin collector, which is responsible for storing the information.
  prefs: []
  type: TYPE_NORMAL
- en: By default, JHipster will generate the application with Zipkin disabled, but
    this can be enabled in the application-`<env>.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a microservice architecture, we need to monitor our services continuously
    and any issues should cause alerts immediately. We need a separate service that
    will continuously monitor and alert us whenever something weird happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prometheus consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus server, which is responsible for scraping and storing the time series
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries to instrument the application code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A push gateway for supporting short-lived jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An exporter to Grafana to visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An alert manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other support tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus is an alternative to JHipster console. It provides monitoring and
    alerting support. This requires running a Prometheus server separately for more
    information. To get started with Prometheus, visit [https://prometheus.io/.](https://prometheus.io/)
  prefs: []
  type: TYPE_NORMAL
- en: It provides multi-dimensional data models, which are time series and are identified
    by metric name and key-value pair. It has a flexible dynamic query language. It
    supports pulling time series out of the box and pushing time series via an intermediary
    gateway. It has multiple modes of graphing and dashboard support.
  prefs: []
  type: TYPE_NORMAL
- en: It is helpful in finding out problems when there is an outage. Since it is autonomous
    and does not depend on any remote services, the data is sufficient for finding
    where the infrastructure is broken.
  prefs: []
  type: TYPE_NORMAL
- en: It is helpful in recording the time series data and monitoring either via machine
    or highly dynamic Service Oriented Architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things to consider when choosing Prometheus over JHipster Console are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is very good at exploiting the metrics of your application and will
    not monitor logs or traces. JHipster console, on the other hand, uses the ELK
    stack and monitors the logs, traces, and metrics of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus can be used to query a huge amount of time series data. ELK on JHipster
    console is much more versatile in terms of tracking and searching the metrics
    and logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JHipster console uses Kibana to visualize the data while Prometheus uses Grafana ([https://grafana.com/](https://grafana.com/))
    to visualize the metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JHipster UAA server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JHipster user accounting and authorizing (UAA) services are merely an OAuth2
    server that can be used for *centralized identity management*. In order to access
    the protected resource and also to avoid unwanted access to the APIs, there has
    to be an authorization server that authorizes the request and provides access
    to the resource.
  prefs: []
  type: TYPE_NORMAL
- en: OAuth2 is an authorization framework that provides access to the request based
    on tokens. Clients request access to a service; if the user is authorized, the
    application receives an authorization grant. After receiving the grant, the client
    requests a token from the authorization server. Once the token is received, the
    client will then request the resource server gets the necessary information.
  prefs: []
  type: TYPE_NORMAL
- en: JHipster supports both standard LDAP protocols and is invoked via JSON APIs.
  prefs: []
  type: TYPE_NORMAL
- en: JHipster UAA is a user accounting and authorizing service for securing JHipster
    microservices using the OAuth2 authorization protocol.
  prefs: []
  type: TYPE_NORMAL
- en: JHipster UAA is a JHipster generated application consisting of user and role
    management. It also has a full-fledged OAuth2 authorization server. This is flexible
    and completely customizable.
  prefs: []
  type: TYPE_NORMAL
- en: Security is essential in a microservice architecture. The following are the
    basic requirements for securing microservices.
  prefs: []
  type: TYPE_NORMAL
- en: They should be authenticated in one place. Users should experience the entire
    experience as a single unit. Once the end user logs in to the application, they
    should be able to access whatever they have access to. They should hold session-related
    information throughout the time they are logged in to the system.
  prefs: []
  type: TYPE_NORMAL
- en: The security service should be stateless. Irrespective of the service, the security
    service should be capable of providing authentication for requests.
  prefs: []
  type: TYPE_NORMAL
- en: They also need to have the ability to provide authentication to machines and
    users. They should be able to distinguish them and trace them. Their function
    should be authorizing the incoming request rather than identifying the end user.
  prefs: []
  type: TYPE_NORMAL
- en: Since the underlying services are scalable, security services should also have
    the ability to scale up and down based on requirements.
  prefs: []
  type: TYPE_NORMAL
- en: They should, of course, be safe from attacks. Any known vulnerability should
    be fixed and updated as and when required.
  prefs: []
  type: TYPE_NORMAL
- en: The previous requirements were satisfied by using the OAuth2 protocol.
  prefs: []
  type: TYPE_NORMAL
- en: JHipster UAA is a centralized server that helps to authenticate and authorize
    users. They also have session-related information and the role-based access control
    with the help of a user and role management that is available inside the system.
  prefs: []
  type: TYPE_NORMAL
- en: The OAuth2 protocol, in general, provides the token for authenticating based
    on the details provided, which makes them stateless and able to authenticate a
    request from any source.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen the benefits of a microservice architecture over monolithic
    applications, and the components that we need to run a microservice application
    such as JHipster Registry, Consul, Zuul, Zipkin, the ELK stack, Hystrix, Prometheus,
    and the JHipster UAA server. In our next chapter, we will see how to build microservices
    using JHipster. We will also learn how we can choose the previous components and
    how easy it is to set them up with JHipster.
  prefs: []
  type: TYPE_NORMAL
