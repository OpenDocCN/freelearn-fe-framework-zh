<html><head></head><body>
		<div id="_idContainer119">
			<h1 id="_idParaDest-181"><em class="italic"><a id="_idTextAnchor204"/>Chapter 12</em>: Continuous Deployment with CircleCI and AWS</h1>
			<p>In the last two chapters, we prepared our application through tests with Mocha. We have built an application that is ready for the production environment.</p>
			<p>We will now generate a production build that's ready for deployment. We've arrived at the point where we can set up <strong class="bold">Amazon Elastic Container Service </strong>(<strong class="bold">Amazon ECS</strong>) and implement the ability to build and deploy Docker images through a continuous deployment workflow.</p>
			<p>The process of continuous deployment will help to keep changes small for the production environment. Keeping changes in your application continuous and small will make issues trackable and fixable, whereas releasing a set of multiple features at once will leave the location for bugs open for investigation as multiple things will have changed with just one release.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Production-ready bundling</li>
				<li>What is Docker?</li>
				<li>Configuring Docker</li>
				<li>Setting up AWS RDS (short for <strong class="bold">AWS Relational Database Service</strong>) as a production database</li>
				<li>What is continuous integration/deployment?</li>
				<li>Setting up continuous deployment with CircleCI</li>
				<li>Deploying our application to <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">Amazon ELB</strong>) and ECS with AWS <strong class="bold">Application Load Balancer</strong> (<strong class="bold">ALB</strong>)</li>
			</ul>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor205"/>Technical requirements</h1>
			<p>The source code for this chapter is available in the following GitHub repository:</p>
			<p><a href="https://github.com/PacktPublishing/Full-Stack-Web-Development-with-GraphQL-and-React-2nd-Edition/tree/main/Chapter12">https://github.com/PacktPublishing/Full-Stack-Web-Development-with-GraphQL-and-React-2nd-Edition/tree/main/Chapter12</a></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor206"/>Preparing the final production build</h1>
			<p>We have come a long way to get here. Now is the time when we should look at how we currently run <a id="_idIndexMarker903"/>our application and how we should prepare it for a production environment.</p>
			<p>Currently, we use our application in a development environment while working on it. It is not highly optimized for performance or low-bandwidth usage. We include developer functionalities with the code so that we can debug it properly.</p>
			<p>For use in a real production environment, we should only include what is necessary for the user. When setting the <strong class="source-inline">NODE_ENV</strong> variable to <strong class="source-inline">production</strong>, we remove most of the unnecessary development mechanics.</p>
			<p>By bundling our server-side code, we will get rid of unnecessary loading times and will improve the performance. To bundle our backend code, we are going to set up a new webpack configuration file. Follow these instructions:</p>
			<ol>
				<li>Install the following two dependencies:<p class="source-code">npm install --save-dev webpack-node-externals @babel/plugin-transform-runtime</p><p>These packages do the following:</p><ul><li>The <strong class="source-inline">webpack-node-externals</strong> package gives you the option to exclude specific modules while bundling your application with webpack. It reduces the final bundle size.</li><li>The <strong class="source-inline">@babel/plugin-transform-runtime</strong> package is a small plugin that enables us to reuse Babel's helper methods, which usually get inserted into every processed file. It reduces the final bundle size.</li></ul></li>
				<li>Create a <strong class="source-inline">webpack.server.build.config.js</strong> file next to the other webpack files with <a id="_idIndexMarker904"/>the following content:<p class="source-code">const path = require('path');</p><p class="source-code">const nodeExternals = require('webpack-node-externals');</p><p class="source-code">const buildDirectory = 'dist/server';</p><p class="source-code">module.exports = {</p><p class="source-code">  mode: 'production',</p><p class="source-code">  entry: [</p><p class="source-code">    './src/server/index.js'</p><p class="source-code">  ],</p><p class="source-code">  output: {</p><p class="source-code">    path: path.join(__dirname, buildDirectory),</p><p class="source-code">    filename: 'bundle.js',</p><p class="source-code">    publicPath: '/server'</p><p class="source-code">  },</p><p class="source-code">  module: {</p><p class="source-code">    rules: [{</p><p class="source-code">      test: /\.js$/,</p><p class="source-code">      use: {</p><p class="source-code">        loader: 'babel-loader',</p><p class="source-code">        options: {</p><p class="source-code">          plugins: ["@babel/plugin-transform-runtime"]</p><p class="source-code">        }</p><p class="source-code">      },</p><p class="source-code">    }],</p><p class="source-code">  },</p><p class="source-code">  node: {</p><p class="source-code">    __dirname: false,</p><p class="source-code">    __filename: false,</p><p class="source-code">  },</p><p class="source-code">  target: 'node',</p><p class="source-code">  externals: [nodeExternals()],</p><p class="source-code">  plugins: [],</p><p class="source-code">};</p><p>The preceding configuration file is very simple and not complex. Let's go through <a id="_idIndexMarker905"/>the settings that we use to configure webpack:</p><ul><li>We load our new <strong class="source-inline">webpack-node-externals</strong> package at the top.</li><li>The <strong class="source-inline">build</strong> directory, where we save the bundle, is in the <strong class="source-inline">dist</strong> folder, inside of a special <strong class="source-inline">server</strong> folder.</li><li>The <strong class="source-inline">mode</strong> field is set to <strong class="source-inline">'production'</strong>.</li><li>The <strong class="source-inline">entry</strong> point for webpack is the server's root <strong class="source-inline">index.js</strong> file.</li><li>The <strong class="source-inline">output</strong> property holds the standard fields to bundle our code and saves it inside of the folder specified through the <strong class="source-inline">buildDirectory</strong> variable.</li><li>We use the previously installed <strong class="source-inline">@babel/plugin-transform-runtime</strong> plugin in the <strong class="source-inline">module</strong> property to reduce the file size for our bundle.</li><li>Inside of the <strong class="source-inline">node</strong> property, you can set Node.js-specific configuration options. The <strong class="source-inline">__dirname</strong> field tells webpack that the global <strong class="source-inline">__dirname</strong> variable is used with its default settings and is not customized by webpack. The same goes for the <strong class="source-inline">__filename</strong> property.</li><li>The <strong class="source-inline">target</strong> field accepts multiple environments in which the generated bundle should work. For our case, we set it to <strong class="source-inline">'node'</strong>, as we want to run our backend in Node.js.</li><li>The <strong class="source-inline">externals</strong> property gives us the possibility to exclude specific dependencies from our bundle. By using the <strong class="source-inline">webpack-node-externals</strong> package, we prevent all <strong class="source-inline">node_modules</strong> packages from being included in our bundle.</li></ul></li>
				<li>To make use <a id="_idIndexMarker906"/>of our new build configuration file, we add two new commands to the <strong class="source-inline">scripts</strong> field of our <strong class="source-inline">package.json</strong> file. As we are trying to generate a final production build that we can publicize, we have to build our client-side code in parallel. Add the following two lines to the <strong class="source-inline">scripts</strong> field of the <strong class="source-inline">package.json</strong> file:<p class="source-code">"build": "npm run client:build &amp;&amp; npm run server:build",</p><p class="source-code">"server:build": "webpack --config webpack.server.build.config.js"</p><p>The <strong class="source-inline">build</strong> command uses the <strong class="source-inline">&amp;&amp;</strong> syntax to chain two <strong class="source-inline">npm run</strong> commands. It executes the build process for our client-side code first, and afterward, it bundles the entire server-side code. The result is that we have a filled <strong class="source-inline">dist</strong> folder with a <strong class="source-inline">client</strong> folder and a <strong class="source-inline">server</strong> folder. Both can import components dynamically.</p></li>
				<li>To start our server with the new production code, we are going to add one further command to the <strong class="source-inline">scripts</strong> field. The old <strong class="source-inline">npm run server</strong> command would start the server-side code in the unbundled version, which is not what we want. Insert the following line into the <strong class="source-inline">package.json</strong> file:<p class="source-code">"server:production": "node dist/server/bundle.js"</p><p>The preceding command simply executes the <strong class="source-inline">bundle.js</strong> file from the <strong class="source-inline">dist/server</strong> folder, using the plain <strong class="source-inline">node</strong> command to launch our backend.</p><p>Now, you should be able to generate your final build by running <strong class="source-inline">npm run build</strong>. Before starting the production server as a test, however, make sure that you have set all environment variables for your database correctly, or your <strong class="source-inline">JWT_SECRET</strong>, for example. Then, you can execute the <strong class="source-inline">npm run server:production</strong> command to launch the backend.</p></li>
				<li>Our tests need to be run in a way to reflect the same production conditions, because only then can we verify that all features that are enabled in the live environment <a id="_idIndexMarker907"/>work correctly. To make sure that is true, we need to change how we execute the tests. Edit the <strong class="source-inline">test</strong> command of the <strong class="source-inline">package.json</strong> file to reflect this change, as follows:<p class="source-code">"test": "npm run build &amp;&amp; mocha --exit test/ --require babel-hook --require @babel/polyfill --recursive",</p><p>Now, you should be able to test your application with the same generated production bundles.</p></li>
			</ol>
			<p>In the next section, we will cover how to use Docker to bundle your entire application.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor207"/>Setting up Docker</h1>
			<p>Publishing an application is a critical step that requires a lot of work and care. Many things can go <a id="_idIndexMarker908"/>wrong when releasing a new version. We have already made sure that we can test our application before it goes live.</p>
			<p>The real act of transforming our local files into a production-ready package, which is then uploaded to a server, is the most onerous task. Regular applications generally rely on a server that is preconfigured with all the packages that the application needs to run. For example, when looking at a standard PHP setup, most people rent a preconfigured server. This means that the PHP runtime, with all the extensions, such as the MySQL PHP library, are installed via the built-in package manager of the operating system. This procedure applies not only to PHP but also to nearly any other programming language. This might be okay for general websites or applications that are not too complex, but for professional software development or deployment, this process can lead to issues, such as the following:</p>
			<ul>
				<li>The configuration needs to be done by someone that knows the requirements of the application and the server itself.</li>
				<li>A second server needs the same configuration in order to allow our application to run. While doing that configuration, we must ensure that all servers are standardized and consistent with one another.</li>
				<li>All of the servers have to be reconfigured when the runtime environment gets an update, either because the application requires it, or due to other reasons, such as security updates. In this case, everything must be tested again.</li>
				<li>Multiple <a id="_idIndexMarker909"/>applications running inside of the same server environment may require different package versions or may interfere with each other.</li>
				<li>The deployment process must be executed by someone with the required knowledge.</li>
				<li>Starting an application directly on a server exposes it to all the services running on the server. Other processes could take over your complete application since they run within the same environment.</li>
				<li>Also, the application is not limited to using a specified maximum of the server's resources.</li>
			</ul>
			<p>Many people have tried to figure out how to avoid these consequences by introducing a new containerization and deployment workflow.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor208"/>What is Docker?</h2>
			<p>One of the most important pieces of software is called Docker. It was released in 2013, and its function is to isolate the application within a container by offering its own runtime environment, <a id="_idIndexMarker910"/>without having access to the server itself.</p>
			<p>The aim of a container is to isolate the application from the operating system of the server.</p>
			<p>Standard virtual machines can also accomplish this by running a guest operating system for the application. Inside of the virtual machine, all packages and runtimes can be installed to prepare it for your application. This solution comes with significant overhead, of course, because we are running a second operating system that's just for our application. It is not scalable when many services or multiple applications are involved.</p>
			<p>On the other hand, Docker containers work entirely differently. The application itself, and all of its dependencies, receive a segment of the operating system's resources. All processes are isolated by the host system inside of those resources.</p>
			<p>Any server <a id="_idIndexMarker911"/>supporting the container runtime environment (which is Docker) can run your dockerized application. The great thing is that the actual operating system is abstracted away. Your operating system will be very slim, as nothing more than the kernel and Docker is required.</p>
			<p>With Docker, the developer can specify how the container image is composed. They can directly test and deploy those images on their infrastructure.</p>
			<p>To see the process and advantages that Docker provides, we are going to build a container image that includes our application and all the dependencies it needs to run.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor209"/>Installing Docker</h2>
			<p>As with any <a id="_idIndexMarker912"/>virtualization software, Docker has to be installed via the regular package manager of your operating system. </p>
			<p>I will assume that you are using a Debian-based system. If this is not the case, please get the <a id="_idIndexMarker913"/>correct instructions for your system at <a href="https://docs.docker.com/install/overview/">https://docs.docker.com/install/overview/</a>.</p>
			<p>Continue with the following instructions to get Docker up and running:</p>
			<ol>
				<li value="1">Update your system's package manager, as follows:<p class="source-code"><strong class="bold">sudo apt-get update</strong></p></li>
				<li>We can install the Docker package to our system, as follows:<p class="source-code"><strong class="bold">sudo apt-get install docker</strong></p><p>If you are running an Ubuntu version with snap installed, you can also use the following command:</p><p class="source-code"><strong class="bold">sudo snap install docker</strong></p></li>
			</ol>
			<p>That's everything that is required to get a working copy of Docker on your system.</p>
			<p>Next, you will learn how to use Docker by building your first Docker container image.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor210"/>Dockerizing your application</h2>
			<p>Many companies have adopted Docker and replaced their old infrastructure setup, thereby largely <a id="_idIndexMarker914"/>reducing system administration. Still, there is some work to do before deploying your application straight to production.</p>
			<p>One primary task is to <a id="_idIndexMarker915"/>dockerize your application. The term <strong class="bold">dockerize</strong> means that you take care of wrapping your application inside of a valid Docker container.</p>
			<p>There are many service providers that connect Docker with CI or continuous deployment because they work well together. In the last section of this chapter, you will learn what continuous deployment is and how it can be implemented. We are going to rely on such a service provider. It will provide us with an automatic workflow for our continuous deployment process. Let's first start dockerizing our application.</p>
			<h3>Writing your first Dockerfile</h3>
			<p>The conventional <a id="_idIndexMarker916"/>approach to generating a Docker image of your application is to create a <strong class="source-inline">Dockerfile</strong> at the root of your project. But what is a <strong class="source-inline">Dockerfile</strong> for?</p>
			<p>A <strong class="source-inline">Dockerfile</strong> is a series <a id="_idIndexMarker917"/>of commands that are run through the Docker <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>). The typical workflow in such a file looks <a id="_idIndexMarker918"/>as follows:</p>
			<ol>
				<li value="1">A <strong class="source-inline">Dockerfile</strong> starts from a base image, which is imported using the <strong class="source-inline">FROM</strong> command. This base image may include a runtime environment, like Node.js, or other things that your project can make use of. The container images are downloaded <a id="_idIndexMarker919"/>from Docker Hub, which is a central container registry that you can find at <a href="https://hub.docker.com/">https://hub.docker.com/</a>. There is the option to download the images from custom registries, too.</li>
				<li>Docker offers many commands to interact with the image and your application code. Those <a id="_idIndexMarker920"/>commands can be looked up at <a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a>.</li>
				<li>After the configuration of the image has finished and all build steps are complete, you will need to provide a command that will be executed when your application's Docker container starts.</li>
				<li>The result <a id="_idIndexMarker921"/>of the build steps will be a new Docker image (see <em class="italic">Figure 12.1</em>). The image is saved on the machine where it was generated.</li>
				<li>Optionally, you can now publish your new image to a registry, where other applications or users can pull your image. You can also upload them as private images or private registries.</li>
			</ol>
			<p>We will start by generating a simple Docker image. First, create the <strong class="source-inline">Dockerfile</strong> inside of the root of your project. The filename is written without any file extensions.</p>
			<p>The first task is to find a matching base image that we can use for our project. The criteria by which we choose a base image are the dependencies and runtime environment. As we have mainly used Node.js without relying on any other server-side package that needs <a id="_idIndexMarker922"/>to be covered from our Docker container, we only need to find a base image that provides Node.js. For the moment, we will ignore the database, and we'll focus on it again in a later step. </p>
			<p>Docker Hub is the official container image registry, providing many minimalistic images. Just insert the following line inside of our new <strong class="source-inline">Dockerfile</strong>, in the root of our project:</p>
			<p class="source-code">FROM node:14</p>
			<p>As we mentioned before, we use the <strong class="source-inline">FROM</strong> command to download our base image. As the name of the preceding image states, it includes Node.js in version 14. There are numerous other versions that you can use. Beyond the different versions, you can also find different flavors (for example, a Node.js image based on an Alpine Linux image). Take a look at the image's <strong class="source-inline">README</strong> to get an overview of the available options at <a href="https://hub.docker.com/_/node/">https://hub.docker.com/_/node/</a>.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">I recommend that you read through the reference documentation of the <strong class="source-inline">Dockerfile</strong>. Many advanced commands and scenarios are explained there, which will help you to customize your Docker workflow. Just go to <a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a>.</p>
			<p>After Docker has run the <strong class="source-inline">FROM</strong> command, you will be working directly within this base image, and all further commands will then run inside of this environment. You can access all features that the underlying operating system provides. Of course, the features are limited by the image that you have chosen. A <strong class="source-inline">Dockerfile</strong> is only valid if it starts with the <strong class="source-inline">FROM</strong> command.</p>
			<p>The next step <a id="_idIndexMarker923"/>for our <strong class="source-inline">Dockerfile</strong> is to create a new folder, in which the application will be stored and run. Add the following line to the <strong class="source-inline">Dockerfile</strong>:</p>
			<p class="source-code">WORKDIR /usr/src/app</p>
			<p>The <strong class="source-inline">WORKDIR</strong> command changes the directory to the specified path. The path that you enter lives inside of the filesystem of the image, which does not affect your computer's filesystem. From then on, the <strong class="source-inline">RUN</strong>, <strong class="source-inline">CMD</strong>, <strong class="source-inline">ENTRYPOINT</strong>, <strong class="source-inline">COPY</strong>, and <strong class="source-inline">ADD</strong> Docker commands will be executed in the new working directory. Furthermore, the <strong class="source-inline">WORKDIR</strong> command will create a new folder if it does not exist yet.</p>
			<p>Next, we need to get our application's code inside of the new folder. Until now, we have only made sure that the base image was loaded. The image that we are generating does not include our application yet. Docker provides a command to move our code into the final image.</p>
			<p>As the third line of our <strong class="source-inline">Dockerfile</strong>, add the following code:</p>
			<p class="source-code">COPY . .</p>
			<p>The <strong class="source-inline">COPY</strong> command accepts two parameters. The first one is the source, which can be a file or folder. The second parameter is the destination path inside of the image's filesystem. You can use a subset of regular expressions to filter the files or folders that you copy.</p>
			<p>After Docker has executed the preceding command, all contents living in the current directory will be copied over to the <strong class="source-inline">/usr/src/app</strong> path. The current directory, in this case, is the root of our project folder. All files are now automatically inside of the final Docker image. You can interact with the files through all Docker commands but also with the commands that the shell provides.</p>
			<p>One important task is that we install all <strong class="source-inline">npm</strong> packages that our application relies on. When running the <strong class="source-inline">COPY</strong> command, such as in the preceding code, all files and folders are transferred, including the <strong class="source-inline">node_modules</strong> folder. This could lead to problems when trying to run the application, however. Many <strong class="source-inline">npm</strong> packages are compiled when they are being installed, or they differentiate between operating systems. We must make sure that the packages that we use are clean, and work in the environment that we want them to work in. We must do two things to accomplish this, as follows:</p>
			<ol>
				<li value="1">Create a <strong class="source-inline">.dockerignore</strong> file in the root of the project folder, next to the <strong class="source-inline">Dockerfile</strong>, and enter the following content:<p class="source-code">node_modules</p><p>The <strong class="source-inline">.dockerignore</strong> file is comparable to the <strong class="source-inline">.gitignore</strong> file, which excludes special files <a id="_idIndexMarker924"/>or folders from being tracked by Git. Docker reads the <strong class="source-inline">.dockerignore</strong> file before all files are sent to the Docker daemon. If it is able to read a valid <strong class="source-inline">.dockerignore</strong> file, all specified files and folders are excluded. The preceding two lines exclude the whole <strong class="source-inline">node_modules</strong> folder.</p></li>
				<li>Install the <strong class="source-inline">npm</strong> packages inside of Docker. Add the following line of code to the <strong class="source-inline">Dockerfile</strong>:<p class="source-code"><strong class="bold">RUN npm install</strong></p><p>The <strong class="source-inline">RUN</strong> command executes <strong class="source-inline">npm install</strong> inside of the current working directory. The related <strong class="source-inline">package.json</strong> file and <strong class="source-inline">node_modules</strong> folder are stored in the filesystem of the Docker image. Those files are directly committed and are included in the final image. Docker's <strong class="source-inline">RUN</strong> command sends the command that we pass as the first parameter into Bash and executes it. To avoid the problems of spaces in the shell commands, or other syntax problems, you can pass the command as an array of strings, which will be transformed by Docker into valid Bash syntax. Through <strong class="source-inline">RUN</strong>, you can interact with other system-level tools (such as <strong class="source-inline">apt-get</strong> or <strong class="source-inline">curl</strong>).</p><p>Now that all files and dependencies are in the correct filesystem, we can start Graphbook from our new Docker image. Before doing so, there are two things that we need to do – we have to allow for external access to the container via the IP and define what the container should do when it has started.</p></li>
				<li>Graphbook uses port <strong class="source-inline">8000</strong> by default, under which it listens for incoming requests, be it a GraphQL or normal web request. When running a Docker container, it receives its own network, with an IP and ports. We must make port <strong class="source-inline">8000</strong> available to the public, not only inside of the container itself. Insert the following line at the end of the <strong class="source-inline">Dockerfile</strong> to make the port accessible from outside of the container:<p class="source-code">EXPOSE 8000</p><p>It is essential <a id="_idIndexMarker925"/>that you understand that the <strong class="source-inline">EXPOSE</strong> command does not map the inner port <strong class="source-inline">8000</strong> from the container to the matching port of our working machine. By writing the <strong class="source-inline">EXPOSE</strong> command, you give the developer using the image the option to publish port <strong class="source-inline">8000</strong> to any port of the real machine running the container. The mapping is done while starting the container, not when building the image. Later in this chapter, we will look at how to map port <strong class="source-inline">8000</strong> to a port of your local machine.</p></li>
				<li>Finally, we have to tell Docker what our container should do once it has booted. In our case, we want to start our backend (including SSR, of course). Since this should be a simple example, we will start the development server.<p>Add the last line of the <strong class="source-inline">Dockerfile</strong>, as follows:</p><p class="source-code">CMD [ "npm", "run", "server" ]</p><p>The <strong class="source-inline">CMD</strong> command defines the way that our container is booted, and which command to run. We are using the <strong class="source-inline">exec</strong> option of Docker to pass an array of strings. A <strong class="source-inline">Dockerfile</strong> can only have one <strong class="source-inline">CMD</strong> command. The <strong class="source-inline">exec</strong> format does not run a Bash or shell command when using <strong class="source-inline">CMD</strong>.</p><p>The container executes the <strong class="source-inline">server</strong> script of our <strong class="source-inline">package.json</strong> file, which has been copied into the Docker image.</p></li>
			</ol>
			<p>At this point, everything is finished and prepared to generate a basic Docker image. Next, we will continue with getting a container up and running.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor211"/>Building and running Docker containers</h2>
			<p>The <strong class="source-inline">Dockerfile</strong> and <strong class="source-inline">.dockerignore</strong> files are ready. Docker provides us with the tools to generate <a id="_idIndexMarker926"/>a real image, which we can run or share with others. Having a <strong class="source-inline">Dockerfile</strong> on its own does not make an application dockerized.</p>
			<p>Make sure <a id="_idIndexMarker927"/>that the database credentials specified in the <strong class="source-inline">/server/config/index.js</strong> file for the backend are valid for development because they are statically saved there. Furthermore, the MySQL host must allow for remote connections from inside the container.</p>
			<p>Execute the following command to build the Docker image on your local machine:</p>
			<p class="source-code">docker build -t sgrebe/graphbook .</p>
			<p>This command requires you to have the Docker CLI and daemon installed.</p>
			<p>The first option that we use is <strong class="source-inline">-t</strong>, following a string (in our case, <strong class="source-inline">sgrebe/graphbook</strong>). The finished build will be saved under the username <strong class="source-inline">sgrebe</strong> and the application name <strong class="source-inline">graphbook</strong>. This text is also called a <strong class="source-inline">tag</strong>. The only required parameter of the <strong class="source-inline">docker build</strong> command is the build context or the set of files that Docker will use for the container. We specified the current directory as the build context by adding the dot at the end of the command. Furthermore, the <strong class="source-inline">build</strong> action expects the <strong class="source-inline">Dockerfile</strong> to be located within this folder. If you want the file to be taken from somewhere else, you can specify it with the <strong class="source-inline">--file</strong> option.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If the <strong class="source-inline">docker build</strong> command fails, it may be that some environment variables are missing. They usually include the IP and port of the Docker daemon. To look them up, execute the <strong class="source-inline">docker-machine env</strong> command and set the environment variables as returned by the command.</p>
			<p>When the command has finished generating the image, it should be available locally. To prove this, you can use the Docker CLI by running the following command:</p>
			<p class="source-code">docker images</p>
			<p>The output from Docker should look as follows:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/Figure_12.01_B17337.jpg" alt="Figure 12.1 – Docker images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Docker images</p>
			<p>You should see two containers; the first one is the <strong class="source-inline">sgrebe/graphbook</strong> container image, or whatever you used as a tag name. The second one should be the <strong class="source-inline">node</strong> image, which we used as the base for our custom Docker image. The size of the custom image should be much higher because we installed all <strong class="source-inline">npm</strong> packages.</p>
			<p>Now, we should <a id="_idIndexMarker928"/>be able to start our Docker container with this <a id="_idIndexMarker929"/>new image. The following command will launch your Docker container:</p>
			<p class="source-code">docker run -p 8000:8000 -d --env-file .env sgrebe/graphbook</p>
			<p>The <strong class="source-inline">docker run</strong> command also has only one required parameter, which is the image to start the container with. In our case, this is <strong class="source-inline">sgrebe/graphbook</strong>, or whatever you specified as a tag name. Still, we define some optional parameters that we need to get our application working. You can find an explanation of each of them, as follows:</p>
			<ul>
				<li>We set the <strong class="source-inline">-p</strong> option to <strong class="source-inline">8000:8000</strong>. The parameter is used to map ports from the actual host operating system to a specific port inside of the Docker container. The first port is the port of the host machine, and the second one is the port of the container. This option gives us access to the exposed port <strong class="source-inline">8000</strong>, where the application is running under <strong class="source-inline">http://localhost:8000</strong> of our local machine.</li>
				<li>The <strong class="source-inline">--env-file</strong> parameter is required to pass environment variables to the container. Those can be used to hand over the <strong class="source-inline">NODE_ENV</strong> or <strong class="source-inline">JWT_SECRET</strong> variables, for example, which we require throughout our application. We will create this file in a second.</li>
				<li>You can also pass the environment variables one by one using the <strong class="source-inline">-e</strong> option. It is much easier to provide a file, however.</li>
				<li>The <strong class="source-inline">-d</strong> option sets the container to <strong class="bold">detached mode</strong>. This means that your container will not run in the foreground after executing it inside the shell. Instead, after running the command, you will have access to the shell again and will see no output from the container. If you remove the option again, you will see all of the logs that our application triggers.<p class="callout-heading">Important Note</p><p class="callout">The <strong class="source-inline">docker run</strong> command provides many more options. It allows for various advanced setups. The link <a id="_idIndexMarker930"/>to the official documentation is <a href="https://docs.docker.com/engine/reference/run/#general-form">https://docs.docker.com/engine/reference/run/#general-form</a>.</p></li>
			</ul>
			<p>Let's <a id="_idIndexMarker931"/>create the <strong class="source-inline">.env</strong> file in the root directory of our project. Insert the <a id="_idIndexMarker932"/>following content, replacing all placeholders with the correct value for every environment variable:</p>
			<p class="source-code">NODE_ENV=development</p>
			<p class="source-code">JWT_SECRET=YOUR_JWT_SECRET</p>
			<p class="source-code">AWS_ACCESS_KEY_ID=YOUR_AWS_KEY_ID</p>
			<p class="source-code">AWS_SECRET_ACCESS_KEY=YOUR_AWS_SECRET_ACCESS_KEY</p>
			<p>The <strong class="source-inline">.env</strong> file is a simple key-value list, where you can specify one variable per line, which our application can access from its environment variables.</p>
			<p>It is vital that you do not commit this file to the public at any stage. Please add this file directly to the <strong class="source-inline">.gitignore</strong> file.</p>
			<p>If you have filled out this file, you will be able to start the Docker container with the previous command that I showed you. Now that the container is running in detached mode, you will have the problem that you cannot be sure whether Graphbook has started to listen. Consequently, Docker also provides a command to test this, as follows:</p>
			<p class="source-code">docker ps</p>
			<p>The <strong class="source-inline">docker ps</strong> command gives you a list of all running containers. You should find the Graphbook container in there, too. The output should appear as follows:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/Figure_12.02_B17337.jpg" alt="12.2 – Docker running containers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">12.2 – Docker running containers</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Like all commands that Docker provides, the <strong class="source-inline">docker ps</strong> command gives us many options to <a id="_idIndexMarker933"/>customize and filter the output. Read up on all of the features that it offers in the official documentation at <a href="https://docs.docker.com/engine/reference/commandline/ps/">https://docs.docker.com/engine/reference/commandline/ps/</a>.</p>
			<p>Our container is running, and it uses the database that we have specified. You should be able to use Graphbook as you know it by visiting <strong class="source-inline">http://localhost:8000</strong>.</p>
			<p>If you take <a id="_idIndexMarker934"/>a look at the preceding figure, you will see that all running containers receive their own IDs. This ID can be used in various situations <a id="_idIndexMarker935"/>to interact with the container.</p>
			<p>In development, it makes sense to have access to the command-line printouts that our application generates. When running the container in detached mode, you have to use the Docker CLI to see the printouts, using the following command. Replace the ID at the end of the command with the ID of your container:</p>
			<p class="source-code">docker logs 08499322a998</p>
			<p>The <strong class="source-inline">docker logs</strong> command will show you all the printouts that have been made by our application or container recently. Replace the preceding ID with the one given to you by the <strong class="source-inline">docker ps</strong> command. If you want to see the logs in real time while using Graphbook, you can add the <strong class="source-inline">--follow</strong> option.</p>
			<p>As we are running the container in detached mode, you will not be able to stop it by just using <em class="italic">Ctrl</em> + <em class="italic">C</em> as before. Instead, you have to use the Docker CLI again.</p>
			<p>To stop the container again, run the following command:</p>
			<p class="source-code">docker stop 08499322a998</p>
			<p>To finally remove it, run the following command:</p>
			<p class="source-code">docker rm 08499322a998</p>
			<p>The <strong class="source-inline">docker rm</strong> command stops and removes the container from the system. Any changes made to the filesystem inside of the container will be lost. If you start the image again, a new container will be created, with a clean filesystem.</p>
			<p>When working and developing with Docker frequently, you will probably generate many images to test and verify the deployment of your application. These take up a lot of space on your local machine. To remove the images, you can execute the following command:</p>
			<p class="source-code">docker rmi fe30bceb0268</p>
			<p>The ID can <a id="_idIndexMarker936"/>be taken from the <strong class="source-inline">docker images</strong> command, the output of which you can see in the first image in this section. You can <a id="_idIndexMarker937"/>only remove an image if it is not used in a running container.</p>
			<p>We have come far. We have successfully dockerized our application. However, it is still running in development mode, so there is a lot to do.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor212"/>Multi-stage Docker production builds</h2>
			<p>Our current Docker image, which we are creating from the <strong class="source-inline">Dockerfile</strong>, is already useful. We want our application to be transpiled and running in production mode because many things <a id="_idIndexMarker938"/>are not optimized for the public when running in development mode.</p>
			<p>Obviously, we have to run our build scripts for the backend and frontend while generating the Docker image.</p>
			<p>Up until now, we have installed all <strong class="source-inline">npm</strong> packages and copied all files and folders for our project to the container image. This is fine for development because this image is not published or deployed to a production environment. When going live with your application, you will want your image to be as slim and efficient as possible. To achieve this, we will <a id="_idIndexMarker939"/>use a so-called <strong class="bold">multi-stage build</strong>.</p>
			<p>Before Docker implemented the functionality to allow for multi-stage builds, you had to rely on tricks, such as using shell commands to only keep the files that were really required in the container image. The problem that we have is that we copy all files that are used to build the actual distribution code from the project folder. Those files are not needed in the production Docker container, however.</p>
			<p>Let's see how this looks in reality. You can back up or remove the first <strong class="source-inline">Dockerfile</strong> that we wrote, as we will start with a blank one now. The new file still needs to be called <strong class="source-inline">Dockerfile</strong>. All the following lines of code go directly into this empty <strong class="source-inline">Dockerfile</strong>. Follow these instructions to get the multi-stage production build running:</p>
			<ol>
				<li value="1">Our new <a id="_idIndexMarker940"/>file starts with the <strong class="source-inline">FROM</strong> command again. We are going to have multiple <strong class="source-inline">FROM</strong> statements because we are preparing a multi-stage build. The first one should look as follows:<p class="source-code">FROM node:14 AS build</p><p>We are introducing the first build stage here. As before, we are using the <strong class="source-inline">node</strong> image in version 14. Furthermore, we append the <strong class="source-inline">AS build</strong> suffix, which tells Docker that this stage, and everything that we do in it, will be accessible under the name <strong class="source-inline">build</strong> later. A new stage is started with every new <strong class="source-inline">FROM</strong> command.</p></li>
				<li>Next, we initialize the working directory, as we did in our first <strong class="source-inline">Dockerfile</strong>, as follows:<p class="source-code">WORKDIR /usr/src/app</p></li>
				<li>It is essential to only copy the files that we really need. It hugely improves the performance if you reduce the number of files that need to be processed:<p class="source-code">COPY .babelrc ./</p><p class="source-code">COPY package*.json ./</p><p class="source-code">COPY webpack.server.build.config.js ./</p><p class="source-code">COPY webpack.client.build.config.js ./</p><p class="source-code">COPY src src</p><p class="source-code">COPY assets assets</p><p class="source-code">COPY public public</p><p>We copy the <strong class="source-inline">.babelrc</strong>, <strong class="source-inline">package.json</strong>, <strong class="source-inline">package-lock.json</strong>, and webpack files that are required for our application. These include all information we need to generate a production build for the frontend and backend. Furthermore, we also copy the <strong class="source-inline">src</strong>, <strong class="source-inline">public</strong>, and <strong class="source-inline">assets</strong> folders, because they include the code and CSS that will be transpiled and bundled.</p></li>
				<li>Like in our first <strong class="source-inline">Dockerfile</strong>, we must install all <strong class="source-inline">npm</strong> packages; otherwise, our application won't work. We do this with the following line of code:<p class="source-code">RUN npm install</p></li>
				<li>After all the packages have been installed successfully, we can start the build process. We added the <strong class="source-inline">build</strong> script in the first section of this chapter. Add the following <a id="_idIndexMarker941"/>line to execute the script that will generate the production bundles in the Docker image:<p class="source-code">RUN npm run build</p><p>The following command will generate a <strong class="source-inline">dist</strong> folder for us, where the runnable code (including CSS) will be stored. After the <strong class="source-inline">dist</strong> folder with all bundles has been created, we will no longer need most of the files that we initially copied over to the current build stage.</p></li>
				<li>To get a clean Docker image that only contains the <strong class="source-inline">dist</strong> folder and the files that we need to run the application, we will introduce a new build stage that will generate the final image. The new stage is started with a second <strong class="source-inline">FROM</strong> statement, as follows:<p class="source-code">FROM node:14</p><p>We are building the final image in this build step; therefore, it does not need its own name.</p></li>
				<li>Again, we need to specify the working directory for the second stage, as the path is not copied from the first build stage:<p class="source-code">WORKDIR /usr/src/app</p></li>
				<li>Before continuing, we need to ensure that the application has access to all environment variables. For that, add the following lines to the <strong class="source-inline">Dockerfile</strong>:<p class="source-code">ENV NODE_ENV production</p><p class="source-code">ENV JWT_SECRET JWT_SECRET</p><p class="source-code">ENV username YOUR_USERNAME</p><p class="source-code">ENV password YOUR_PASSWORD</p><p class="source-code">ENV database YOUR_DATABASE</p><p class="source-code">ENV host YOUR_HOST</p><p class="source-code">ENV AWS_ACCESS_KEY_ID AWS_ACCESS_KEY_ID</p><p class="source-code">ENV AWS_SECRET_ACCESS_KEY AWS_SECRET_ACCESS_KEY</p><p>We use the <strong class="source-inline">ENV</strong> command from Docker to fill the environment variables while building the image.</p></li>
				<li>Because we have given our first build stage a name, we can access the filesystem <a id="_idIndexMarker942"/>of this stage through that name. To copy the files from the first stage, we can add a parameter to the <strong class="source-inline">COPY</strong> statement. Add the following commands to the <strong class="source-inline">Dockerfile</strong>:<p class="source-code">COPY --from=build /usr/src/app/package.json package.json</p><p class="source-code">COPY --from=build /usr/src/app/dist dist</p><p class="source-code">COPY start.sh start.sh</p><p class="source-code">COPY src/server src/server</p><p>As you should see in the preceding code, we are copying the <strong class="source-inline">package.json</strong> file and the <strong class="source-inline">dist</strong> folder. However, instead of copying the files from our original project folder, we are getting those files directly from the first build stage. For this, we use the <strong class="source-inline">--from</strong> option, following the name of the stage that we want to access; so, we enter the name <strong class="source-inline">build</strong>. The <strong class="source-inline">package.json</strong> file is needed because it includes all dependencies and the <strong class="source-inline">scripts</strong> field, which holds the information on how to run the application in production. The <strong class="source-inline">dist</strong> folder is, of course, our bundled application.</p><p>Furthermore, we copy a <strong class="source-inline">start.sh</strong> file that we will create and the server folder because in there we have all the database migrations.</p></li>
				<li>Note that we only copy the <strong class="source-inline">package.json</strong> file and the <strong class="source-inline">dist</strong> folder. Our <strong class="source-inline">npm</strong> dependencies are not included in the application build inside of the <strong class="source-inline">dist</strong> folder. As a result, we need to install the <strong class="source-inline">npm</strong> packages in the second build stage, too:<p class="source-code"><strong class="bold">RUN npm install --only=production</strong></p><p>The production image should only hold <strong class="source-inline">npm</strong> packages that are really required; <strong class="source-inline">npm</strong> offers the <strong class="source-inline">only</strong> parameter, which lets you install only the production packages, as an example. It will exclude all <strong class="source-inline">devDependecies</strong> of your <strong class="source-inline">package.json</strong> file. This is really great for keeping your image size low.</p><p>Then, there <a id="_idIndexMarker943"/>are three <strong class="source-inline">npm</strong> packages that are technically not a dependency, which is defined in our <strong class="source-inline">package.json</strong> file, because they are not required to get our application running. Still, they are needed to get our database migrations applied. Add the following <strong class="source-inline">RUN</strong> command to the <strong class="source-inline">Dockerfile</strong>:</p><p class="source-code"><strong class="bold">RUN npm install -g mysql2 sequelize sequelize-cli</strong></p><p>The above command will install Sequelize, the Sequelize CLI, and the <strong class="source-inline">mysql2</strong> package. We will leverage them to apply migrations at the start of the Docker container. There are also further ways to trigger them manually and not at the start of a Docker container, but this will work for our setup.</p></li>
				<li>The last two things to do here are to expose the container port to the public and to execute the <strong class="source-inline">CMD</strong> command, which will let the image run a command of our <strong class="source-inline">package.json</strong> file when the container has booted:<p class="source-code"><strong class="bold">EXPOSE 8000</strong></p><p class="source-code"><strong class="bold">CMD [ "sh", "start.sh" ]</strong></p></li>
				<li>Lastly, we need to create a <strong class="source-inline">start.sh</strong> file at the root of the project with the following content:<p class="source-code">sequelize db:migrate --migrations-path src/server/migrations --config src/server/config/index.js --env production</p><p class="source-code">npm run server:production</p><p>In the <strong class="source-inline">start.sh</strong> file, we have two lines. The first one runs all database migrations. The last one starts the server based on the generated production bundle.</p></li>
			</ol>
			<p>Now, you can execute the <strong class="source-inline">docker build</strong> command again and try to start the container. There is only one problem – the database credentials are read from the environment variables when <a id="_idIndexMarker944"/>running in production. As the production setup for a database cannot be on our local machine, it needs to live somewhere on a real server. We could also accomplish this through Docker, but this would involve a very advanced Docker configuration. We would need to save the MySQL data in separate storage because Docker does not persist data of any kind, by default.</p>
			<p>Personally, I like to rely on a cloud host, which handles the database setup for me. It is not only great for the overall setup but also improves the scalability of our application. The next section will cover Amazon RDS and how to configure it for use with our application. You can use any database infrastructure that you like.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor213"/>Amazon RDS</h1>
			<p>AWS offers <a id="_idIndexMarker945"/>Amazon <strong class="bold">RDS</strong>, which is an easy tool for setting up a relational database in just a few clicks. Shortly, I will <a id="_idIndexMarker946"/>explain how to create your first database with RDS, and afterward, we will look at how to insert environment variables correctly in order to get a database connection going with our application.</p>
			<p>The first step is to log in to the AWS Management Console, as we did in <a href="B17337_07_Final_ASB_ePub.xhtml#_idTextAnchor154"><em class="italic">Chapter 7</em></a>, <em class="italic">Handling Image Uploads</em>. You can find the service by clicking on the <strong class="bold">Services</strong> tab in the top bar and searching for <strong class="source-inline">RDS</strong>.</p>
			<p>After navigating to <strong class="bold">RDS</strong>, you will see the dashboard, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Figure_12.03_B17337.jpg" alt="Figure 12.3 – AWS RDS&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – AWS RDS</p>
			<p>Follow <a id="_idIndexMarker947"/>these instructions to set the RDS database up:</p>
			<ol>
				<li value="1">Initialize a new database by hitting the <strong class="bold">Create database</strong> button. You will be presented <a id="_idIndexMarker948"/>with a new screen, where you should select an engine for our new database and how to create it, as shown in the following screenshot:<div id="_idContainer078" class="IMG---Figure"><img src="image/Figure_12.04_B17337.jpg" alt="Figure 12.4 – AWS RDS Engine selection&#13;&#10;"/></div><p class="figure-caption">Figure 12.4 – AWS RDS Engine selection</p><p>I recommend that you select <strong class="bold">MySQL</strong> here. You should also be able to select <strong class="bold">Amazon Aurora</strong> or <strong class="bold">MariaDB</strong>, as they are also MySQL compatible; for this book, I have chosen MySQL. Also, stay with the standard creation method. Continue by scrolling down.</p></li>
				<li>You will need to specify the use case for your database. The production option is only <a id="_idIndexMarker949"/>recommended for live applications because this will include higher costs. Choose the free tier, as shown in the following screenshot:<div id="_idContainer079" class="IMG---Figure"><img src="image/Figure_12.05_B17337.jpg" alt="Figure 12.5 – AWS RDS Templates selection&#13;&#10;"/></div><p class="figure-caption">Figure 12.5 – AWS RDS Templates selection</p></li>
				<li>Continue <a id="_idIndexMarker950"/>by scrolling down. Next, you need to fill in the database credentials to authenticate on the backend later. Fill in the details, as shown here: <div id="_idContainer080" class="IMG---Figure"><img src="image/Figure_12.06_B17337.jpg" alt="Figure 12.6 – AWS RDS database credentials&#13;&#10;"/></div><p class="figure-caption">Figure 12.6 – AWS RDS database credentials</p></li>
				<li>Next, you need to select the AWS instance and, by that, the computing power of the database. Select <strong class="bold">db.t2.micro</strong>, which is free and enough for our use case now. It should look like as follows:<div id="_idContainer081" class="IMG---Figure"><img src="image/Figure_12.07_B17337.jpg" alt="Figure 12.7 – AWS RDS instance class&#13;&#10;"/></div><p class="figure-caption">Figure 12.7 – AWS RDS instance class</p></li>
				<li>You will <a id="_idIndexMarker951"/>now be asked for connectivity settings. It is important that you select <strong class="bold">Public access</strong>, with <strong class="bold">Yes</strong> checked. This does not share your database with the public but makes it accessible from other IPs and other EC2 instances if you select them in your AWS security group. Also, you need to create a new subnet group and give a new security group name:<p class="figure-caption"> </p><div id="_idContainer082" class="IMG---Figure"><img src="image/Figure_12.08_B17337.jpg" alt="Figure 12.8 – AWS RDS network settings&#13;&#10;"/></div><p class="figure-caption">Figure 12.8 – AWS RDS network settings</p></li>
				<li>In the <strong class="bold">Additional configuration</strong> section, we need to provide an initial database <a id="_idIndexMarker952"/>name, which will create a database inside RDS after finishing the setup:<div id="_idContainer083" class="IMG---Figure"><img src="image/Figure_12.09_B17337.jpg" alt="Figure 12.9 – The Additional configuration window&#13;&#10;"/></div><p class="figure-caption">Figure 12.9 – The Additional configuration window</p></li>
				<li>Finish the setup process for your first AWS RDS database by clicking on <strong class="bold">Create database</strong> at the bottom of the screen.</li>
			</ol>
			<p>You <a id="_idIndexMarker953"/>should now be redirected to the list of all databases.</p>
			<p>Click on the new database instance that has been created. If you scroll down, you will see a list of security groups. Click on the group with the <strong class="bold">CIDR/IP - Inbound</strong> type:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/Figure_12.10_B17337.jpg" alt="Figure 12.10 – AWS security group rules&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.10 – AWS security group rules</p>
			<p>If you click on the first rule, you will be able to insert the IP that is allowed to access the database. If you insert the <strong class="source-inline">0.0.0.0</strong> IP, it will allow any remote IP to access the database. This is not a recommended database setup for production use, but it makes it easier to test it with multiple environments in developmental use.</p>
			<p>The credentials that you have specified for the database must be included in the <strong class="source-inline">.env</strong> file for running our Docker container, as follows:</p>
			<p class="source-code">username=YOUR_USERNAME</p>
			<p class="source-code">password=YOUR_PASSWORD</p>
			<p class="source-code">database=YOUR_DATABASE</p>
			<p class="source-code">host=YOUR_HOST</p>
			<p>The <strong class="source-inline">host</strong> URL can be taken from the Amazon RDS instance dashboard. It should look something like <strong class="source-inline">INSTANCE_NAME.xxxxxxxxxx.eu-central-1.rds.amazonaws.com</strong>.</p>
			<p>Now, you should <a id="_idIndexMarker954"/>be able to run the build for your Docker image again, without any problems. The database has been set up and is available.</p>
			<p>Next, we will look at how we can automate the process of generating the Docker image through continuous integration.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor214"/>Configuring continuous integration</h1>
			<p>Many people (especially developers) will <a id="_idIndexMarker955"/>have heard of <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) or <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>). However, most of them cannot <a id="_idIndexMarker956"/>explain their meanings and the differences <a id="_idIndexMarker957"/>between the two terms. So, what is CI and CD, in reality?</p>
			<p>When it comes to releasing your application, it might seem easy to upload some files to a server and then start the application through a simple command in the shell, via SSH.</p>
			<p>This approach might be a solution for many developers or small applications that are not updated often. However, for most scenarios, it is not a good approach. The word <em class="italic">continuous</em> represents the fact that all changes or updates are continuously either tested, integrated, or even released. This would be a lot of work, and it would be tough to do if we stayed with a simple file upload and took a manual approach. Automating this workflow makes it convenient to update your application at any time.</p>
			<p>CI is the development practice where all developers commit their code to the central project repository at least once a day to bring their changes to the mainline stream of code. The integrated code will be verified by automated test cases. This will avoid problems when trying to go live at a specific time.</p>
			<p>CD goes further; it's based on the main principles of CI. Every time the application is successfully built and tested, the changes are directly released to the customer. This is what we are going to implement.</p>
			<p>Our automation process will be based on <strong class="bold">CircleCI</strong>. It is a third-party service offering a CI and CD platform, with a massive number of features.</p>
			<p>To sign up for <a id="_idIndexMarker958"/>CircleCI, visit <a href="https://circleci.com/signup/">https://circleci.com/signup/</a>.</p>
			<p>You will need to have a Bitbucket or GitHub account in order to sign up. This will also be the source from which the repositories of your application will be taken, for which we can begin using CI or CD.</p>
			<p>To get your <a id="_idIndexMarker959"/>project up and running with CircleCI, you will need to click on the <strong class="bold">Projects</strong> button in the left-hand panel, or you will be redirected there because you have no projects set up yet. After signing up, you should see all your repositories inside of CircleCI.</p>
			<p>Select the project that you want to process with CircleCI by hitting <strong class="bold">Set Up Project</strong> on the right-hand side of the project. You will then be confronted with the following screen:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/Figure_12.11_B17337.jpg" alt="Figure 12.11 – CircleCI Projects&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.11 – CircleCI Projects</p>
			<p>The problem is that you have not configured your repository or application accordingly. You are required to create a folder called <strong class="source-inline">.circleci</strong> and a file inside of it, called <strong class="source-inline">config.yml</strong>, which tells CircleCI what to do when a new commit is pushed to the repository. CircleCI will either ask you to set it up yourself or it will do it for you. I recommend selecting that we are going to do it on our own, as this book guides you through the steps.</p>
			<p>Next, set <strong class="bold">sample config</strong> as <strong class="bold">Node</strong>. The final step will be to push a new commit with the matching CircleCI config.</p>
			<p>We will create <a id="_idIndexMarker960"/>a straightforward CircleCI configuration so that we can test that everything is working. The final configuration will be done at a later step when we have configured Amazon ECS, which will be the host of our application.</p>
			<p>So, create a <strong class="source-inline">.circleci</strong> folder at the root of our project and a <strong class="source-inline">config.yml</strong> file inside of this new folder. The <strong class="source-inline">.yml</strong> file extension stands for <strong class="bold">YAML</strong>, which is a file format for saving various configurations or data. What is important here is that all <strong class="source-inline">.yml</strong> files need a correct indentation. Otherwise, they will not be valid files and cannot be understood by CircleCI.</p>
			<p>Insert the following code into the <strong class="source-inline">config.yml</strong> file:</p>
			<p class="source-code">version: 2.1</p>
			<p class="source-code">jobs:</p>
			<p class="source-code">  build:</p>
			<p class="source-code">    docker:</p>
			<p class="source-code">      - image: circleci/node:14</p>
			<p class="source-code">    steps:</p>
			<p class="source-code">      - checkout</p>
			<p class="source-code">      - run:</p>
			<p class="source-code">          command: echo "This is working"</p>
			<p>Let's quickly go through all the steps in the file, as follows:</p>
			<ol>
				<li value="1">The file starts with a <strong class="source-inline">version</strong> specification. We are using version 2.1, as this is the current version of CircleCI.</li>
				<li>Then, we will have a list of <strong class="source-inline">jobs</strong> that get executed in parallel. As we only have one thing that we want to do, we can only see the <strong class="source-inline">build</strong> job that we are running. Later, we will add the whole Docker build and publish the functionality here.</li>
				<li>Each job receives an executor type, which needs to be <strong class="source-inline">machine</strong>, <strong class="source-inline">docker</strong>, or <strong class="source-inline">macos</strong>. We are using the <strong class="source-inline">docker</strong> type because we can rely on many prebuilt images of CircleCI. The image is specified in a separate <strong class="source-inline">image</strong> property. There, I have specified <strong class="source-inline">node</strong> in version 14, because we need Node.js for our CI workflow.</li>
				<li>Each job <a id="_idIndexMarker961"/>then receives several steps that are executed with every commit that is pushed to the Git repository.</li>
				<li>The first step is the <strong class="source-inline">checkout</strong> command, which clones the current version of our repository so that we can use it in any further steps.</li>
				<li>Lastly, to test that everything has worked, we use the <strong class="source-inline">run</strong> step. It lets us execute a command directly in the Docker <strong class="source-inline">node:14</strong> image that we have started with CircleCI. Each command that you want to execute must be prefixed with <strong class="source-inline">command</strong>.</li>
			</ol>
			<p>The result of this config file should be that we have pulled the current master branch of our application and printed the text <strong class="source-inline">This is working</strong> at the end. To test the CircleCI setup, commit and push this file to your GitHub or Bitbucket repository.</p>
			<p>CircleCI should automatically notify you that it has started a new <strong class="bold">CI</strong> job for our repository. You can find the job by hitting the <strong class="bold">Jobs</strong> button in the left-hand panel of CircleCI. The newest job should be at the top of the list. Click on the job to see the details. They should look as follows:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/Figure_12.12_B17337.jpg" alt="Figure 12.12 – CircleCI pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.12 – CircleCI pipeline</p>
			<p>In the preceding <a id="_idIndexMarker962"/>screenshot, each step is represented in a separate row at the bottom of the window. You can expand each row to see the logs that are printed while executing the specific command shown in the current row. The preceding screenshot shows that the job has been successful.</p>
			<p>Now that we have configured CircleCI to process our repository on each push, we must take a look at how to host and deploy our application directly, after finishing the build.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor215"/>Deploying applications to Amazon ECS</h1>
			<p>CircleCI executes our build steps each time we push a new commit. Now, we want to build our <a id="_idIndexMarker963"/>Docker image and deploy it automatically <a id="_idIndexMarker964"/>to a machine that will serve our application to the public.</p>
			<p>Our database and uploaded images are hosted on AWS already, so we can also use AWS to serve our application. Setting up AWS correctly is a significant task, and it takes a large amount of time. We will use Amazon ECS to run our Docker image. Still, to correctly set up the network, security, and container registry is too complex to be explained in just one chapter. I recommend that you take a course or pick up a separate book to understand and learn advanced setups with AWS, and the configuration that is needed to get production-ready hosting. For now, we will use ECS to get the container, including the database connection, running.</p>
			<p>Before directly <a id="_idIndexMarker965"/>going to Amazon ECS and creating <a id="_idIndexMarker966"/>your cluster, we need to prepare two services – one is AWS <strong class="bold">ALB</strong>, which stands for <strong class="bold">Application Load Balancer</strong>, and the other is Amazon ECR. If you set up an ECS cluster, there will be one or more instances <a id="_idIndexMarker967"/>of the same service or, to be exact, task running. Those tasks need to receive traffic, but if you release a new version, they should also be exchanged with new tasks while still serving the traffic. That is a good job for AWS ALB, as it can split traffic between the instances and handle dynamic port mapping if tasks are exchanged.</p>
			<p>Log in to the AWS Management Console, search for the EC2 service, and click on it. Then, follow these instructions:</p>
			<ol>
				<li value="1">Scroll down on the left panel until you see the <strong class="bold">Load Balancing</strong> section, as shown in the following screenshot:<div id="_idContainer087" class="IMG---Figure"><img src="image/Figure_12.13_B17337.jpg" alt="Figure 12.13 – AWS Load Balancing section&#13;&#10;"/></div><p class="figure-caption">Figure 12.13 – AWS Load Balancing section</p></li>
				<li>Click on <strong class="bold">Load Balancers</strong> and you will see an empty list. We need to click on <strong class="bold">Create Load Balancer</strong> in the top left corner now. On the next page, select <strong class="bold">Application Load Balancer</strong>.</li>
				<li>We need to specify a name for our load balancer and a scheme. We will go with the <strong class="bold">Internet-facing</strong> option because that way we can access it from outside AWS:<div id="_idContainer088" class="IMG---Figure"><img src="image/Figure_12.14_B17337.jpg" alt="Figure 12.14 – AWS ALB configuration&#13;&#10;"/></div><p class="figure-caption">Figure 12.14 – AWS ALB configuration</p><p class="callout-heading">Important Note</p><p class="callout">Normally, you <a id="_idIndexMarker968"/>would not make the load balancer public but instead add another <strong class="bold">Content Delivery Network</strong> (<strong class="bold">CDN</strong>), cache, and firewall <a id="_idIndexMarker969"/>in front of your application to protect it from <strong class="bold">Distributed Denial of Service </strong>(<strong class="bold">DDoS</strong>) attacks or unnecessary load. Services on AWS such as Route53 and CloudFront work well together to accomplish this, but they are out of the scope of this book.</p></li>
				<li>In the <a id="_idIndexMarker970"/>next step, we need to select a VPC that <a id="_idIndexMarker971"/>will bring the attached resources into one private network. There should be an existing one from the database that you can select. Please do that and select the availability zones, as shown in the following screenshot:<div id="_idContainer089" class="IMG---Figure"><img src="image/Figure_12.15_B17337.jpg" alt="Figure 12.15 – AWS ALB network settings&#13;&#10;"/></div><p class="figure-caption">Figure 12.15 – AWS ALB network settings</p></li>
				<li>Next, you <a id="_idIndexMarker972"/>need to select the security <a id="_idIndexMarker973"/>group that we have created with the AWS RDS database, as shown in the following screenshot:<div id="_idContainer090" class="IMG---Figure"><img src="image/Figure_12.16_B17337.jpg" alt="Figure 12.16 – AWS ALB Security groups&#13;&#10;"/></div><p class="figure-caption">Figure 12.16 – AWS ALB Security groups</p></li>
				<li>If you scroll further down, you can see that you need to specify the routing for your load balancer, that is, where the load or traffic needs to go, depending on some rules that you need to define. To do that, we need to define a target group. There should be a link reading <strong class="bold">Create </strong><strong class="bold">target group</strong> below the selection <a id="_idIndexMarker974"/>input, as shown in the following screenshot:<div id="_idContainer091" class="IMG---Figure"><img src="image/Figure_12.17_B17337.jpg" alt="Figure 12.17 – AWS ALB routing&#13;&#10;"/></div><p class="figure-caption">Figure 12.17 – AWS ALB routing</p><p>If you <a id="_idIndexMarker975"/>have created and selected the target group, it should look like the preceding figure, so let's do it.</p></li>
				<li>Open the link in a separate tab or window; as mentioned before, you should see the following screen:<div id="_idContainer092" class="IMG---Figure"><img src="image/Figure_12.18_B17337.jpg" alt="Figure 12.18 – AWS target group creation&#13;&#10;"/></div><p class="figure-caption">Figure 12.18 – AWS target group creation</p></li>
				<li>Select <strong class="bold">Instances</strong>, which <a id="_idIndexMarker976"/>means load <a id="_idIndexMarker977"/>balancing between different instances in one VPC. Give the target group a name and select the same VPC as before.</li>
				<li>After that, you can hit the <strong class="bold">Next</strong> button. You will be presented with the following screen:<div id="_idContainer093" class="IMG---Figure"><img src="image/Figure_12.19_B17337.jpg" alt="Figure 12.19 – AWS target group targets&#13;&#10;"/></div><p class="figure-caption">Figure 12.19 – AWS target group targets</p><p>This screen normally shows you all instances that would be included in your target group. Because we did not create the ECS cluster yet, this is empty. You can continue by hitting the <strong class="bold">Create target group</strong> button.</p></li>
				<li>Go back <a id="_idIndexMarker978"/>to the wizard to set up AWS ALB. Hit <a id="_idIndexMarker979"/>the <strong class="bold">Refresh</strong> button next to the target group selection and select the target group.</li>
				<li>Hit the <strong class="bold">Create load balancer</strong> button at the end of the screen.</li>
			</ol>
			<p>Finally, you should have reached the end of the wizard and be presented with a summary, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/Figure_12.20_B17337.jpg" alt="Figure 12.20 – AWS ALB creation summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.20 – AWS ALB creation summary</p>
			<p>The next <a id="_idIndexMarker980"/>thing we need to prepare is an Amazon <a id="_idIndexMarker981"/>ECR repository. Amazon ECR is nothing more than an alternative to Docker Hub or any other Docker registry. In a Docker registry, you can push the Docker images that you built for your application. This is the basis on which our ECS cluster will run.</p>
			<p>To set up your ECR repository, search for <strong class="source-inline">ECR</strong> in the top bar and click on the corresponding service. You should be presented with the following screen:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/Figure_12.21_B17337.jpg" alt="Figure 12.21 – Amazon ECR overview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.21 – Amazon ECR overview</p>
			<p>To set <a id="_idIndexMarker982"/>up your Amazon ECR repository, follow <a id="_idIndexMarker983"/>these steps:</p>
			<ol>
				<li value="1">Click on <strong class="bold">Create repository</strong> on the right side to get into the creation wizard.</li>
				<li>Next, you need to provide a name for your repository, as shown in the following screenshot:<div id="_idContainer096" class="IMG---Figure"><img src="image/Figure_12.22_B17337.jpg" alt="Figure 12.22 – Amazon ECR settings&#13;&#10;"/></div><p class="figure-caption">Figure 12.22 – Amazon ECR settings</p><p>We leave <a id="_idIndexMarker984"/>our registry private because <a id="_idIndexMarker985"/>no one external should have access to it. If you need to have it public for everyone, you can change this setting.</p></li>
				<li>Click <strong class="bold">Create repository</strong> to set it up. You will see the following update table now:<div id="_idContainer097" class="IMG---Figure"><img src="image/Figure_12.23_B17337.jpg" alt="Figure 12.23 – Amazon ECR repository created&#13;&#10;"/></div><p class="figure-caption">Figure 12.23 – Amazon ECR repository created</p></li>
				<li>Later, you will need the <strong class="source-inline">URI</strong>, which is shown next to the registry name.</li>
			</ol>
			<p>Now we <a id="_idIndexMarker986"/>are prepared to start setting up the ECS cluster. To find ECS, just go to the services bar at the top and search for <strong class="source-inline">ECS</strong>. If you click <a id="_idIndexMarker987"/>this service and go to the <strong class="bold">Clusters</strong> section, it will show you all the running ECS clusters. It should look like the following:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/Figure_12.24_B17337.jpg" alt="Figure 12.24 – Amazon ECS Clusters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.24 – Amazon ECS Clusters</p>
			<p>The process to <a id="_idIndexMarker988"/>configure ECS is very complex, and we will follow the most basic configuration in the scope of this book. Follow these instructions to get it working:</p>
			<ol>
				<li value="1">Hit the <strong class="bold">Create Cluster</strong> button (see <em class="italic">Figure 12.24</em>) to get started.</li>
				<li>On the next screen, you will be asked what kind of instances you want to use. We are going to use the EC2 Linux instances, as shown in the following screenshot:<div id="_idContainer099" class="IMG---Figure"><img src="image/Figure_12.25_B17337.jpg" alt="Figure 12.25 – Amazon ECS cluster template&#13;&#10;"/></div><p class="figure-caption">Figure 12.25 – Amazon ECS cluster template</p></li>
				<li>Click <strong class="bold">Next</strong> to get <a id="_idIndexMarker989"/>to the configuration wizard for your cluster.</li>
				<li>You will be asked for a cluster name. The default configuration is fine otherwise. The only option that requires a change is the instance type. I recommend going with <strong class="source-inline">t2.micro</strong> because it does not cost that much, which is good for development. The <strong class="bold">Number of instances</strong> option specifies how many parallel running EC2 instances we want. For development, mostly one is fine, but if you need to scale, this is something you need to increase:<div id="_idContainer100" class="IMG---Figure"><img src="image/Figure_12.26_B17337.jpg" alt="Figure 12.26 – Amazon ECS cluster configuration&#13;&#10;"/></div><p class="figure-caption">Figure 12.26 – Amazon ECS cluster configuration</p></li>
				<li>Scroll down to provide some further networking configuration. As we have selected <a id="_idIndexMarker990"/>three subnets when configuring the load balancer, we should also now select those three subnets:<div id="_idContainer101" class="IMG---Figure"><img src="image/Figure_12.27_B17337.jpg" alt="Figure 12.27 – Amazon ECS cluster network settings&#13;&#10;"/></div><p class="figure-caption">Figure 12.27 – Amazon ECS cluster network settings</p></li>
				<li>After selecting the subnets, you need to select the security group that we also used for the ALB configuration:<div id="_idContainer102" class="IMG---Figure"><img src="image/Figure_12.28_B17337.jpg" alt="Figure 12.28 – Amazon ECS Security settings&#13;&#10;"/></div><p class="figure-caption">Figure 12.28 – Amazon ECS Security settings</p></li>
				<li>Hit <strong class="bold">Create Cluster</strong> and AWS will start the process to spin everything up. This might take some time.</li>
				<li>Once AWS is done, you can click the <strong class="bold">View Cluster</strong> button, which will take you to the detailed cluster page.</li>
			</ol>
			<p>We have <a id="_idIndexMarker991"/>defined now that AWS is running an ECS cluster based on one EC2 instance. One thing we did not do so far is to define what this cluster does. For that, we need to go to <strong class="bold">Task definitions</strong> on the left-hand panel. Then, follow these instructions:</p>
			<ol>
				<li value="1">Click on the <strong class="bold">Create new task definition</strong> button.</li>
				<li>Select the <strong class="bold">EC2</strong> type to make your task compatible with this type of cluster.</li>
				<li>Give your task definition a name and the <strong class="bold">ecsTaskExecutionRole</strong> role. It should <a id="_idIndexMarker992"/>look like the following screenshot:<div id="_idContainer103" class="IMG---Figure"><img src="image/Figure_12.29_B17337.jpg" alt="Figure 12.29 – Amazon ECS task definition &#13;&#10;"/></div><p class="figure-caption">Figure 12.29 – Amazon ECS task definition </p></li>
				<li>Scroll down and give your task a size, which means a memory size and CPU size that it will need to process its task. It should look like the following screenshot:<div id="_idContainer104" class="IMG---Figure"><img src="image/Figure_12.30_B17337.jpg" alt="Figure 12.30 – Amazon ECS task definition size&#13;&#10;"/></div><p class="figure-caption">Figure 12.30 – Amazon ECS task definition size</p><p>This setting <a id="_idIndexMarker993"/>needs to be aligned with the resources your cluster has.</p></li>
				<li>Now, click the <strong class="bold">Add container</strong> button to define which types of containers will run inside of this ECS task:<div id="_idContainer105" class="IMG---Figure"><img src="image/Figure_12.31_B17337.jpg" alt="Figure 12.31 – Amazon ECS task definition container configuration&#13;&#10;"/></div><p class="figure-caption">Figure 12.31 – Amazon ECS task definition container configuration</p><p>In this <a id="_idIndexMarker994"/>dialog, you first need to provide a name for your container. Secondly, you need to provide the URI from the ECR registry that we created at the beginning. You need to append the tag for your image, which will be <strong class="source-inline">:latest</strong> for the moment. As we did not publish any image yet, it will not work anyway, but we will fix this later.</p></li>
				<li>Scroll down to the <strong class="bold">Port mappings</strong> section. Fill <strong class="bold">Container port</strong> with the number <strong class="source-inline">8000</strong> because this is the default port that we use for Graphbook. <strong class="bold">Host port</strong> needs to stay at <strong class="source-inline">0</strong>, as this will be automatically assigned by your load balancer. AWS ALB will dynamically map a free port to the container port. We do not need to care exactly which port it will be.</li>
				<li>Under the environment section, we need to add all environment variables that our application requires to start. The variables shown in the following screenshot should be enough to get your container running:<div id="_idContainer106" class="IMG---Figure"><img src="image/Figure_12.32_B17337.jpg" alt="Figure 12.32 – Amazon ECS task definition container environment variables&#13;&#10;"/></div><p class="figure-caption">Figure 12.32 – Amazon ECS task definition container environment variables</p></li>
				<li>The only <a id="_idIndexMarker995"/>setting that is helpful is under the <strong class="bold">Storage and logging</strong> section. I recommend activating CloudWatch Logs, as you can then see all the logs from your application. It should look like the following screenshot:<div id="_idContainer107" class="IMG---Figure"><img src="image/Figure_12.33_B17337.jpg" alt="Figure 12.33 – Amazon ECS task definition container logs&#13;&#10;"/></div><p class="figure-caption">Figure 12.33 – Amazon ECS task definition container logs</p></li>
				<li>After enabling this option, you can hit the <strong class="bold">Add</strong> button at the bottom of the dialog.</li>
			</ol>
			<p>These are all the important things that you need to do. There is a multitude of further detailed configurations that you can do. For us, they are not required to get our application running, and they are out of the scope of this book.</p>
			<p>The container definitions table should look like the following screenshot:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Figure_12.34_B17337.jpg" alt="Figure 12.34 – Amazon ECS task definition container definitions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.34 – Amazon ECS task definition container definitions</p>
			<p>What we just <a id="_idIndexMarker996"/>did is the most basic ECS setup that <a id="_idIndexMarker997"/>you can do. Across all the services that we just set up, we used the simplest configuration that you can do, but there is a tremendous amount of advanced setups and configurations we did not have a look at.</p>
			<p>Now, you can click on the <strong class="bold">Create</strong> button at the very end of the ECS task definition wizard. After AWS has successfully created the task definition, go back to the main <strong class="bold">Clusters</strong> screen of ECS. You will see this cluster overview:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/Figure_12.35_B17337.jpg" alt="Figure 12.35 – Amazon ECS cluster overview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.35 – Amazon ECS cluster overview</p>
			<p>It just shows you that, still, nothing is running inside your cluster. To fix that, click on your cluster name at the top left. </p>
			<p>The actual problem is that no service has been created that is using the just-configured task definition, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/Figure_12.36_B17337.jpg" alt="Figure 12.36 – Amazon ECS cluster services&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.36 – Amazon ECS cluster services</p>
			<p>Hit the <strong class="bold">Create</strong> button above the table.</p>
			<p>Then, you <a id="_idIndexMarker998"/>need to provide the following data:</p>
			<ol>
				<li value="1">For <strong class="bold">Launch type</strong>, you need to select <strong class="bold">EC2</strong>, as with the previous steps.</li>
				<li>Then, you <a id="_idIndexMarker999"/>need to select the task definition family, which should match the name of your previously created task definition.</li>
				<li>Also, you need to select the cluster that we are currently looking at.</li>
				<li>You need to provide a service name for your service, such as <strong class="source-inline">graphbook-service</strong>.</li>
				<li>For the <strong class="bold">Number of tasks</strong> option, the value <strong class="source-inline">1</strong> is fine. It means that only one task will run in this service at the same time. This is also restricted by the health percentage. The minimum health percent of <strong class="source-inline">100</strong> means that at least one service that is correctly functioning should be running, whereas the maximum percent of <strong class="source-inline">200</strong> means that only a maximum of two tasks that are healthy are allowed to be running. This restriction is required for the moment where you update your service with a new application version. At this moment, there will be two versions running at the same time that will be exchanged. So, currently, we have <strong class="source-inline">200</strong> health percent.</li>
			</ol>
			<p>That is all the information that you need to provide. The rest of the settings can be left as they are set by default. The result should look like the following screenshot:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/Figure_12.37_B17337.jpg" alt="Figure 12.37 – Amazon ECS service configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.37 – Amazon ECS service configuration</p>
			<p>You can <a id="_idIndexMarker1000"/>now hit the <strong class="bold">Next step</strong> button at the bottom <a id="_idIndexMarker1001"/>of the screen to continue.</p>
			<p>The next screen is very important because this requires our AWS ALB to be set up.</p>
			<p>You need to select the <strong class="bold">Application Load Balancer</strong> option and then select our previously created ALB. The settings should match the configuration in the following screenshot:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/Figure_12.38_B17337.jpg" alt="Figure 12.38 – Amazon ECS service load balancing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.38 – Amazon ECS service load balancing</p>
			<p>This setting <a id="_idIndexMarker1002"/>will make use of our ALB and do the dynamic port mapping to the container that is running within this service's tasks.</p>
			<p>If we scroll down, we need to provide details on how the ALB will map to the container. Now, you should see this message:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/Figure_12.39_B17337.jpg" alt="Figure 12.39 – Amazon ECS service load balancing mapping&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.39 – Amazon ECS service load balancing mapping</p>
			<p>You need to click the <strong class="bold">Add to load balancer</strong> button. The good thing is you can just select the target group that we created previously. After selecting it, you should see the following screen:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/Figure_12.40_B17337.jpg" alt="Figure 12.40 – Amazon ECS load balancing target group configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.40 – Amazon ECS load balancing target group configuration</p>
			<p>You can <a id="_idIndexMarker1003"/>now hit the <strong class="bold">Next step</strong> button at the end of the screen and on the next screen where it asks for auto-scaling, which we do not require. You should be able to click the <strong class="bold">Create Service</strong> option at the bottom of the summary screen.</p>
			<p>AWS will try to create the service now and spin up the tasks. The problem is that we did not push any Docker image so far. The ECS service will not be able to spin up any task correctly because of that.</p>
			<p>So, let's fix that.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor216"/>Setting up CircleCI with Amazon ECR and ECS</h2>
			<p>We will <a id="_idIndexMarker1004"/>start with a blank CircleCI config again; so, empty the old <strong class="source-inline">config.yml</strong> file.</p>
			<p>One <a id="_idIndexMarker1005"/>important thing we did not do so far is to set up automated testing within our pipeline. Otherwise, our commits will trigger an automated <a id="_idIndexMarker1006"/>pipeline and just deploy the untested <a id="_idIndexMarker1007"/>code, which might cause production issues that we want to prohibit.</p>
			<p>So, let's do this first. Follow these steps:</p>
			<ol>
				<li value="1">Insert these lines into our <strong class="source-inline">config.yml</strong> file, as follows:<p class="source-code">version: 2.1</p><p class="source-code">jobs:</p><p class="source-code">  test:</p><p class="source-code">    docker:</p><p class="source-code">      - image: circleci/node:14</p><p class="source-code">        auth:</p><p class="source-code">          username: $DOCKERHUB_USERNAME</p><p class="source-code">          password: $DOCKERHUB_PASSWORD</p><p class="source-code">        environment:</p><p class="source-code">          host: localhost</p><p class="source-code">          username: admin</p><p class="source-code">          password: passw0rd</p><p class="source-code">          database: graphbook</p><p class="source-code">          JWT_SECRET: 1234</p><p>This configuration creates a <strong class="source-inline">test</strong> job and pulls one Docker image. The Docker image is the Node.js image from CircleCI that we will run our application inside for testing purposes. At the same time, we pass credentials to actually pull the image, but we also pass some default environment variables that we will make use of in the next step.</p></li>
				<li>Add <a id="_idIndexMarker1008"/>another image to the <strong class="source-inline">test</strong> job:<p class="source-code">      - image: circleci/mysql:8.0.4</p><p class="source-code">        command: [--default-authentication-</p><p class="source-code">                  plugin=mysql_native_password]</p><p class="source-code">        auth:</p><p class="source-code">          username: $DOCKERHUB_USERNAME</p><p class="source-code">          password: $DOCKERHUB_PASSWORD</p><p class="source-code">        environment:</p><p class="source-code">          MYSQL_ROOT_PASSWORD: passw0rd</p><p class="source-code">          MYSQL_DATABASE: graphbook</p><p class="source-code">          MYSQL_USER: admin</p><p class="source-code">          MYSQL_PASSWORD: passw0rd</p><p>This <a id="_idIndexMarker1009"/>image is for the MySQL database, against <a id="_idIndexMarker1010"/>which we can run our <a id="_idIndexMarker1011"/>migrations but also test scripts. This will be created from scratch every time the pipeline runs. You can see here that we also provide the same environment variables. This will set up the MySQL database with these credentials and the Node.js container will have those credentials in the environment variables.</p></li>
				<li>As you can see in the preceding steps, we are using syntax such as <strong class="source-inline">$DOCKERHUB_USERNAME</strong> to inject variables from the CircleCI settings into our pipeline. That way, we do not need to repeat them repeatedly, but also, they are not committed into our code. Set up the environment variables according to the following screenshot:<div id="_idContainer115" class="IMG---Figure"><img src="image/Figure_12.41_B17337.jpg" alt="Figure 12.41 – CircleCI project environment variables&#13;&#10;"/></div><p class="figure-caption">Figure 12.41 – CircleCI project environment variables</p></li>
				<li>Now, we <a id="_idIndexMarker1012"/>need to add functionality to actually <a id="_idIndexMarker1013"/>run our tests. We will do this <a id="_idIndexMarker1014"/>inside a <strong class="source-inline">steps</strong> property that CircleCI is <a id="_idIndexMarker1015"/>able to understand. Just add the code below the previously added <strong class="source-inline">jobs</strong> section:<p class="source-code">    steps:</p><p class="source-code">      - checkout</p><p class="source-code">      - run: npm install</p><p>The flow of the test job is quite easy. First, we check out our code and then we install all dependencies that our application requires.</p></li>
				<li>Then, we also require the same Sequelize packages to run our database migrations as we had in our <strong class="source-inline">Dockerfile</strong>. Add the following code to do so:<p class="source-code">      - run:</p><p class="source-code">          name: "Install Sequelize"</p><p class="source-code">          command: sudo npm install -g mysql2 </p><p class="source-code">                   sequelize sequelize-cli</p></li>
				<li>Then, we <a id="_idIndexMarker1016"/>need to wait for the database <a id="_idIndexMarker1017"/>image to come up. If we do not <a id="_idIndexMarker1018"/>do that, when taking the next <a id="_idIndexMarker1019"/>steps, the commands will fail if the database is not yet running. Add the following code to wait for our database to come up:<p class="source-code">      - run:</p><p class="source-code">          name: Waiting for MySQL to be ready</p><p class="source-code">          command: |</p><p class="source-code">            for i in 'seq 1 10';</p><p class="source-code">              do</p><p class="source-code">                nc -z 127.0.0.1 3306 &amp;&amp; echo Success </p><p class="source-code">                  &amp;&amp; exit 0</p><p class="source-code">                echo -n .</p><p class="source-code">                sleep 1</p><p class="source-code">              done</p><p class="source-code">              echo Failed waiting for MySQL &amp;&amp; exit 1</p></li>
				<li>Once the database is up, we can run our database migrations against the <strong class="source-inline">test</strong> database. Add the following code to run them:<p class="source-code">      - run:</p><p class="source-code">          name: "Run migrations for test DB"</p><p class="source-code">          command: sequelize db:migrate </p><p class="source-code">            --migrations-path src/server/migrations </p><p class="source-code">            --config src/server/config/index.js </p><p class="source-code">            --env production</p></li>
				<li>Now, we can finally run our test against the freshly created database. Just use the below code to get it working:<p class="source-code">      - run:</p><p class="source-code">          name: "Run tests"</p><p class="source-code">          command: npm run test</p><p class="source-code">          environment:</p><p class="source-code">            NODE_ENV: production</p><p>If one <a id="_idIndexMarker1020"/>of the tests fails, the complete <a id="_idIndexMarker1021"/>pipeline will fail. This ensures <a id="_idIndexMarker1022"/>that only working application <a id="_idIndexMarker1023"/>code is released to the public.</p></li>
				<li>The last thing to do to get our automated tests running is to set up the CircleCI workflow. You can copy the following code to get it running:<p class="source-code">workflows:</p><p class="source-code">  build-and-deploy:</p><p class="source-code">    jobs:</p><p class="source-code">      - test</p></li>
			</ol>
			<p>You can commit and push this new config file into your Git repository, and CircleCI should automatically process it and create a new job for you.</p>
			<p>The resulting job should look as follows:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/Figure_12.42_B17337.jpg" alt="Figure 12.42 – CircleCI test pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.42 – CircleCI test pipeline</p>
			<p>Next, we <a id="_idIndexMarker1024"/>need to build the Docker image <a id="_idIndexMarker1025"/>and push this to our registry. Thanks <a id="_idIndexMarker1026"/>to CircleCI, this is quite easy.</p>
			<p>Add this <a id="_idIndexMarker1027"/>configuration to your <strong class="source-inline">config.yml</strong> file below the version specification:</p>
			<p class="source-code">orbs:</p>
			<p class="source-code">  aws-ecr: circleci/aws-ecr@7.2.0</p>
			<p>A CircleCI orb is a set of package configurations that you can share or make use of without needing to write all the steps on your own. This orb that we have just added can build and push a Docker image to Amazon ECR, which we set up in the previous section.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can find <a id="_idIndexMarker1028"/>all the available CircleCI orbs and their documentation on the official CircleCI orb website: <a href="https://circleci.com/developer/orbs">https://circleci.com/developer/orbs</a>.</p>
			<p>To leverage this orb, add one workflow step, as shown in the following screenshot:</p>
			<p class="source-code">- aws-ecr/build-and-push-image:</p>
			<p class="source-code">  repo: "graphbook"</p>
			<p class="source-code">  tag: "${CIRCLE_SHA1}"</p>
			<p class="source-code">  requires:</p>
			<p class="source-code">    - test</p>
			<p>The preceding configuration will build and push your Docker image to Amazon ECR, specified by the <strong class="source-inline">repo</strong> attribute. It will also wait for the <strong class="source-inline">test</strong> step because we have mentioned this in the <strong class="source-inline">requires</strong> property.</p>
			<p>If you commit and push this configuration, you will see that in the CircleCI pipeline, there is a <a id="_idIndexMarker1029"/>separate ECR step. If that is completed, you will be able to find a new Docker image inside the ECR repository.</p>
			<p>The only <a id="_idIndexMarker1030"/>thing missing now is to make use of this Docker image inside of Amazon ECS. If you remember, we specified the Docker <a id="_idIndexMarker1031"/>image inside of our Amazon ECS task <a id="_idIndexMarker1032"/>definition. Updating this manually after each pipeline run is not feasible though. To automate this process, add one further orb at the top of the CircleCI config:</p>
			<p class="source-code">aws-ecs: circleci/aws-ecs@02.2.1</p>
			<p>CircleCI also has us covered if we want to update and push a new task definition to our service. To leverage this orb, add this code as the last workflow step:</p>
			<p class="source-code">  - aws-ecs/deploy-service-update:</p>
			<p class="source-code">    requires:</p>
			<p class="source-code">      - aws-ecr/build-and-push-image</p>
			<p class="source-code">      - test</p>
			<p class="source-code">    family: "graphbook-task-definition"</p>
			<p class="source-code">    cluster-name: "graphbook-cluster"</p>
			<p class="source-code">    service-name: "graphbook-service"</p>
			<p class="source-code">    container-image-name-updates: "container=</p>
			<p class="source-code">      graphbook-container,tag=${CIRCLE_SHA1}"</p>
			<p>This step waits for both the test and the ECR job to complete. Afterward, it will create a new task definition revision on Amazon ECS with the given <strong class="source-inline">family</strong> name. It will then update the service with the given name inside the given cluster.</p>
			<p>Commit and push the new config file, and you will see the following pipeline with three jobs running:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/Figure_12.43_B17337.jpg" alt="Figure 12.43 – CircleCI CD pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.43 – CircleCI CD pipeline</p>
			<p>Amazon <a id="_idIndexMarker1033"/>ECS will take some time to replace the <a id="_idIndexMarker1034"/>currently running task, but after that, the new version of your application will be running.</p>
			<p>Still, the <a id="_idIndexMarker1035"/>question is how we can access Graphbook <a id="_idIndexMarker1036"/>now. For that, we can go to our AWS ALB, go to the <strong class="bold">Load Balancers</strong> section, and click our ALB. It will show the following information:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Figure_12.44_B17337.jpg" alt="Figure 12.44 – AWS ALB DNS name&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.44 – AWS ALB DNS name</p>
			<p>Under the given DNS name, we can access the load balancer and, via that, our application.</p>
			<p>As mentioned <a id="_idIndexMarker1037"/>previously, this is not recommended, but <a id="_idIndexMarker1038"/>the fully fledged AWS setup <a id="_idIndexMarker1039"/>is out of the scope of this book. You <a id="_idIndexMarker1040"/>should be able to access Graphbook under that link.</p>
			<p>That is all you need to do. It will test the application code with our test suite, build and push a new Docker image, and lastly update the task definition and ECS service to replace the old task with an updated task with the new task definition.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor217"/>Summary</h1>
			<p>In this chapter, you learned how to dockerize your application using a normal <strong class="source-inline">Dockerfile</strong> and a multi-stage build.</p>
			<p>Furthermore, I have shown you how to set up an exemplary CD workflow using CircleCI and AWS. You can replace the deployment process with a more complex setup while continuing to use your Docker image.</p>
			<p>Having read this chapter, you have learned everything from developing a complete application to deploying it to a production environment. Your application should now be running on Amazon ECS.</p>
			<p>At this point, you have learned all the important things, including setting up React with Webpack, developing a local setup, server-side rendering, and how to tie all the things together with GraphQL. Also, you are able to release changes frequently with CD. Looking ahead, there are still things that we can improve – for example, the scalability of our application or bundle splitting, which are not handled in the scope of this book, but there are many resources available that will help you improve in these areas. </p>
			<p>I hope you enjoyed the book and wish you every success!</p>
		</div>
	</body></html>