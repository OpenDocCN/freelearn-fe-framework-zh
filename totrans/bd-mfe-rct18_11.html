<html><head></head><body>
		<div id="_idContainer060" class="calibre2">
			<h1 id="_idParaDest-120" class="chapter-number"><a id="_idTextAnchor119" class="pcalibre1 calibre6 pcalibre"/>8</h1>
			<h1 id="_idParaDest-121" class="calibre7"><a id="_idTextAnchor120" class="pcalibre1 calibre6 pcalibre"/>Deploying Microfrontends to Kubernetes</h1>
			<p class="calibre3">In the previous chapter, we learned how to manually deploy our microfrontends to a static storage provider such <span>as Firebase.</span></p>
			<p class="calibre3">In this chapter, we will go deeper into cloud and DevOps territory by learning how to deploy our apps to a managed Kubernetes cluster. Kubernetes has become the de facto choice to deploy enterprise-grade web apps (both backend and frontend) to <span>the cloud.</span></p>
			<p class="calibre3">When it comes to deploying SPAs, we run usually the webpack <strong class="source-inline">build</strong> command to generate our JavaScript bundles and assets in the <strong class="source-inline">/build</strong> or <strong class="source-inline">/dist</strong> folder, which we then simply copy to a static website hosting provider to make our app available to our users. However, deploying microfrontends is a bit <span>more complex.</span></p>
			<p class="calibre3">In this chapter, we will see how to deploy our module-federated microfrontend to a managed <span>Kubernetes cluster.</span></p>
			<p class="calibre3">We will cover the <span>following topics:</span></p>
			<ul class="calibre15">
				<li class="calibre14">How to containerize our apps <span>using Docker</span></li>
				<li class="calibre14">The basics of Kubernetes and its <span>various components</span></li>
				<li class="calibre14">Some basic commands to manage our <span>Kubernetes cluster</span></li>
				<li class="calibre14">DevOps and how to automate deploying our micro-apps <span>to Kubernetes</span></li>
			</ul>
			<p class="calibre3">By the end of this chapter, we will have our microfrontend apps running on a Kubernetes cluster in Azure. We will deploy them via an automated <strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">Continuous Delivery</strong> (<strong class="bold">CD</strong>) pipeline that will automatically build and deploy the necessary apps whenever code <span>is merged.</span></p>
			<h1 id="_idParaDest-122" class="calibre7"><a id="_idTextAnchor121" class="pcalibre1 calibre6 pcalibre"/>Technical requirements</h1>
			<p class="calibre3">In addition to all the standard technical requirements that we mentioned in the previous chapters, you will need <span>the following:</span></p>
			<ul class="calibre15">
				<li class="calibre14">An Azure <span>cloud subscription</span></li>
				<li class="calibre14">Access to GitHub and <span>GitHub Actions</span></li>
				<li class="calibre14">A high-level understanding of CI and <span>CD concepts</span></li>
				<li class="calibre14">Knowledge of Docker and containerizing apps will <span>be helpful</span></li>
			</ul>
			<p class="calibre3">The code files for this chapter can be found at the following URL, where we essentially started with the microfrontend we built in <a href="B18987_06.xhtml#_idTextAnchor093" class="pcalibre1 calibre6 pcalibre"><span><em class="italic">Chapter </em></span><span><em class="italic">6</em></span></a><span>: </span><a href="https://github.com/PacktPublishing/Building-Micro-Frontends-with-React" class="pcalibre1 calibre6 pcalibre"><span>https://github.com/PacktPublishing/Building-Micro-Frontends-with-React</span></a><span>.</span></p>
			<p class="calibre3">We also assume you have a basic working knowledge of Git, such as branching committing code and raising a <span>pull request.</span></p>
			<h1 id="_idParaDest-123" class="calibre7"><a id="_idTextAnchor122" class="pcalibre1 calibre6 pcalibre"/>Introduction to Kubernetes</h1>
			<p class="calibre3"><strong class="bold">Kubernetes</strong>, also <a id="_idIndexMarker335" class="pcalibre1 calibre6 pcalibre"/>known as <strong class="bold">K8s</strong>, has taken the cloud and DevOps world by storm. Originally developed by Google and now part of the Cloud Native Computing Foundation, Kubernetes provides all the tools necessary to deploy and manage large-scale, mission-critical applications on the cloud from a <span>single interface.</span></p>
			<p class="calibre3">Traditionally, managing a large-scale, production-grade application on the cloud meant having to deal with things such as web servers, load balancers, auto-scaling, and internal and external traffic routing. Kubernetes now brings all of that under a single umbrella and provides a consistent way to manage all the components of a <span>cloud environment.</span></p>
			<p class="calibre3">The premise of Kubernetes is that you tell it the end state of what you want via a spec file, and Kubernetes will go about getting it done for you. For example, if you tell Kubernetes that you want three replicas for your application with a service load balancer, Kubernetes will figure out how to spin up the three replicas and ensure that the traffic is equally distributed between the three replicas. If, for some reason, one of the pods restarts or shuts down, Kubernetes will automatically spin up a new pod to ensure that, at any given time, three replicas of the pod service traffic. Similarly, when you deploy a new version of the app, Kubernetes will take over the responsibility of gradually spinning up new pods with the latest version of the app, while gracefully shutting down the pods with the older version of <span>the application.</span></p>
			<p class="calibre3">Through the rest of this section, we will look at some of the key components of Kubernetes that <a id="_idIndexMarker336" class="pcalibre1 calibre6 pcalibre"/>apply to us, along with the architecture to deploy our microfrontend <span>on Kubernetes.</span></p>
			<h2 id="_idParaDest-124" class="calibre5"><a id="_idTextAnchor123" class="pcalibre1 calibre6 pcalibre"/>What is Kubernetes?</h2>
			<p class="calibre3">Kubernetes is a platform-agnostic container orchestration platform that enables the deployment, scaling, and management of containerized applications in a cluster <span>of machines.</span></p>
			<p class="calibre3">It abstracts the underlying infrastructure, allowing you to run your applications in a variety of environments, including on-premises data centers, public cloud providers such as Microsoft Azure, Google Cloud Platform, and Amazon Web Services, and even on your <span>own laptop.</span></p>
			<p class="calibre3">Kubernetes is designed to be highly modular and extensible, and it integrates with a variety of tools and services to support the complete life cycle of an application, including deployment, scaling, monitoring, and maintenance. It is widely adopted in the industry and has become the <a id="_idIndexMarker337" class="pcalibre1 calibre6 pcalibre"/>de facto standard for <span>container orchestration.</span></p>
			<h2 id="_idParaDest-125" class="calibre5"><a id="_idTextAnchor124" class="pcalibre1 calibre6 pcalibre"/>Key concepts of Kubernetes</h2>
			<p class="calibre3">Kubernetes can be <a id="_idIndexMarker338" class="pcalibre1 calibre6 pcalibre"/>quite a vast topic and would need a dedicated area of focus to go deep into it. You can go into the details of the various components of Kubernetes here: <a href="https://kubernetes.io/docs/concepts/overview/components" class="pcalibre1 calibre6 pcalibre">https://kubernetes.io/docs/concepts/overview/components</a>. However, as a frontend engineer and for the scope of this book, there are six basic concepts and terms that you need to be <span>aware of:</span></p>
			<ul class="calibre15">
				<li class="calibre14"><strong class="bold">Nodes</strong>: A node is a worker machine in a Kubernetes cluster. It can be a physical or virtual machine, and it is responsible for running the containerized applications deployed <span>to it.</span></li>
				<li class="calibre14"><strong class="bold">Pods</strong>: A pod is the basic execution unit of a Kubernetes application. It is a logical host for one or more containers, as well as all containers in a pod run on the same node. Pods provide a shared context for containers, such as shared storage <span>and networking.</span></li>
				<li class="calibre14"><strong class="bold">Services</strong>: A service is a logical abstraction over a group of pods. It defines a policy to access the pods, typically via a stable IP address or DNS name. Services allow you to decouple the dependencies between your applications, enabling you to scale or update a group of pods without affecting the consumers of <span>the service.</span></li>
				<li class="calibre14"><strong class="bold">Deployments</strong>: A deployment is a declarative way to manage a ReplicaSet, which is a set of identical pods that are deployed to the cluster. Deployments allow you to specify the desired state of your application, and Kubernetes will ensure that the actual state matches the desired state. This includes rolling updates, rollbacks, <span>and self-healing.</span></li>
				<li class="calibre14"><strong class="bold">Ingress</strong>: Ingress is a way to expose your services to the external world. It provides a way to map external traffic to a specific service in your cluster, typically via a stable IP address or DNS name. Ingress can also provide additional features, such as SSL termination and load balancing. Think of it as a router where a URL is mapped to <span>a service.</span></li>
				<li class="calibre14"><strong class="bold">Namespaces</strong>: A namespace is a logical partition in a Kubernetes cluster. It allows you to use th<a id="_idIndexMarker339" class="pcalibre1 calibre6 pcalibre"/>e same resources (such as names) in different contexts, and it can be used to isolate resources within <span>a cluster.</span></li>
			</ul>
			<h2 id="_idParaDest-126" class="calibre5"><a id="_idTextAnchor125" class="pcalibre1 calibre6 pcalibre"/>Kubernetes architecture for microfrontends</h2>
			<p class="calibre3">When deploying <a id="_idIndexMarker340" class="pcalibre1 calibre6 pcalibre"/>our microfrontends on<a id="_idIndexMarker341" class="pcalibre1 calibre6 pcalibre"/> Kubernetes, we create a pod for each micro app, and this micro app is exposed internally via an <span>Ingress service.</span></p>
			<p class="calibre3">The home app module federates all these micro-apps. The following diagram helps to explain the <span>architecture better:</span></p>
			<div class="calibre2">
				<div id="_idContainer057" class="img---figure">
					<img src="image/Figure_8.01_B18987.jpg" alt="Figure 8.1 – Kubernetes topology architecture to deploy microfrontends" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Kubernetes topology architecture to deploy microfrontends</p>
			<p class="calibre3">As you can see in <span><em class="italic">Figure 8</em></span><em class="italic">.1</em>, each of our micro-apps is deployed within its own pod. These pods can be replicated or set to auto-scale as traffic increases. This is denoted by the dotted box around the pod. These pods are exposed via a service, which acts as a sort of load balancer. Therefore, the home app service is the single endpoint for all the replications of the home micro <span>app pod.</span></p>
			<p class="calibre3">Each of the services is exposed via an Ingress route. This is where we define the URL for our micro app, which eventually will be used in our module federation configuration. This is what the overall Kubernetes architecture will <span>look like.</span></p>
			<p class="calibre3">With this, we come to the end of this section, where we learned about some of the key concepts of<a id="_idIndexMarker342" class="pcalibre1 calibre6 pcalibre"/> Kubernetes, such as nodes, pods, services, Ingress, and <a id="_idIndexMarker343" class="pcalibre1 calibre6 pcalibre"/>the architecture of our micro-apps within a Kubernetes cluster. In the next section, we will see how to go about containerizing our app so that it can be deployed into a <span>Kubernetes cluster.</span></p>
			<h1 id="_idParaDest-127" class="calibre7"><a id="_idTextAnchor126" class="pcalibre1 calibre6 pcalibre"/>Containerizing our micro-apps with Docker</h1>
			<p class="calibre3">Containers are a <a id="_idIndexMarker344" class="pcalibre1 calibre6 pcalibre"/>way to package software applications in<a id="_idIndexMarker345" class="pcalibre1 calibre6 pcalibre"/> a standardized and portable way, allowing them to run consistently across different environments. They provide a lightweight and efficient way to run applications and are particularly well-suited for microservices architectures, where an application is composed of multiple, independently<a id="_idIndexMarker346" class="pcalibre1 calibre6 pcalibre"/> <span>deployable</span><span><a id="_idIndexMarker347" class="pcalibre1 calibre6 pcalibre"/></span><span> services.</span></p>
			<p class="calibre3">In this section, we will look at how to install Docker and create a Docker image by creating <span>a Dockerfile.</span></p>
			<h2 id="_idParaDest-128" class="calibre5"><a id="_idTextAnchor127" class="pcalibre1 calibre6 pcalibre"/>Installing Docker</h2>
			<p class="calibre3">Docker Engine is <a id="_idIndexMarker348" class="pcalibre1 calibre6 pcalibre"/>available for personal use on multiple Linux, Mac, and Windows systems via Docker Desktop. You can follow the instructions here to install the Docker <span>engine: </span><a href="https://docs.docker.com/engine/install/" class="pcalibre1 calibre6 pcalibre"><span>https://docs.docker.com/engine/install/</span></a><span>.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you don’t want to or can’t use Docker Desktop on your Windows or Mac, there are alternatives, such as Rancher Desktop, Podman, <span>and Colima.</span></p>
			<p class="calibre3">Once you have Docker installed, verify it by running the following command in <span>the terminal:</span></p>
			<pre class="console">
docker -v</pre>			<p class="calibre3">If it returns the version of Docker, then you are all set, and it means that Docker was installed successfully on <span>your system.</span></p>
			<h2 id="_idParaDest-129" class="calibre5"><a id="_idTextAnchor128" class="pcalibre1 calibre6 pcalibre"/>Creating standalone app builds</h2>
			<p class="calibre3">Before we can start <a id="_idIndexMarker349" class="pcalibre1 calibre6 pcalibre"/>creating a Docker image, we will first need to ensure that the build outputs of our micro apps are self-contained and can run in standalone mode. We do this by adding the following lines in each of the <strong class="source-inline">next.config.js</strong> files, <span>like so:</span></p>
			<pre class="source-code">
<strong class="bold1">const path = require("path");</strong>
module.exports = {
  <strong class="bold1">output: "standalone",</strong>
<strong class="bold1">  experimental: {</strong>
<strong class="bold1">    outputFileTracingRoot: path.join(__dirname, "../../"),</strong>
<strong class="bold1">  },</strong>
…
}</pre>			<p class="calibre3"><strong class="source-inline">outputFileTracingRoot</strong> is an experimental feature introduced in Next.js 12+  onward; this helps reduce the size of the build outputs, especially when we want to try and reduce our <a id="_idIndexMarker350" class="pcalibre1 calibre6 pcalibre"/>Docker <span>image sizes.</span></p>
			<p class="calibre3">Make sure to add these lines to the <strong class="source-inline">next.config.js</strong> file for each of the <span>micro apps.</span></p>
			<h2 id="_idParaDest-130" class="calibre5"><a id="_idTextAnchor129" class="pcalibre1 calibre6 pcalibre"/>Creating a Dockerfile</h2>
			<p class="calibre3">The next step is to <a id="_idIndexMarker351" class="pcalibre1 calibre6 pcalibre"/>create our Dockerfile, which contains the instructions for Docker to create our <span>Docker image.</span></p>
			<p class="calibre3">Since we need to create a Docker image for each micro app, we will create a Dockerfile within <strong class="source-inline">apps/home</strong>. The default filename we usually give to this <span>is </span><span><strong class="source-inline">Dockerfile</strong></span><span>.</span></p>
			<p class="calibre3">Let's add the following commands to this Dockerfile. We will use the default Dockerfile provided by Turborepo <span>and Next.js.</span></p>
			<p class="calibre3">We will build our Dockerfile as a multi-stage file, which allows us to leverage the caching of the layers and also ensures that the size of the Docker image is as small <span>as possible.</span></p>
			<p class="calibre3">We will build it in three stages, starting with the <span>builder stage:</span></p>
			<pre class="source-code">
FROM node:18-alpine AS base
FROM base AS builder
# Check https://github.com/nodejs/docker-node/tree/b4117f9333da4138b03a546ec926ef50a31506c3#nodealpine to understand why libc6-compat might be needed.
RUN apk add --no-cache libc6-compat
RUN apk update
# Set working directory
WORKDIR /app
RUN yarn global add turbo
COPY . .
RUN turbo prune --scope=home --docker</pre>			<p class="calibre3">As you can see, we use a base image of Node Alpine 18.14, and we call it the builder stage. Alpine is the most minimalistic version <span>of Node.js.</span></p>
			<p class="calibre3">Now, we install the <strong class="source-inline">libc6-compact</strong> library and run the <strong class="source-inline">update</strong> command. Then, we set the working directory for the app and <span>install turbo.</span></p>
			<p class="calibre3">Then, we copy everything from our repo (note the space between the two periods in the <span><strong class="source-inline">COPY</strong></span><span> command).</span></p>
			<p class="calibre3">Finally, we run the <strong class="source-inline">turbo prune</strong> command to extract all the files necessary for the home <span>micro app.</span></p>
			<p class="calibre3">Now, we will move on to the installer stage and continue writing the following code immediately <a id="_idIndexMarker352" class="pcalibre1 calibre6 pcalibre"/>after the <span>previous code:</span></p>
			<pre class="source-code">
FROM base AS installer
RUN apk add --no-cache libc6-compat
RUN apk update
WORKDIR /app
# First install the dependencies (as they change less often)
COPY .gitignore .gitignore
COPY --from=builder /app/out/json/ .
COPY --from=builder /app/out/pnpm-lock.yaml ./pnpm-lock.yaml
RUN yarn global add pnpm
RUN pnpm install --no-frozen-lockfile
# Build the project
COPY --from=builder /app/out/full/ ./
COPY turbo.json turbo.json
RUN ENV=PROD yarn turbo run build --filter=home...</pre>			<p class="calibre3">Again, we start by defining the base image as the installer, running the regular <strong class="source-inline">apk add</strong> and <strong class="source-inline">update</strong> commands, and setting the <span>working directory.</span></p>
			<p class="calibre3">Then, we copy the <strong class="source-inline">.gitignore</strong> file as well as the relevant files from the <strong class="source-inline">/app/out</strong> folder from the <span>builder stage.</span></p>
			<p class="calibre3">We then install <strong class="source-inline">pnpm</strong> and run the <strong class="source-inline">pnpm</strong> <span>install command.</span></p>
			<p class="calibre3">Then, we copy all the files from the <strong class="source-inline">app/out/full</strong> folder from our builder stage and run the <strong class="source-inline">turbo </strong><span><strong class="source-inline">build</strong></span><span> command.</span></p>
			<p class="calibre3">Then, we move on to the<a id="_idIndexMarker353" class="pcalibre1 calibre6 pcalibre"/> final runner stage where we write the <span>following code:</span></p>
			<pre class="source-code">
FROM base AS runner
WORKDIR /app
# Don't run production as root
RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs
USER nextjs
COPY --from=installer /app/apps/home/next.config.js .
COPY --from=installer /app/apps/home/package.json .
# Automatically leverage output traces to reduce image size
# https://nextjs.org/docs/advanced-features/output-file-tracing
COPY --from=installer --chown=nextjs:nodejs /app/apps/home/.next/standalone ./
COPY --from=installer --chown=nextjs:nodejs /app/apps/home/.next/static ./apps/home/.next/static
COPY --from=installer --chown=nextjs:nodejs /app/apps/home/public ./apps/home/public
CMD node apps/home/server.js</pre>			<p class="calibre3">In the preceding code, we basically create a user group to avoid the security risks of running the code as root, and then we copy the relevant files from our installer stage and run the <span><strong class="source-inline">node</strong></span><span> command.</span></p>
			<p class="calibre3">Now, we need to create a <strong class="source-inline">.dockerignore</strong> file in the root of the repo, where we list the files and folders that we don’t want Docker to copy to <span>the image:</span></p>
			<pre class="source-code">
node_modules
npm-debug.log
**/node_modules
.next
**/.next</pre>			<p class="calibre3">Let's test the Dockerfile to see whether it builds. From the root of the application, run the following command in <span>the terminal:</span></p>
			<pre class="console">
docker build -t home -f apps/home/Dockerfile .</pre>			<p class="calibre3"><strong class="source-inline">-t</strong> stands for the tag<a id="_idIndexMarker354" class="pcalibre1 calibre6 pcalibre"/> name, and it will create a Docker image with the name <strong class="source-inline">home</strong>. The <strong class="source-inline">-f</strong> part is the path to <span>the Dockerfile.</span></p>
			<p class="calibre3">Note the space and period at the end of the command, which is important. The period at the end denotes the build context – that is, the set of files and folders Docker should use to build the image. The period also denotes that we want to package all the files and folders in the <span>current directory.</span></p>
			<p class="calibre3">This command will take several minutes to run on its first time, as Docker will download the base node image and other dependencies. The subsequent builds will be a lot faster, as Docker will cache the layers and reuse them if the layer <span>hasn’t changed.</span></p>
			<p class="calibre3">You can run the Docker image locally by running the <span>following command:</span></p>
			<pre class="console">
docker run -p 3000:3000 home</pre>			<p class="calibre3">Once we’ve verified that this works fine, we will need to create similar Dockerfiles for each of <span>our apps.</span></p>
			<p class="calibre3">So, in <strong class="source-inline">apps/catalog</strong> and <strong class="source-inline">apps/checkout</strong>, copy and paste the Dockerfile and replace all instances of <strong class="source-inline">home</strong> with the relevant micro <span>app name.</span></p>
			<p class="calibre3">Note that each of these micro apps runs on the same port, <strong class="source-inline">3000</strong>, so to test them locally, we can test only one image at a time, unless you change the <strong class="source-inline">hostPort</strong> value to something different or use a <span>docker-compose file.</span></p>
			<p class="calibre3">Now that we have learned how to dockerize our micro apps and run them locally, we will move on to the next section on setting up <span>Docker Hub.</span></p>
			<h2 id="_idParaDest-131" class="calibre5"><a id="_idTextAnchor130" class="pcalibre1 calibre6 pcalibre"/>Setting up Docker Hub to store Docker images</h2>
			<p class="calibre3">In the previous <a id="_idIndexMarker355" class="pcalibre1 calibre6 pcalibre"/>section, we created Docker images of our apps and were able to run them locally. For us to be able to deploy them on Kubernetes, we need to store them in a container library from where our DevOps pipelines can pull the images. We will use a free artifact registry solution such as Docker Hub for this. Alternatively, you can use other container registry solutions provided by various hosting providers, such as Azure Container Registry, Google Container Registry, and Amazon Elastic <span>Container Registry:</span></p>
			<ol class="calibre13">
				<li class="calibre14">Log in/register at <a href="https://hub.docker.com" class="pcalibre1 calibre6 pcalibre">https://hub.docker.com</a>, and then create three public repositories one for each micro-app. We will call them <span>the following:</span><ul class="calibre16"><li class="calibre14"><span><strong class="source-inline1">ebuy-home</strong></span></li><li class="calibre14"><span><strong class="source-inline1">ebuy-catalog</strong></span></li><li class="calibre14"><span><strong class="source-inline1">ebuy-checkout</strong></span></li></ul></li>
				<li class="calibre14">Make a note of the Docker registry paths, which are usually of the <strong class="source-inline1">&lt;your-username&gt;/ebuy-home</strong> format, <strong class="source-inline1">&lt;your-username&gt;/ebuy-catalog</strong> format, and <span>so on.</span></li>
				<li class="calibre14">Then, let's create an access token that will be needed for our CI and CD pipelines. Go to <strong class="bold">Account Settings</strong>, and on the <strong class="bold">Security</strong> page, create a new access token and give it a description. Under <strong class="bold">Access permissions</strong>, select <strong class="bold">Read and Write</strong>, as our pipelines will need to push and pull the <span>Docker images.</span></li>
				<li class="calibre14">Once the token is generated, copy and keep it safe, as it will never be displayed again. (You can always generate a new token if you’ve lost the <span>old one.)</span></li>
			</ol>
			<p class="calibre3">Our work on Docker Hub <span>is done!</span></p>
			<p class="calibre3">In the next section, we <a id="_idIndexMarker356" class="pcalibre1 calibre6 pcalibre"/>will create our Kubernetes configuration files that will be used to spin up our <span>Kubernetes cluster.</span></p>
			<h1 id="_idParaDest-132" class="calibre7"><a id="_idTextAnchor131" class="pcalibre1 calibre6 pcalibre"/>Creating a Kubernetes configuration file</h1>
			<p class="calibre3">Earlier in this<a id="_idIndexMarker357" class="pcalibre1 calibre6 pcalibre"/> chapter, in the <em class="italic">Introduction to Kubernetes</em> section, we learned about the various Kubernetes services that we will use to deploy <span>our microfrontends.</span></p>
			<p class="calibre3">Deploying these services on Kubernetes is commonly done by defining the various configuration settings in a <strong class="source-inline">.yaml</strong> file and then applying the configuration to the <span>Kubernetes cluster.</span></p>
			<p class="calibre3">In this section, we will learn about the structure of these Kubernetes spec files and how to go about creating them for our deployments, services, <span>and Ingress.</span></p>
			<h2 id="_idParaDest-133" class="calibre5"><a id="_idTextAnchor132" class="pcalibre1 calibre6 pcalibre"/>The structure of a Kubernetes spec file</h2>
			<p class="calibre3">A Kubernetes spec file <a id="_idIndexMarker358" class="pcalibre1 calibre6 pcalibre"/>is a YAML document that describes the desired state of a Kubernetes object, such as a Deployment, Pod, Service, or ConfigMap. The structure of a Kubernetes spec file generally consists of two main parts – the metadata section and the spec section. Each file always starts by defining the <strong class="source-inline">apiVersion</strong> and the <strong class="source-inline">kind</strong> of <span>spec file.</span></p>
			<p class="calibre3">The metadata section includes information about the object, such as its name, labels, and annotations. This section is used by Kubernetes to manage the object and enable other objects to <span>reference it.</span></p>
			<p class="calibre3">The spec section includes the desired state of the object, such as the container image, resource requests and limits, networking configuration, and any other relevant settings. This section is used<a id="_idIndexMarker359" class="pcalibre1 calibre6 pcalibre"/> by Kubernetes to create and manage the object according to its <span>desired state.</span></p>
			<h2 id="_idParaDest-134" class="calibre5"><a id="_idTextAnchor133" class="pcalibre1 calibre6 pcalibre"/>Creating spec files to deploy our microfrontends</h2>
			<p class="calibre3">As we saw earlier, the<a id="_idIndexMarker360" class="pcalibre1 calibre6 pcalibre"/> structure of a<a id="_idIndexMarker361" class="pcalibre1 calibre6 pcalibre"/> Kubernetes spec file follows a hierarchical format, with each section and its corresponding properties nested under the appropriate heading. Additionally, many Kubernetes objects have properties that are specific to their type, so the structure of the spec file may vary depending on the object <span>being described.</span></p>
			<p class="calibre3">Let's start by creating these files in a folder called <strong class="source-inline">k8s</strong> within each of the micro <span>apps folders.</span></p>
			<p class="calibre3">Let’s start by creating the <strong class="source-inline">/apps/home/k8s/deployment.yml</strong> file with the following code. The <strong class="source-inline">deployment.yml</strong> file contains the configuration to set up and configure the Kubernetes pods within which our micro app <span>will run:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: home
  namespace: default
  labels:
    app: home
spec:
  replicas: 1
  selector:
    matchLabels:
      app: home
  template:
    metadata:
      labels:
        app: home
    spec:
      containers:
        - name: home
          <strong class="bold1">image: &lt;dockerUserID&gt;/ebuy-home:latest</strong>
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP</pre>			<p class="calibre3">As you read through the <strong class="source-inline">deployment.yml</strong> configuration file, you will see that we label the app as <strong class="source-inline">home</strong> and also use the same name to define the name of our container. We define the number of replicas as one, which means it will spin up one pod; increase this number to two or more if you want multiple replicas of the pod. Then, within the container section of the<a id="_idIndexMarker362" class="pcalibre1 calibre6 pcalibre"/> file, we define the name <a id="_idIndexMarker363" class="pcalibre1 calibre6 pcalibre"/>of the path of the Docker image it should use and the ports and protocols that it should use. Replace this with the values of your Docker repository. Note <strong class="source-inline">:latest</strong> at the end of the Docker image value; this is something we add to ensure that Kubernetes always picks up the latest version of the <span>Docker image.</span></p>
			<p class="calibre3">Now, we define the service, which acts as a sort of load balancer over one or more replicas of <span>the pod.</span></p>
			<p class="calibre3">Create a new file called <strong class="source-inline">/apps/home/k8s/service.yml</strong> with the <span>following code:</span></p>
			<pre class="source-code">
kind: Service
apiVersion: v1
metadata:
  name: home
  namespace: default
  labels:
    app: home
spec:
  type: LoadBalancer
  selector:
    app: home
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
      name: home</pre>			<p class="calibre3">The <strong class="source-inline">service.yml</strong> file is quite straightforward, wherein we provide the necessary metadata such as the <strong class="source-inline">name</strong>, <strong class="source-inline">label</strong>, and <strong class="source-inline">namespace</strong> for the <span>Kubernetes cluster.</span></p>
			<p class="calibre3">Then, within the specs, we define what type of service this is; we will set it as a <strong class="source-inline">LoadBalancer</strong>. This will <a id="_idIndexMarker364" class="pcalibre1 calibre6 pcalibre"/>help <a id="_idIndexMarker365" class="pcalibre1 calibre6 pcalibre"/>expose a public IP address that we will need later and, finally, within the <strong class="source-inline">ports</strong> section, the protocol and port numbers on which we will expose <span>the service.</span></p>
			<p class="calibre3">Finally, we need to define the <strong class="source-inline">ingress.yml</strong> file where we will assign a URL to the service. Create a file called <strong class="source-inline">/apps/home/k8s/ingress.yml</strong> with the <span>following code.</span></p>
			<p class="calibre3">The Ingress within Kubernetes essentially runs nginx under the hood, so if you are familiar with nginx, configuring this should <span>be easy:</span></p>
			<pre class="source-code">
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: home
  namespace: default
  labels:
    app: home
  annotations:
    # nginx.ingress.kubernetes.io/enable-cors: 'true'
    # nginx.ingress.kubernetes.io/cors-allow-origin: '*'
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /home(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: home
            port:
              number: 80</pre>			<p class="calibre3">This is generally a bit of a <a id="_idIndexMarker366" class="pcalibre1 calibre6 pcalibre"/>tricky<a id="_idIndexMarker367" class="pcalibre1 calibre6 pcalibre"/> file to configure, as this is where you define the URL structures and rewrite rules and other nginx configurations as you’d do for a web server. As you can see, we define the regular metadata information under annotations, and we define the various rewrite rules and nginx configurations, such as CORS. Then, we set the <strong class="source-inline">regex</strong> path, which tells Kubernetes through which URLs it should direct traffic to this service and pod. Finally we need to copy and paste the K8s folder into each of our micro apps and update the relevant paths and app names to match the name of the <span>micro app.</span></p>
			<p class="calibre3">As we come to the end of this section, we’ve seen how to create Kubernetes spec files to deploy pods, how to set up a service that sits over these pods, and finally, the ingress that provides routing to these pods. In the next section, we will create an Azure Kubernetes cluster, against which<a id="_idIndexMarker368" class="pcalibre1 calibre6 pcalibre"/> we will<a id="_idIndexMarker369" class="pcalibre1 calibre6 pcalibre"/> execute <span>these specs.</span></p>
			<h1 id="_idParaDest-135" class="calibre7"><a id="_idTextAnchor134" class="pcalibre1 calibre6 pcalibre"/>Setting up a managed Kubernetes Cluster on Azure</h1>
			<p class="calibre3">In this section, we<a id="_idIndexMarker370" class="pcalibre1 calibre6 pcalibre"/> will learn how to set up <a id="_idIndexMarker371" class="pcalibre1 calibre6 pcalibre"/>a managed Kubernetes cluster on Azure. The reason it’s called <em class="italic">managed</em> is because the master node, which is sort of the brain of Kubernetes, is managed by Azure, and we only need to spin up the worker nodes. We will see how to log in to Azure and create a subscription key, and we will install Azure CLI and collect the various credentials that we need for our <span>DevOps pipeline.</span></p>
			<p class="calibre3">For this chapter, we will <a id="_idIndexMarker372" class="pcalibre1 calibre6 pcalibre"/>use <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) to set up our cloud-based managed Kubernetes cluster. You can also set up a managed Kubernetes cluster on Google Cloud using <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), or<a id="_idIndexMarker373" class="pcalibre1 calibre6 pcalibre"/> you can use Amazon <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) <span>on AWS.</span></p>
			<p class="calibre3">Irrespective of whichever hosting provider you use to set up your Kubernetes cluster, the Dockerfile and the Kubernetes configuration <strong class="source-inline">.yaml</strong> files remain <span>the same.</span></p>
			<h2 id="_idParaDest-136" class="calibre5"><a id="_idTextAnchor135" class="pcalibre1 calibre6 pcalibre"/>Logging into the Azure portal and setting up a subscription key</h2>
			<p class="calibre3">To carry out any <a id="_idIndexMarker374" class="pcalibre1 calibre6 pcalibre"/>activity on the Azure platform, you need to have the login<a id="_idIndexMarker375" class="pcalibre1 calibre6 pcalibre"/> credentials for the platform and a subscription key. All the resources that we create within Azure need to be mapped to a subscription key, which eventually is used by Azure to calculate the hosting charges. To do this, follow <span>these steps:</span></p>
			<ol class="calibre13">
				<li class="calibre14">Head over to <a href="https://portal.azure.com" class="pcalibre1 calibre6 pcalibre">https://portal.azure.com</a> and log in with a Microsoft login; if you don’t have one, you can always sign up <span>for one.</span></li>
				<li class="calibre14">Once logged into the portal, search for <strong class="source-inline1">Subscriptions</strong> and add a <strong class="bold">Pay-As-You-Go</strong> subscription. If you have an <strong class="bold">Azure for Student</strong> or free trial subscription in your list, feel free to select either one of them as well. This subscription will be used for all the hosting costs that will be incurred as part of the various services you run <span>within Azure.</span></li>
				<li class="calibre14">Then, in the search box, search for <strong class="source-inline1">Resource Group</strong> and create a resource group. Let’s call it <strong class="source-inline1">ebuy-rg</strong>; the <strong class="source-inline1">rg</strong> suffix stands<a id="_idIndexMarker376" class="pcalibre1 calibre6 pcalibre"/> for <strong class="bold">resource group</strong>. It would have selected the default subscription that you created in the earlier step. For the region, you can select <strong class="bold">US East</strong> or a region of your choice; for the sake of consistency in this chapter, we will stick with <span><strong class="bold">US East</strong></span><span>.</span><p class="calibre3">In Azure, it is always a good practice to create a resource group for a project and then have all the various services associated with that project within the resource group. This <a id="_idIndexMarker377" class="pcalibre1 calibre6 pcalibre"/>allows us to easily manage the resources <a id="_idIndexMarker378" class="pcalibre1 calibre6 pcalibre"/>within the resource group, especially when we want to shut down all the services for <span>the project.</span></p></li>
				<li class="calibre14">Next, we will create our AKS cluster; search for <strong class="source-inline1">Azure Kubernetes Service (AKS)</strong>, click on the <strong class="bold">Create</strong> button in the top-left corner, and then select the <strong class="bold">Create a Kubernetes cluster</strong> menu item. You will be presented with a screen, as shown in the <span>following screenshot:</span></li>
			</ol>
			<div class="calibre2">
				<div id="_idContainer058" class="img---figure">
					<img src="image/Figure_8.02_B18987.jpg" alt="Figure 8.2 – The Create Kubernetes cluster screen" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The Create Kubernetes cluster screen</p>
			<ol class="calibre13">
				<li value="5" class="calibre14">Select the <a id="_idIndexMarker379" class="pcalibre1 calibre6 pcalibre"/>subscription <a id="_idIndexMarker380" class="pcalibre1 calibre6 pcalibre"/>and resource group we created in the earlier steps, and then, in the Cluster preset configuration, select <strong class="bold">Production Standard</strong> as the preset configuration. You can also choose other higher configurations; however, note that the AKS cluster is the most expensive component of your Azure <span>monthly billing.</span></li>
				<li class="calibre14">Provide the Kubernetes cluster name as <strong class="source-inline1">ebuy</strong>, and select the same region where you have your resource group created; in our case, it is <strong class="bold">(US) East US</strong>. For the Kubernetes version, you can choose to leave it as default or select <strong class="bold">1.26.6 </strong>to ensure the settings are consistent with the code and configuration defined in the chapter. For the scale method, set it to <strong class="bold">Autoscale</strong>, and for the maximum number of nodes, leave it at <strong class="bold">1</strong> or <strong class="bold">2</strong>. Finally, hit <strong class="bold">Review + Create</strong>, and then after <a id="_idIndexMarker381" class="pcalibre1 calibre6 pcalibre"/>the <a id="_idIndexMarker382" class="pcalibre1 calibre6 pcalibre"/>validation check is done, <span>hit </span><span><strong class="bold">Create</strong></span><span>.</span></li>
			</ol>
			<p class="calibre3">We now have our Kubernetes cluster running <span>within AKS.</span></p>
			<h2 id="_idParaDest-137" class="calibre5"><a id="_idTextAnchor136" class="pcalibre1 calibre6 pcalibre"/>Accessing your Kubernetes cluster via the Azure CLI</h2>
			<p class="calibre3">The de facto <a id="_idIndexMarker383" class="pcalibre1 calibre6 pcalibre"/>approach <a id="_idIndexMarker384" class="pcalibre1 calibre6 pcalibre"/>to interacting with your Kubernetes cluster on Azure is via the Azure CLI at <a href="https://learn.microsoft.com/en-us/cli/azure/" class="pcalibre1 calibre6 pcalibre">https://learn.microsoft.com/en-us/cli/azure/</a>. If you are working with Kubernetes it is best to also install kubectl, the instructions for which you can find here <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/" class="pcalibre1 calibre6 pcalibre">https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/</a> </p>
			<p class="calibre3">Follow the documentation at the preceding URL to get the Azure CLI set up on <span>your system.</span></p>
			<p class="calibre3">Once you have the Azure CLI up and running, the next step is to log in using the <span>following command:</span></p>
			<pre class="console">
az login</pre>			<p class="calibre3">Once you’ve successfully logged in, it will display the details of the subscription and tenant details for <span>your subscription.</span></p>
			<p class="calibre3">Run a couple of the following commands to get a feel for the Azure CLI and the basic <span>Kubernetes commands:</span></p>
			<ul class="calibre15">
				<li class="calibre14"><strong class="source-inline1">az aks list</strong>   //: To get a list of all your <span>aks clusters</span></li>
				<li class="calibre14"><strong class="source-inline1">az aks get-credentials --resource-group ebuy-rg --name ebuy  </strong>//: To connect to your <span>aks cluster</span></li>
				<li class="calibre14"><strong class="source-inline1">kubectl get nodes   </strong>//: To get a list of all <span>the nodes</span></li>
				<li class="calibre14"><strong class="source-inline1">kubectl get pods   </strong>//: To get a list of all the pods running (we don’t have any pods running yet, so don’t worry if you get an <span>error message)</span></li>
			</ul>
			<p class="calibre3">These are just a few commands to help you get started; if you are keen to learn about the rest of the kubectl commands head over to the official <em class="italic">kubectl Cheat </em><span><em class="italic">Sheet</em></span><span>: </span><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/" class="pcalibre1 calibre6 pcalibre"><span>https://kubernetes.io/docs/reference/kubectl/cheatsheet/</span></a><span>.</span></p>
			<p class="calibre3">Once you are happy trying out the different kubectl commands and comfortable interacting with <a id="_idIndexMarker385" class="pcalibre1 calibre6 pcalibre"/>your<a id="_idIndexMarker386" class="pcalibre1 calibre6 pcalibre"/> Kubernetes cluster, we will proceed to the next step of gathering the necessary credentials to <span>automate deployments.</span></p>
			<h2 id="_idParaDest-138" class="calibre5"><a id="_idTextAnchor137" class="pcalibre1 calibre6 pcalibre"/>Generating credentials for your DevOps pipelines</h2>
			<p class="calibre3">For any DevOps pipeline to<a id="_idIndexMarker387" class="pcalibre1 calibre6 pcalibre"/> access the various resources on Azure to spin up Kubernetes clusters, it will need <span>access permissions.</span></p>
			<p class="calibre3">We will now collect the necessary access permissions. Ensure that you are logged in at <a href="https://portal.azure.com" class="pcalibre1 calibre6 pcalibre">https://portal.azure.com</a>, or log in via the <strong class="source-inline">az login</strong> <span>CLI command.</span></p>
			<p class="calibre3">The following is a list of IDs and secrets that we need from Azure and the process to find them within the <span>Azure portal:</span></p>
			<ul class="calibre15">
				<li class="calibre14"><strong class="bold">Subscription ID</strong>: Search for <strong class="source-inline1">Subscriptions</strong> and select your subscription to display the <span>subscription ID.</span></li>
				<li class="calibre14"><strong class="bold">Tenant ID</strong>: Search for <strong class="source-inline1">Azure Active Directory</strong> and note the <span>Tenant_ID displayed</span></li>
				<li class="calibre14">Then, we need to create a service principle that can create and manage resources within our resource group; we do that using the az CLI. In the terminal, fire the following command, replacing <strong class="source-inline1">{subscriptionid}</strong> with the value you noted in the previous steps, and <strong class="source-inline1">{resource-group}</strong> with the name of the resource group; in this case, it <span>is </span><span><strong class="source-inline1">ebuy-rg</strong></span><span>:</span><pre class="source-code">
az ad sp create-for-rbac --name “MyApp” --role Contributor --scopes /subscriptions/{subscriptionid}/resourceGroups/ebuy-rg --sdk-auth</pre><p class="calibre3">Run the command, and if all goes well, it will publish a list of configuration variables, as shown in <span><em class="italic">Figure 8</em></span><em class="italic">.3</em>, which you can easily save for <span>further use.</span></p></li>			</ul>
			<div class="calibre2">
				<div id="_idContainer059" class="img---figure">
					<img src="image/Figure_8.03_B18987.jpg" alt="Figure 8.3 – Output from running the command to create a service principle" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Output from running the command to create a service principle</p>
			<p class="calibre3">Note down the configurations from the preceding output, as we will need it in the <span>following steps.</span></p>
			<p class="calibre3">Now that we have all<a id="_idIndexMarker388" class="pcalibre1 calibre6 pcalibre"/> the necessary credentials we need, let's proceed to the next section on setting up the CI and CD pipelines where we will use <span>these credentials.</span></p>
			<h1 id="_idParaDest-139" class="calibre7"><a id="_idTextAnchor138" class="pcalibre1 calibre6 pcalibre"/>Setting up CI/CD with GitHub Actions</h1>
			<p class="calibre3">In this section, we<a id="_idIndexMarker389" class="pcalibre1 calibre6 pcalibre"/> will learn how to go about setting <a id="_idIndexMarker390" class="pcalibre1 calibre6 pcalibre"/>up a DevOps pipeline using GitHub Actions. A DevOps pipeline is a series of steps that we define to automate the build and deployment of our apps. In this section, we will learn how to set up GitHub secrets and the workflow .<span><strong class="source-inline">yml</strong></span><span> file.</span></p>
			<p class="calibre3">GitHub Actions is an automation and workflow tool provided by GitHub that allows developers to automate software development workflows and streamline their software development process. With GitHub Actions, you can create custom workflows that automate tasks such as building, testing, deploying, and releasing code directly from your GitHub repository. Other tools that we can use for CI and CD are Jenkins, Azure DevOps, Google Cloud Build, and so on. For the purpose of this chapter, we will use <span>GitHub Actions.</span></p>
			<h2 id="_idParaDest-140" class="calibre5"><a id="_idTextAnchor139" class="pcalibre1 calibre6 pcalibre"/>Setting up GitHub secrets</h2>
			<p class="calibre3">As part of the CI and<a id="_idIndexMarker391" class="pcalibre1 calibre6 pcalibre"/> CD steps, GitHub Actions needs to push the Docker image to Docker Hub and spin up new Kubernetes pods, and so on. For all these activities, it needs to be able to log in to the systems with the right credentials. As a rule and for security purposes, we should never directly hardcode the usernames or passwords directly into the DevOps pipelines. The correct way is to create GitHub secrets and use those in <span>your pipelines.</span></p>
			<p class="calibre3">First and foremost, make sure you have committed and pushed the latest changes we’ve made so far <span>to GitHub.</span></p>
			<p class="calibre3">Let's create our GitHub secrets by first going to the <strong class="bold">Settings</strong> tab on the GitHub repo and then to the <strong class="bold">Secrets and variables</strong> section. Then, under <strong class="bold">Actions</strong>, we will create the following secrets along with the corresponding values that we noted down earlier from Docker and the <span>Azure subscription:</span></p>
			<pre class="source-code">
AZ_CLIENT_ID
AZ_CLIENT_SECRET
AZ_SUBSCRIPTION_ID
AZ_TENANT_ID
DOCKERHUB_USERNAME
DOCKERHUB_TOKEN</pre>			<p class="calibre3">We will create these as secrets in our DevOps pipeline. These secrets can be accessed in the pipeline as <strong class="source-inline">${{ </strong><span><strong class="source-inline">secrets.&lt;variable-name&gt; }}</strong></span><span>.</span></p>
			<h2 id="_idParaDest-141" class="calibre5"><a id="_idTextAnchor140" class="pcalibre1 calibre6 pcalibre"/>Getting started with GitHub Actions</h2>
			<p class="calibre3">GitHub Actions is a<a id="_idIndexMarker392" class="pcalibre1 calibre6 pcalibre"/> relatively new feature provided by GitHub that allows you to create workflows to automate tasks. It can also be used to set up an automated CI and CD pipeline, which is exactly what we will use it for in <span>this chapter.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about GitHub Actions in detail <span>here: </span><a href="https://docs.github.com/en/actions" class="pcalibre1 calibre6 pcalibre"><span>https://docs.github.com/en/actions</span></a><span>.</span></p>
			<p class="calibre3">Creating a GitHub action is straightforward. All we need to do is, at the root of our project folder, create a folder called <strong class="source-inline">.github/workflows</strong> and then a <strong class="source-inline">.yaml</strong> file. Once pushed to GitHub, it will automatically detect that you have a workflow file and it will execute it as per <span>the triggers:</span></p>
			<ol class="calibre13">
				<li class="calibre14">Let's create <a id="_idIndexMarker393" class="pcalibre1 calibre6 pcalibre"/>our <strong class="source-inline1">.yaml</strong> file at <strong class="source-inline1">.github/workflows/home-build-deploy.yml</strong>, and within it, let’s write the <span>following code:</span><pre class="source-code">
name: home-build-deploy
on:
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - apps/home/**</pre><p class="calibre3">We will provide a name for our GitHub action; which is what will be shown in GitHub Actions. Then, we define the triggers, <strong class="source-inline">on push:</strong> and <strong class="source-inline">on:workflow_dispatch</strong>. The<strong class="source-inline"> workflow_dispatch</strong> trigger allows you to manually trigger a pipeline when needed (especially when testing your pipelines), and as you can see, <strong class="source-inline">on push</strong> has further options for <strong class="source-inline">branches: main</strong> and <strong class="source-inline">paths: apps/catalog/**</strong>. This means a change to any file within the <strong class="source-inline">home micro-app</strong> that is pushed to the <strong class="source-inline">main</strong> branch will trigger this pipeline. The <strong class="source-inline">paths</strong> section is critical to ensure that the pipeline builds and deploys only the changed <span>micro app.</span></p></li>				<li class="calibre14">Now, we need to define the list of jobs that GitHub actions should run; we will do this <span>as follows:</span><pre class="source-code">
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    strategy:
    permissions:
    steps:</pre><p class="calibre3">For every job we<a id="_idIndexMarker394" class="pcalibre1 calibre6 pcalibre"/> define in the pipeline, we need to define what operating system the DevOps pipeline needs to run on, any strategies, what permissions to provide, and finally, the steps that it needs <span>to run.</span></p><p class="calibre3">Now, we will expand into each of <span>these sections.</span></p></li>				<li class="calibre14">Since the commands to build and deploy the micro apps remain the same, we will use a matrix strategy that allows us to define variables that can be used later in these steps. Within the strategy section, write the <span>following code:</span><pre class="source-code">
strategy:
      fail-fast: false
      matrix:
        include:
          - dockerfile: './apps/home/Dockerfile'
            image: areai51/ebuy-home
            k8sManifestPath: './apps/home/k8s/'</pre><p class="calibre3">We set the <strong class="source-inline">fail-fast</strong> option to <strong class="source-inline">false</strong> so that GitHub action continues to run the pipeline for the other micro apps, even if one of them fails. Then, we define the matrix of our variables, which are <span>as follows:</span></p><ul class="calibre16"><li class="calibre14"><strong class="source-inline1">Dockerfile</strong>: The path to where the micro app’s Dockerfile is located in your <span>code base</span></li><li class="calibre14"><strong class="source-inline1">Image</strong>: The path to the Docker image in <span>Docker Hub</span></li><li class="calibre14"><strong class="source-inline1">k8sManifestPath</strong>: The location of the Kubernetes manifest files needed to spin up your micro app pod, services, <span>and ingress</span></li></ul></li>			</ol>
			<p class="calibre3">For permissions, we set <span>the following:</span></p>
			<pre class="source-code">
    permissions:
      contents: read
      packages: write</pre>			<p class="calibre3">We set the <strong class="source-inline">contents</strong> scope to read and the <strong class="source-inline">packages</strong> scope <span>to write.</span></p>
			<p class="calibre3">The next series of steps is where the actual <span>work happens.</span></p>
			<p class="calibre3">As we will see, every<a id="_idIndexMarker395" class="pcalibre1 calibre6 pcalibre"/> step has two to three properties – the first is <strong class="source-inline">name</strong>; then <strong class="source-inline">uses</strong>, which is the component that is used to perform the step; and finally, <strong class="source-inline">with</strong>, which is optional and defines the additional properties required to perform <span>the step.</span></p>
			<p class="calibre3">All of the code in the following steps will be in the <strong class="source-inline">steps:</strong> section of the <strong class="source-inline">.</strong><span><strong class="source-inline">yml</strong></span><span> file:</span></p>
			<ol class="calibre13">
				<li class="calibre14">We start by checking out <span>the repository:</span><pre class="source-code">
    - name: Checkout Repository
      uses: actions/checkout@v3.3.0</pre></li>				<li class="calibre14">Then, we log in to Docker Hub, passing our username and the access token as the password. Note that we pass them as secrets, which we <span>defined earlier:</span><pre class="source-code">
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}</pre></li>				<li class="calibre14">In the next step, we extract the <strong class="source-inline1">git SHA</strong> value, which we will use to tag our <span>Docker images:</span><pre class="source-code">
      - name: Extract git SHA
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ matrix.image }}
          tags: |
            type=sha</pre></li>				<li class="calibre14">The next step is<a id="_idIndexMarker396" class="pcalibre1 calibre6 pcalibre"/> the build and push command, where we build the Docker image by passing the micro app name via the matrix variable, and then we push that build Docker image to Docker Hub, using the <strong class="source-inline1">git</strong> SHA value as the <span>image tag:</span><pre class="source-code">
      - name: Build and push micro app docker image
        uses: docker/build-push-action@v4.0.0
        with:
          context: "."
          file: ${{ matrix.dockerfile }}
          push: true
          tags: ${{ steps.meta.outputs.tags }}</pre></li>				<li class="calibre14">Once the Docker images are pushed to Docker Hub, it’s time for us to set up our Kubernetes pods and services, for which we first need to set <span>up </span><span><strong class="source-inline1">Kubectl</strong></span><span>:</span><pre class="source-code">
      - name: Setup Kubectl
        uses: azure/setup-kubectl@v3</pre></li>				<li class="calibre14">First, we log in to Azure using the client ID and <span>client secrets:</span><pre class="source-code">
      - name: Azure Login
        uses: Azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZ_CLIENT_ID }}","clientSecret":"${{ secrets.AZ_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZ_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZ_TENANT_ID }}"}'</pre></li>				<li class="calibre14">Next, we set up the <span>Kubernetes context:</span><pre class="source-code">
      - name: Set K8s Context
        uses: Azure/aks-set-context@v3
        with:
          cluster-name: ebuy
          resource-group: ebuy-rg</pre></li>				<li class="calibre14">Finally, we<a id="_idIndexMarker397" class="pcalibre1 calibre6 pcalibre"/> run the Kubernetes <span>deploy commands:</span><pre class="source-code">
      - name: Deploy to K8s
        uses: Azure/k8s-deploy@v4
        with:
          namespace: "default"
          action: deploy
          manifests: |
            ${{ matrix.k8sManifestPath }}
          images: |
            ${{ steps.meta.outputs.tags }}</pre><p class="calibre3">Once you’ve verified that all the indentation in the file is correct, go ahead and commit the file to the <span>main branch.</span></p><p class="calibre3">Then, make a small change to any one of the code files within the home app, commit it, and push it to GitHub. After committing your change, head over to the actions tab at <a href="https://github.com" class="pcalibre1 calibre6 pcalibre">github.com</a>, and you should be able to see the GitHub pipeline begin <span>to run.</span></p></li>			</ol>
			<p class="calibre3">Follow the steps as GitHub Actions goes step by step through the jobs. If there are any errors, the jobs will fail, so look through the errors and make the necessary fixes. Feel free to seek help from your friends and the community as you navigate through this critical step, and keep testing until the pipeline <span>runs successfully.</span></p>
			<p class="calibre3">Once the pipeline builds successfully, make copies of the workflow file within the <strong class="source-inline">.github/workflows</strong> folder to build and deploy the other micro apps. We will call these files <strong class="source-inline">.github/workflows/catalog-build-deploy.yml</strong> <span>and .</span><span><strong class="source-inline">github/workflows/checkout-build-deploy.yml</strong></span><span>.</span></p>
			<p class="calibre3">In the respective files, change<a id="_idIndexMarker398" class="pcalibre1 calibre6 pcalibre"/> all occurrences of the word <strong class="source-inline">home</strong> to <strong class="source-inline">catalog</strong> and <strong class="source-inline">checkout</strong>. For example, in your <strong class="source-inline">catalog-build-deploy.yml</strong> file, you will have <span>the following:</span></p>
			<pre class="source-code">
name: catalog-build-deploy
on:
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - apps/catalog/**</pre>			<p class="calibre3">The <strong class="source-inline">matrix</strong> section under strategies will look <span>as follows:</span></p>
			<pre class="source-code">
      matrix:
        include:
          - dockerfile: "./apps/catalog/Dockerfile"
            image: areai51/ebuy-ssr-catalog
            k8sManifestPath: "./apps/catalog/k8s/"</pre>			<p class="calibre3">Similarly, the <strong class="source-inline">checkout-build-deploy.yml</strong> file will have the <span>following changes:</span></p>
			<pre class="source-code">
name: checkout-build-deploy
on:
   workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - apps/checkout/**</pre>			<p class="calibre3">Also, the <strong class="source-inline">matrix</strong><a id="_idIndexMarker399" class="pcalibre1 calibre6 pcalibre"/> section under <strong class="source-inline">strategies</strong> will be <span>as follows:</span></p>
			<pre class="source-code">
      matrix:
        include:
          - dockerfile: "./apps/checkout/Dockerfile"
            image: areai51/ebuy-ssr-checkout
            k8sManifestPath: "./apps/checkout/k8s/"</pre>			<p class="calibre3">Then, make a small change, commit files to the checkout and catalog apps, and verify that only the relevant pipeline <span>is triggered.</span></p>
			<p class="calibre3">We can also verify the micro app pods have been successfully created within the <strong class="source-inline">ebuy-ssr</strong> Kubernetes cluster by running the following <strong class="source-inline">kubectl get pods</strong> command in <span>the terminal.</span></p>
			<p class="calibre3">If any of the pods don’t show a ready status or have a high restart count, you can look into the pod logs using the <strong class="source-inline">kubectl logs &lt;pod-name&gt;</strong> command in <span>the terminal.</span></p>
			<p class="calibre3">With this, we have successfully created our DevOps pipeline using GitHub Actions, where we learned how to securely save our credentials as GitHub action secrets, created an individual workflow .<strong class="source-inline">yml</strong> file for each micro app, and configured it so that they are triggered only when the corresponding micro app <span>has changed.</span></p>
			<p class="calibre3">While these micro apps are individually running, they will not work with module federation, as the remotes on Kubernetes are different from what we ran locally. In the next section, we <a id="_idIndexMarker400" class="pcalibre1 calibre6 pcalibre"/>will update the remotes to ensure that it works on the cloud <span>as well.</span></p>
			<h1 id="_idParaDest-142" class="calibre7"><a id="_idTextAnchor141" class="pcalibre1 calibre6 pcalibre"/>Updating the remotes</h1>
			<p class="calibre3">Once you have your <a id="_idIndexMarker401" class="pcalibre1 calibre6 pcalibre"/>pipelines deployed successfully, log in to <a href="https://portal.azure.com" class="pcalibre1 calibre6 pcalibre">portal.azure.com</a>, go to the Kubernetes services, select your Kubernetes cluster, go to the Services and Ingress link, and note the external IP address for the service of the <span>micro apps.</span></p>
			<p class="calibre3">You can achieve the same by running the <strong class="source-inline">kubectl get services</strong> command in <span>the terminal.</span></p>
			<p class="calibre3">Once we have the IP address, we need to update our module federation remotes with the <span>updated URLs.</span></p>
			<p class="calibre3">Now, as you may have figured out, the URLs for our microapps are different locally and on Kubernetes. Since we want to be able to run our apps locally as well as on Kubernetes, we will need to conditionally load in the remotes based on whether the app is running in <strong class="source-inline">dev</strong> or <strong class="source-inline">production</strong> mode. We do this <span>as follows:</span></p>
			<p class="calibre3">In the <strong class="source-inline">apps/home/next.config.js</strong> file within the <strong class="source-inline">remotes</strong> object, we update the code <span>as follows:</span></p>
			<pre class="source-code">
const remotes = (isServer) =&gt; {
  const location = isServer ? "ssr" : "chunks";
  const ENV = process.env.ENV;
  const CATALOG_URL_LOCAL = 'http://localhost:3001';
const CHECKOUT_URL_LOCAL = 'http://localhost:3002’;
  const CATALOG_URL_PROD = 'http://&lt;your-k8s-ip-address&gt;’
  const CHECKOUT_URL_PROD = 'http://&lt;your-k8s-ip-address&gt;’
  const CATALOG_REMOTE_HOST = ENV === 'PROD' ? CATALOG_URL_PROD : CATALOG_URL_LOCAL;
  const CHECKOUT_REMOTE_HOST = ENV === 'PROD' ? CHECKOUT_URL_PROD : CHECKOUT_URL_LOCAL;
  return {
    catalog: `catalog@${CATALOG_REMOTE_HOST}/_next/static/${location}/remoteEntry.js`,
    checkout: `checkout@${CHECKOUT_REMOTE_HOST}/_next/static/${location}/remoteEntry.js`,
  };
};</pre>			<p class="calibre3">What we do here is<a id="_idIndexMarker402" class="pcalibre1 calibre6 pcalibre"/> define a new variable called <strong class="source-inline">ENV</strong> that captures whether the app is running in dev or prod mode, then we create the consts for <strong class="source-inline">LOCAL URL</strong> and <strong class="source-inline">PROD URLS</strong> for our micro apps, and conditionally set the values of the <strong class="source-inline">CATALOG_REMOTE_HOST</strong> and <strong class="source-inline">CHECKOUT_REMOTE_HOST</strong> values based on the <span><strong class="source-inline">ENV</strong></span><span> values.</span></p>
			<p class="calibre3">Make the same set of changes to the <strong class="source-inline">next.config.js</strong> files in the checkout and catalog apps, and then save <span>the changes.</span></p>
			<p class="calibre3">Now, we can and build the apps locally to verify that things <span>work fine.</span></p>
			<p class="calibre3">Run the <strong class="source-inline">pnpm dev</strong> command from the root of <span>the project.</span></p>
			<p class="calibre3">Once this works locally, let us commit the changes to Git and let the GitHub actions auto-trigger and deploy the new apps to our <span>Kubernetes cluster.</span></p>
			<p class="calibre3">Once it’s all done, head over to the URL of the home micro app (<strong class="source-inline">http://&lt;your-k8s-ip-address&gt;/</strong>) and verify that the app <span>is working.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Make sure the catalog and checkout apps are deployed first before the home app pipeline starts. This is because, in prod mode, the home app now expects the <strong class="source-inline1">remoteEntry.js</strong> files to be <a id="_idIndexMarker403" class="pcalibre1 calibre6 pcalibre"/>present at the URLs we defined in the <strong class="source-inline1">CATALOG_URL_PROD</strong> and <span><strong class="source-inline1">CHECKOUT_URL_PROD</strong></span><span> constants.</span></p>
			<h1 id="_idParaDest-143" class="calibre7"><a id="_idTextAnchor142" class="pcalibre1 calibre6 pcalibre"/>Summary</h1>
			<p class="calibre3">And with that, we have come to the end of this chapter. I hope you have been able to follow along and enjoyed the joys and pains of wearing a DevOps <span>engineer’s hat.</span></p>
			<p class="calibre3">As you can see, we covered a lot in this chapter. We learned about Kubernetes and its various key components. We saw how you spin up an empty Kubernetes cluster on Azure and learned about the Kubernetes spec files that deploy our micro apps into a Kubernetes cluster. We learned how to containerize our micro apps using Docker and how to set up Docker Hub as a remote image repository. Then, we went through the detailed steps of setting up a CI/CD pipeline using GitHub Actions, and finally, we made the necessary tweaks to our code base so that we can run our module-federated microfrontend on Kubernetes. Now that you have managed to complete this chapter, give yourself a pat on the back and take a well-deserved break before we start with the next chapter, where we will see how to manage our microfrontend <span>in production.</span></p>
		</div>
	</body></html>