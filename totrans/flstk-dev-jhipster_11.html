<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying with Docker Compose</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>We have generated the application and it is ready for production. In this chapter, we will focus on how to deploy the application using Docker Compose. We will also see the various options that JHipster provides for deployment, followed by how to deploy our registry and console alongside the application.</span><br/></p>
<p class="mce-root">In this chapter, we will look into:</p>
<ul>
<li>A short introduction to Docker Compose</li>
<li>Kickstarting Kubernetes</li>
<li>Introducing OpenShift</li>
<li>Explaining Rancher</li>
</ul>
<p>Then we will discuss locally with JHipster Registry and JHipster Console</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing microservice deployment options</h1>
                </header>
            
            <article>
                
<div class="packt_quote">The success of an application not only depends on how well we design it. It depends on how well we implement (deploy and maintain) it.</div>
<p>A well-designed microservice application in a low-availability environment is useless. So it is equally important to decide on a deployment strategy that increases its chances to succeed. When it comes to deployment, there is a plethora of tools available. Each one of them has its pros and cons, and we have to choose one that is suitable for our needs. JHipster currently provides sub-generators to create configuration files to containerize, deploy, and manage the microservices via the following:</p>
<ul>
<li>Docker </li>
<li>Kubernetes (also helps to orchestrate your deployment)</li>
<li>OpenShift (also provides private cloud deployment)</li>
<li>Rancher (also provides complete container management)</li>
</ul>
<p>We will see them in detail in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A short introduction to Docker Compose</h1>
                </header>
            
            <article>
                
<p><span>Shipping code to the server is always difficult, especially when you want to scale it. This is mainly because we have to manually create the same environment and make sure the application has all the necessary connectivity (to other services) that is needed. This was a major pain point for teams when shipping and scaling their code.</span></p>
<div class="packt_quote"><span>Shipping code to the server is difficult.</span></div>
<p>Containers were the game-changer in this field. They helped to bundle the entire application along with dependencies in a shippable container, and all we need is to provide an environment in which these containers can run. This simplified the process of shipping code to the server and also among the development teams. This also reduced the amount of time a team spent making sure that the application ran seamlessly across the environment. </p>
<p>Containers solve the application deployment problem, but how do we scale them?</p>
<p>The Docker Compose tool comes to the rescue here. First, let's see what Docker Compose is, and then see what problems it solves.</p>
<p>Docker Compose is a tool that helps to define and run multi-container Docker applications with a single file. That is, we use a <kbd>.yaml</kbd> file to define the requirements and/or dependencies of the application. Then, with <kbd>docker-compose</kbd>, we can create newer deployments and start our applications as defined in the <kbd>docker-compose</kbd> file.</p>
<p>So, what is required in a <kbd>docker-compose</kbd> file?</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">The following code segment is a sample <kbd>docker-compose</kbd> file that will start a Redis database on port <kbd>5000</kbd>:</p>
<pre class="highlight" style="padding-left: 60px"><strong>version: '3'
services:
  web:
    build: .
    ports:
     - "5000:5000"
  redis:
    image: "redis:alpine"</strong></pre>
<p>The first line of the <kbd>docker-compose</kbd> file should be the version of the <kbd>docker-compose</kbd> tool. </p>
<p>Then we need to specify all the necessary services that we need for our application to run. They should be defined in the <kbd>services:</kbd> section.</p>
<p>We can also define multiple services inside here, giving a name to each (<kbd>web</kbd> and <kbd>redis</kbd>).</p>
<p>This is followed by how to build the <kbd>service</kbd> (either via a command to build or referring a Dockerfile). </p>
<p>If the application needs any port access, we can configure it using <kbd>5000:5000</kbd> (that is internal port: external port). </p>
<p>Then, we have to specify the volume information. This basically tells <kbd>docker-compose</kbd> to serve the files from the location specified.   </p>
<p>Once we have specified the services required for our application, then we can start the application via <kbd>docker-compose</kbd>. This will start your entire application along with the services, and expose the <kbd>services</kbd> on the port specified. </p>
<p>With <kbd>docker-compose</kbd>, we can perform the following operations:</p>
<ul>
<li><strong>Start</strong>: <kbd>docker-compose -f &lt;docker_file&gt; up</kbd></li>
<li><strong>Stop</strong>: <kbd>docker-compose -f &lt;docker_file&gt; down</kbd>  </li>
</ul>
<p>We can also perform the following operations:</p>
<ul>
<li><strong>List the running services and their status</strong>: <kbd>docker ps</kbd><em> </em></li>
<li><strong>Logs</strong>: <kbd>docker log &lt;container_id&gt;</kbd></li>
</ul>
<p>In the compose file, we can add the project name, as follows:</p>
<pre class="highlight" style="padding-left: 60px"><strong>version: '3'</strong><br/><strong>COMPOSE_PROJECT_NAME: "myapp"
services:
  web:
    build: .
    ports:
     - "5000:5000"
  redis:
    image: "redis:alpine"</strong></pre>
<p>This can be used for identifying multiple environments. With the help of this, we can isolate multiple environments. This helps us to handle multiple instances across various <kbd>dev</kbd>, <kbd>QA</kbd>, and <kbd>prod</kbd> environments.</p>
<p><kbd>Docker-compose</kbd> is itself a great tool for deploying your application along with all the services it needs. It provides infrastructure as a code. It is an excellent choice for development, QA, and other environments except for production. But why?</p>
<p><kbd>Docker-compose</kbd> is really good for creating and starting your application. However, when you want to update an existing container there will be a definite downtime, since <kbd>docker-compose</kbd> will recreate the entire container (there are few workarounds to make this happen but still, <kbd>docker-compose</kbd> needs some improvement in this space.)</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kickstarting Kubernetes</h1>
                </header>
            
            <article>
                
<p>According to the Kubernetes website: </p>
<div class="packt_quote packt_infobox"><em>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.</em></div>
<p>It is a simple and powerful tool for automatic deployment, scaling, and managing containerized applications. It provides zero downtime when you roll out a newer application or update an existing application. You can automate it to scale in and out based on certain factors. It also provides self-healing, such that Kubernetes automatically detects the failing application and spins up a new instance. We can also define secrets and configuration that can be used across instances.</p>
<p>Kubernetes primarily focuses on zero downtime production applications upgrades, and also scales them as required. </p>
<p>A single deployable component is called a <strong>pod</strong> in Kubernetes. This can be as simple as a running process in the container. A group of pods can be combined together to form a <strong>deployment</strong>. </p>
<p>Similar to <kbd>docker-compose</kbd>, we can define the applications and their required services in a single YAML file or multiple files (as per our convenience).</p>
<p>Here also, we start with an <kbd>apiVersion</kbd> in a Kubernetes deployment file.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The following code is a sample Kubernetes file that will start a Nginx server:</p>
<pre style="padding-left: 60px"><strong>apiVersion: v1</strong><br/><strong>kind: Service</strong><br/><strong>metadata:</strong><br/><strong>  name: nginxsvc</strong><br/><strong>  labels:</strong><br/><strong>    app: nginx</strong><br/><strong>spec:</strong><br/><strong>  type: NodePort</strong><br/><strong>  ports:</strong><br/><strong>  - port: 80</strong><br/><strong>    protocol: TCP</strong><br/><strong>    name: http</strong><br/><strong>  - port: 443</strong><br/><strong>    protocol: TCP</strong><br/><strong>    name: https</strong><br/><strong>  selector:</strong><br/><strong>    app: nginx</strong></pre>
<p>Followed by the type, which takes either a pod, deployment, namespace, ingress (load balancing the pods), role, and many more.</p>
<p>Ingress forms a layer between the services and the internet so that all the inbound connections are controlled or configured with the ingress controller before sending them to Kubernetes services on the cluster. On the other hand, the egress controller controls or configures services going out of the Kubernetes cluster.</p>
<p>This is followed by the metadata information, <span>such as the type of environments, the application name (nginxsvc), and labels (Kubernetes uses this information to identify and segregate the pods). Kubernetes uses this metadata information to identify the particular pods or a group of pods, and we can manage the instances with this metadata. This is one of the key differences with <kbd>docker-compose</kbd>, where <kbd>docker-compose</kbd> doesn't have the flexibility of defining the metadata about the containers.</span></p>
<p>This is followed by the spec, where we define the specification of the images or our application. We can also define the pull strategy for our images as well as define the environment variables along with the exposed ports. We can define the resource limitations on the machine (or VM) for a particular service. They provide health checks, that is, each service is monitored for the health and when some services fail, they are immediately replaced by newer ones. <span>They also provide service discovery out of the box, by assigning e</span>ach pod an IP, which makes it easier for the services to identify and interact with them. They also provide a better dashboard, to visualize your architecture and the status of the application. You can do most of the management via this dashboard, such as checking the status, logs, scale up, or down the services, and so on.</p>
<p>Since Kubernetes provide a complete orchestration of our services with configurable options, it makes it really hard to set up initially, and this means it is not ideal for a development environment. We also need the <strong>kubectl</strong> CLI tool for management. Despite the fact that we use Docker images inside, the Docker CLI can't be used.</p>
<p>There is also <strong>Minikube</strong> (minified Kubernetes), which is used for developing and testing applications locally.</p>
<div class="packt_quote packt_tip">Kubernetes not only takes care of containerizing your application, it also helps to scale, manage, and deploy your application. It orchestrates your entire application deployment. Additionally, it also provides service discovery and automated health checks.</div>
<p>We will focus more on the Kubernetes sub-generator in the following chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing OpenShift</h1>
                </header>
            
            <article>
                
<p>OpenShift is a multi-cloud, open source container application platform. It is based on Kubernetes and used for developing, deploying, and managing applications. It is a common platform for developers and operations. It helps them to build, deploy, and manage applications consistently across hybrid cloud and multi-cloud infrastructures.</p>
<p>For developers, it provides a self-service platform in which they can provision, build, and deploy applications and their components. With automated workflows for converting your source to the image, it helps developers go from source to ready-to-run, dockerized images.</p>
<p>For operations, it provides a secure, enterprise-grade Kubernetes for policy-based controls and automation for application management, such as cluster services, scheduling, and orchestration with load balancing and auto-scaling capabilities.</p>
<p>JHipster also provides OpenShift deployment files as a separate sub-generator. We can generate them by running <kbd>jhipster openshift</kbd> and answering the questions as needed. This will generate OpenShift related deployment files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Explaining Rancher</h1>
                </header>
            
            <article>
                
<p>Rancher is a container management platform. It is also open source. It helps to deploy and maintain containers for any organization. Rancher is merely a deployment server that is installed on any Linux machine or cluster. So to use Rancher, we should first start the Rancher container, and this requires Docker to be available.</p>
<p>Once started, we can log in to Rancher and start deploying our applications. It also has role management. Rancher provides an option to choose between Swarm, Kubernetes, or Cattle (and other cluster deployment options). It also provides details about the infrastructure and applications that are deployed. It shows detailed information about the containers, registries, data pools, and other information (related to the container and infrastructure).</p>
<p>It also provides options to tweak the Kubernetes or Swarm settings as needed, so it makes it much easier to scale up and down. It also provides options to launch the entire application stack via its UI or using <kbd>docker-compose.yml</kbd> and <kbd>rancher-compose.yml</kbd>. It also has the capability to load the external services and use them (such as a load balancer).</p>
<p><span>JHipster also provides Rancher deployment files as a separate sub-generator. We can generate them by running <kbd>jhipster rancher</kbd> and answering the questions as needed. This will generate the Rancher configuration files.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generated Docker Compose files</h1>
                </header>
            
            <article>
                
<p>By default, JHipster will generate Docker Compose files that enable us to run the application completely in the containerized environment, irrespective of the options chosen. For example, in the gateway application that we have generated, the following files are generated by default under <kbd>src/main/docker</kbd>:</p>
<ul>
<li><kbd>sonar.yml</kbd>: This file creates and starts a SonarQube server</li>
<li><kbd>mysql.yml</kbd>: This file creates and starts a MySQL database server and creates a user and schema</li>
<li><kbd>jhipster-registry.yml</kbd>: This file creates and starts a JHipster Registry service</li>
<li><kbd>app.yml</kbd>: This is the main file that creates and starts the application along with services such as JHipster registry and the database</li>
</ul>
<p>In addition to this, JHipster also creates a Dockerfile, which helps you to containerize the application alone.</p>
<p>Then we can see a folder called <kbd>central-server-config</kbd>. This will be used as a central configuration server for the JHipster Registry. </p>
<p>When the registry and the application are running in Docker, it uses <kbd>application.yml</kbd> from the <kbd>docker-config</kbd> folder as the central configuration server.</p>
<p>On the other hand, when running only the registry in Docker mode, the application, not in Docker, will use <kbd>application.yml</kbd> from the <kbd>localhost-config</kbd> folder. The key difference is that the URL defining the Eureka client varies.</p>
<p>Let's see the Docker files that have been generated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Walking through the generated files</h1>
                </header>
            
            <article>
                
<p>Let's start with the <kbd>app.yml</kbd> file under <kbd>src/main/docker</kbd> inside your gateway application.</p>
<p>As we saw at the beginning of the chapter, the file starts with the Docker version that it supports:</p>
<pre style="padding-left: 60px"><strong>version: '2'</strong></pre>
<p>This is followed by the services section where we define the various services, applications, or components that we will kick start with this Docker file. </p>
<p>Under services section, we will define a name for the service, in our case we have used <kbd>gateway-app</kbd>, followed by the image that we want to use as a container. This image is generated with the help of the Docker file that we have in that folder.</p>
<p>This is followed by the series of environment variables that our application will depend on, they include:</p>
<ul>
<li class="mce-root"><kbd>SPRING_PROFILES_ACTIVE</kbd><span>: Tells the application to run in production mode and expose Swagger endpoints.</span></li>
<li class="mce-root"><kbd>EUREKA_CLIENT_SERVICE_URL_DEFAULTZONE</kbd>: T<span>ells the application where to check for the JHipster Registry (which is the Eureka client that we are using. If we have chosen Consul here, then the application will point to the Consul URL)</span></li>
<li class="mce-root"><kbd>SPRING_CLOUD_CONFIG_URI</kbd>: T<span>ells the application where to look for the <kbd>config</kbd> service for the application.</span></li>
<li class="mce-root"><kbd>SPRING_DATASOURCE_URL</kbd>: T<span>ells the application where to look for the data source.</span></li>
<li class="mce-root"><kbd>JHIPSTER_SLEEP</kbd><span>: Thi</span><span>s is a custom property that we have used to make sure that the JHipster Registry starts before the application starts up.</span></li>
</ul>
<div>
<div>
<div>
<p>Finally, we specify on which port the application should run and be exposed: </p>
</div>
</div>
</div>
<pre style="padding-left: 60px">services:<br/>    gateway-app:<br/>        image: 'gateway'<br/>        environment: <br/>            - SPRING_PROFILES_ACTIVE=prod,swagger<br/>            - EUREKA_CLIENT_SERVICE_URL_DEFAULTZONE=http://admin:$${jhipster.registry.password}@jhipster-registry:8761/eureka<br/>            - SPRING_CLOUD_CONFIG_URI=http://admin:$${jhipster.registry.password}@jhipster-registry:8761/config<br/>            - SPRING_DATASOURCE_URL=jdbc:mysql://gateway-mysql:3306/gateway?.....<br/>            - JHIPSTER_SLEEP=30<br/>        ports:<br/>            8080:8080</pre>
<p>We have just defined the service with the <kbd>docker-compose</kbd> file; now we have to specify two other services that are needed for our application to run. They are the database and JHipster Registry.</p>
<p>So, we register another service called <kbd>gateway-mysql</kbd>, which creates and starts the MySQL server. We can define <span>MySQL</span> as a separate Docker Compose file and link them in here. So, we put an <kbd>extends</kbd> keyword followed by the <kbd>docker-compose</kbd> file and the service that we have to start from the specified <kbd>docker-compose</kbd> file:</p>
<pre>   gateway-mysql:<br/>        extends:<br/>            file: mysql.yml<br/>            service: gateway-mysql.yml</pre>
<p>Then we input the following code for the <kbd>mysql.yml</kbd> file, shown as follows:</p>
<pre style="padding-left: 60px"><span>version</span><span>: </span><span>'2'<br/></span><span>services</span><span>:<br/></span><span>    gateway-mysql</span><span>:</span><span> <br/>        </span><span>image</span><span>: </span><span>mysql:5.7.20</span><span> <br/>        </span><span># volumes:</span><span> <br/>        </span><span># - ~/volumes/jhipster/gateway/mysql/:/var/lib/mysql/</span><span> <br/>        </span><span>environment</span><span>:</span><span> <br/>            - </span><span>MYSQL_USER=root</span><span> <br/>            - </span><span>MYSQL_ALLOW_EMPTY_PASSWORD=yes</span><span> <br/>            - </span><span>MYSQL_DATABASE=gateway</span><span> <br/>        </span><span>ports</span><span>:</span><span> <br/>            - </span><span>3306:3306</span><span> <br/>        </span><span>command</span><span>: </span><span>mysqld --lower_case_table_names=1 --skip-ssl --character_set_server=utf8 --explicit_defaults_for_timestamp</span></pre>
<p>We have again started with the version that it supports followed by the <kbd>services</kbd> keyword and then specify the <kbd>service</kbd> name, <kbd>gateway-mysql</kbd> that is used in the <kbd>app.yml</kbd> file. If you want to specify a volume for the persistent data storage you can uncomment the commented volumes segment. This basically maps the local file location to Docker's internal location so that the data is persistent even if the Docker image itself is replaced or updated.</p>
<p>This is followed by a set of environment variables, such as the username and the password (we have set it to empty here, but for a real production application it is recommended to set it to a more complex password), and then the database schema name.</p>
<p>We have also specified the command that we need to run to start the MySQL server.</p>
<p>Then we go back to the <kbd>app.yml</kbd> file and we then define the JHipster Registry service. This will again extend the <kbd>jhipster-registry.yml</kbd> and <kbd>docker-compose</kbd> file. One more thing to note here is, even though we extend the services from another Docker file, we can override the environment variables that we have specified in the original <kbd>docker-compose</kbd> file. This comes in handy in certain cases where we have to kickstart our application with different or customized values. In our case, we have overridden the location of the Spring Cloud Config server file location from that of the original:</p>
<pre style="padding-left: 60px">    jhipster-registry:<br/>        extends:<br/>            file: jhipster-registry.yml<br/>            service: jhipster-registry<br/>        environment:<br/>            - SPRING_CLOUD_CONFIG_SERVER_NATIVE_SEARCH_LOCATIONS=file:./central-config/docker-config/<br/><br/></pre>
<p>The <kbd>Jhipster-registry.yml</kbd> file:</p>
<div>
<pre style="padding-left: 60px"><span>version</span><span>: </span><span>'2'<br/></span><span>services</span><span>:<br/></span><span>    jhipster-registry</span><span>:<br/>        </span><span>image</span><span>: </span><span>jhipster/jhipster-registry:v3.2.3<br/>    </span><span>volumes</span><span>:<br/>        </span><span>- </span><span>./central-server-config:/central-config<br/>    </span><span># When run with the "dev" Spring profile, the JHipster Registry will<br/>    </span><span># read the config from the local filesystem (central-server-config directory)<br/>    </span><span># When run with the "prod" Spring profile, it will read the configuration from a Git             repository<br/>        </span><span># See http://www.jhipster.tech/microservices-architecture/#registry_app_configuration<br/></span><span>    environment</span><span>:<br/>        </span><span>- </span><span>SPRING_PROFILES_ACTIVE=dev<br/>        </span><span>- </span><span>SECURITY_USER_PASSWORD=admin<br/>        </span><span>- </span><span>JHIPSTER_REGISTRY_PASSWORD=admin<br/>        </span><span>- </span><span>SPRING_CLOUD_CONFIG_SERVER_NATIVE_SEARCH_LOCATIONS=file:./central-config/localhost-config/<br/>        </span><span># - GIT_URI=https://github.com/jhipster/jhipster-registry/<br/>        </span><span># - GIT_SEARCH_PATHS=central-config<br/>    </span><span>ports</span><span>:<br/>        </span><span>- </span><span>8761:8761</span></pre>
<p>We have defined the central-config for JHipster Registry as follows. We have configured the secret for the JWT and the Eureka client's URL. The JWT token specified is used for services to authorize and communicate between them and the registry:</p>
<div>
<pre style="padding-left: 60px"><span># Common configuration shared between all applications<br/></span><span>configserver</span><span>:<br/>    </span><span>name</span><span>: </span><span>Docker JHipster Registry<br/>    </span><span>status</span><span>: </span><span>Connected to the JHipster Registry running in Docker<br/></span><span>jhipster</span><span>:<br/>    </span><span>security</span><span>:<br/>        </span><span>authentication</span><span>:<br/>            </span><span>jwt</span><span>:<br/>                </span><span>secret</span><span>: </span><span>my-secret-token-to-change-in-production<br/></span><span>eureka</span><span>:<br/>    </span><span>client</span><span>:<br/>        </span><span>service-url</span><span>:<br/>            </span><span>defaultZone</span><span>: </span><span>http://admin:${jhipster.registry.password}@localhost:8761/eureka/</span></pre>
<p>Added to these, we also generate a <kbd>sonar.yml</kbd>, (this file is not important for deploying your application):</p>
<div>
<pre style="padding-left: 60px"><span>version</span><span>: </span><span>'2'<br/></span><span>services</span><span>:<br/>    </span><span>gateway-sonar</span><span>:<br/>        </span><span>image</span><span>: </span><span>sonarqube:6.5-alpine<br/></span><span>        </span><span>ports</span><span>:<br/>            </span><span>- </span><span>9000:9000<br/>            </span><span>- </span><span>9092:9092</span></pre>
<p>Similarly, in the microservices, that is, in our invoice and the notification applications, we will have similar files generated. They are the same except for the change in the service name.</p>
<p>Unlike MySQL, MongoDB is also capable of running as a cluster with different nodes and configuration. We need to specify them differently here. So we will create have two docker-compose files. <kbd>mongodb.yml</kbd> is for starting the MongoDB with a single node, and the <kbd>mongodb-cluster.yml</kbd> to start the MongoDB as the cluster.</p>
</div>
<p>Please check the database port number between the gateway and the microservice application. If they use the same database, there may be a clash in the port number since JHipster generates the same port number for both. Change it to any other unused port, or Docker Compose will show an error. In our case, I have changed it to <kbd>3307</kbd>.</p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building and deploying everything to Docker locally</h1>
                </header>
            
            <article>
                
<p>There are multiple ways in which we can use the <kbd>docker-compose</kbd> files based on our needs.</p>
<p>In general, when we are developing the application, we can run the application with the general Maven or Gradle command so that we can debug the application and also reload the changes faster, and start the database and JHipster registry with Docker.</p>
<p>Otherwise, you can start the entire application from the <kbd>app.yml</kbd> file, which will kickstart the database, JHipster Registry, and then the application itself. To do that, open your terminal or Command Prompt, go to the application folder, and then run the following command:</p>
<pre><strong>&gt; cd gateway</strong></pre>
<p>Then we have to first Dockerize the application by taking a production build of our application with the following command:</p>
<pre><strong>&gt; ./gradlew bootRepackage -Pprod buildDocker</strong></pre>
<p>Once done, we can start the app via the <kbd>docker-compose</kbd> command:</p>
<pre><strong>&gt; docker-compose -f src/main/docker/app.yml up -d</strong></pre>
<p><kbd>-f</kbd> specifies the file with which <kbd>docker-compose</kbd> should start the server. The <kbd>-d</kbd> flag tells <kbd>docker-compose</kbd> to run everything in detached mode. This will start the application in Docker and expose the application on port <kbd>8080</kbd>, the registry server on port <kbd>8761</kbd>, and the database on port <kbd>3306</kbd>.</p>
<p>Then we can go to the respective microservices folder and do the same, create a docker image with the following command:</p>
<pre><strong>&gt; ./gradlew bootRepackage -Pprod buildDocker</strong></pre>
<p>Then we can start the application via <kbd>docker-compose</kbd> with the following command:</p>
<pre><strong>&gt; docker-compose -f &lt;filename&gt; -d</strong></pre>
<p>We can check the running Docker containers with the following command:</p>
<pre><strong>&gt; docker ps -a</strong></pre>
<p>It should list all seven containers:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/823303fc-e453-42ca-b415-eb29d6ec4793.png"/></div>
<p>As you can see, there are three app containers (gateway/notification, and invoice), and then a JHipster-Registry, followed by three database containers (two MySQL and one MongoDB. The order may vary).</p>
<div class="packt_tip">If you are using JHipster Version 5 or above use <kbd>bootWar</kbd> instead of the <kbd>bootRepackage</kbd> command in Gradle.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating docker-compose files for microservices</h1>
                </header>
            
            <article>
                
<p>There are many <kbd>docker-compose</kbd> files and maintaining them is hard. Thankfully, JHipster has a <kbd>docker-compose</kbd> <span>sub generator</span><span> </span>bundled with it. The <kbd>Docker-compose</kbd> <span>sub generator</span><span> </span>helps you to organize all your application's<span> D</span>ockerfiles<span> </span>together. It creates a single<span> D</span>ockerfile<span> </span>that refers to the application's Dockerfiles. </p>
<p>Let's go to the base folder and create<span> </span>a folder and name it <kbd>docker-compose</kbd>:</p>
<pre><strong>&gt; mkdir docker-compose &amp;&amp; cd docker-compose</strong></pre>
<p>Once inside the <kbd>docker-compose</kbd> folder, we can run the following command:</p>
<pre> <strong>jhipster docker-compose</strong> </pre>
<p>This will generate the Dockerfiles.</p>
<p>As usual, it will ask us a series of questions, before generating the files:</p>
<div class="CDPAlignCenter CDPAlign"><img height="126" src="assets/6fc3ae79-37a7-4a6c-a6a1-458d5c2f7a51.png" width="501"/></div>
<p>At first, it asks which type of application we would like to deploy. We will select the microservice application as an option.</p>
<p>This is followed by choosing the type of gateway that we would like to use; there are two options available, a JHipster-gateway with Zuul proxy, and the more exciting,<span> </span>Traefik<span> </span>gateway with Consul</p>
<p>Let us choose JHipster-gateway with Zuul proxy:</p>
<div class="CDPAlignCenter CDPAlign"><img height="130" src="assets/2d028b76-22d8-4930-8ee0-f5f9f8b45bfe.png" width="497"/></div>
<p class="mce-root">Then, we have to select the location of the microservices gateway and applications. This is the main reason why we have generated the applications inside a single parent folder. This will help plugins and sub-generators to easily find the docker configuration files created. We will select the default option (<span class="packt_screen">../)</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="112" src="assets/a480c434-a824-4568-bcc5-cca434bbe6d5.png" width="512"/></div>
<p>After selecting the location, JHipster will search inside the given folder for any JHipster generated <span>the</span> application<span> </span>and list them in the next question.</p>
<p>In our case, it lists <span class="packt_screen">notification</span>, <span class="packt_screen">invoice</span>, and <span class="packt_screen">gateway</span>. We can choose all of them and hit <em>Enter</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="202" src="assets/9af279b6-42ad-4a37-9413-0b1590627a08.png" width="527"/></div>
<p>It automatically detects that we have used MongoDB and asks us the next question; whether we would like to have MongoDB as a cluster. We will not choose anything here:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/36154e7c-2dee-4177-93c5-7e844dd8ee14.png"/></div>
<p>Then it asks about the console; whether we need to set up any consoles for the application. We will choose logs and metrics with the JHipster Console (based on ELK and Zipkin):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/604d8af5-db29-4b8e-923f-da698e6691c7.png"/></div>
<p>We can either opt out from the monitoring option or choose Prometheus. That connects with Prometheus and shows metrics only.</p>
<p>Then JHipster asks whether you need Curator or <span>Z</span>ipkin:</p>
<ul>
<li>Curator<span> </span>will help you to curate and manage the indices created by Elasticsearch</li>
<li>Zipkin (as discussed in the previous chapter)</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cf0fdc3c-a344-4a4f-9b3f-5e89b59f50df.png"/></div>
<p>Since the JHipster console is chosen, it will ask for additional pieces of information supported by the console. They include Zipkin and Curator. We have already seen Zipkin. Curator, on the other hand, will help us to manage and curate the indices in Elasticsearch.</p>
<p>We will choose only Zipkin here.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/59d1b8c4-99c8-468a-b7c5-d49e46f271d9.png"/></div>
<div class="packt_infobox"><span>We can also choose nothing here and go with the default option.</span></div>
<p>Finally, it asks for the password for the JHipster Registry; we will go with the default here:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0b3bd232-743e-416d-9c10-d6c2451fbfb5.png"/></div>
<p>That is it; we have just created a higher-level Dockerfile that has information about all the services that we need to run the application successfully.</p>
<p>Now we can just run the entire suite with the following command:</p>
<pre><strong>&gt; docker-compose up -d</strong></pre>
<p>This will start the gateway, notification, invoice, and the registry, along with the console and all other required services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Features of the deployed application</h1>
                </header>
            
            <article>
                
<p>Thus, the deployed applications are ready to be launched. We can launch the JHipster Registry at <kbd>http://localhost:8761</kbd>; it will list all the registered applications:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3eb7cfcc-036a-43f0-9766-e232a6c9196f.png"/></div>
<p>Added to that, the registry also tells us the number of instances that are registered. Navigate to <span class="packt_screen">Eureka</span> | <span class="packt_screen">Instances</span> to check that. Currently, we have one of each instance registered:</p>
<div class="CDPAlignCenter CDPAlign"><img height="241" src="assets/639db113-5f5f-4644-90e3-673bb49742ba.png" width="488"/></div>
<p>Similarly, the Gateway application will list down the microservices that are connected to it. Go to <kbd>http://localhost:8080</kbd>.</p>
<p>Navigate to <span class="packt_screen">Administration</span> | <span class="packt_screen">Gateway</span> to see the microservices applications that are connected to this Gateway application:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/51b49e7e-d335-4760-9356-7aa15d0f1646.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">JHipster console demo</h1>
                </header>
            
            <article>
                
<p>JHipster also provides a console application based on the ELK stack, which can be used for logs and metrics monitoring of the application. JHipster Console is another open source application. It is really useful and provides some nice dashboards to visualize the application. As with other JHipster products, it is much easier to get started with the JHipster Console. </p>
<p>Let's go back to our book folder, and then clone the JHipster console project from GitHub (<a href="https://github.com/jhipster/jhipster-console">https://github.com/jhipster/jhipster-console</a>):</p>
<pre><strong>&gt; git clone https://github.com/jhipster/jhipster-console</strong></pre>
<p>Before we start our console, we need to make our applications log the metrics and log into the console. To make that happen, we need to change a few settings in our applications and then restart them.</p>
<p>Let's go to our <kbd>application-prod.yml</kbd> file in all the applications (gateway and microservices application) and enable the logstash and logs:</p>
<div>
<pre><span>metrics</span><span>: </span><span># DropWizard Metrics configuration, used by MetricsConfiguration    <br/></span><span>    ...<br/>    </span><span>logs</span><span>: </span><span># Reports Dropwizard metrics in the logs<br/>        </span><span>enabled</span><span>: </span><span>true<br/>        </span><span>report-frequency</span><span>: </span><span>60</span><span> </span><span># in seconds<br/></span><span>logging</span><span>:<br/>    </span><span>logstash</span><span>: </span><span># Forward logs to logstash over a socket, used by LoggingConfiguration<br/>        </span><span>enabled</span><span>: </span><span>true<br/>        </span><span>host</span><span>: </span><span>localhost<br/>        </span><span>port</span><span>: </span><span>5000<br/>        </span><span>queue-size</span><span>: </span><span>512</span></pre>
<p>Set enabled to true in <kbd>metrics.logs.enabled</kbd> and also <kbd>logging.logstash.enabled</kbd>. This will push the logs to the console application. JHipster Console will collect this information and show it in nice-looking dashboards with the help of Kibana.</p>
</div>
<p>Once cloned, we can go into this folder and then start the <kbd>jhipster-console</kbd> with the help of <kbd>docker-compose</kbd>:</p>
<pre><strong>&gt; cd jhipster-console</strong><br/><strong>&gt; docker-compose up -d</strong></pre>
<p>That is it. Your console is running on <kbd>http://localhost:5601</kbd>.</p>
<p>Kibana provides the following (customizable) dashboards:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/490f238b-6224-40c9-90a9-45035675d289.png"/></div>
<p>Kibana also provides application-level metrics, such as JVM threads metrics and other details:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/53dc92f3-6f24-41cf-86b2-623c87c342ff.png"/></div>
<p>Added to this, the console also has an interface where we can see the application logs. It shows the log of all the applications deployed. We can filter and search the logs with respect to the application:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a12046f7-0bda-4168-a082-5bc6b2b2918a.png"/></div>
<p>Kibana also provides a machine-learning tab where we can create a job to track the data and then choose any metric available to track it as a sum, count, high count, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling up with Docker Swarm</h1>
                </header>
            
            <article>
                
<p>Docker Swarm is Docker's orchestrating layer to manage the containers. It is a cluster management tool that focuses on creating replicas of your services, networks, as well as storage resources available to it and managing them.</p>
<p>The <span>Docker Swarm</span> is nothing more than a cluster of Docker nodes. They act as a manager or worker. One interesting feature to note is that a Docker container inside the Swarm can either be a manager or worker or both. This helps the swarm to allocate a new manager when the manager nodes go down. </p>
<p>At a high level, the manager nodes are responsible for cluster management tasks and execute containers. The workers are responsible for executing the containers only.</p>
<p>JHipster applications give us the flexibility to scale our entire application with a single command, with JHipster's <kbd>docker-compose sub-generator</kbd>:</p>
<pre><span><strong>&gt; docker-compose scale &lt;app-name&gt;=&lt;scale to&gt;</strong><br/></span></pre>
<p>Now we can scale the instances, using the following command:</p>
<pre><strong>&gt; docker-compose scale invoice-app=2</strong></pre>
<p>The preceding command will spin another invoice instance and we can see it on <span>the dashboard, as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8c9ea7db-888c-4ceb-87a6-dfc146928e9c.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how to generate, set up, and start JHipster Registry and console, and, we have looked at their features. This was followed by how to scale the application with <kbd>docker-swarm</kbd>.</p>
<p>In the next chapter, we will see how to deploy the application to the Google Cloud using Kubernetes.</p>
<p> </p>


            </article>

            
        </section>
    </body></html>